{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0cd007-c07d-41ae-a7d1-e8a396936a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75927828-0e72-4da3-a840-98d46c1e6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#!python3 -m pip install wandb\n",
    "import wandb\n",
    "\n",
    "from Core_Utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d84851-1726-4bfb-955d-209cdc80bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d39e84-a7e5-4344-af5d-ed91b8c5d2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f7d1cf-6309-45ae-b1c2-73609064339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    The class to implement Additive attention aka Bhadanau Attention. As seen in lectures.    \n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size,D,expected_dim,batch_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.U_att = nn.Linear(hidden_size*expected_dim, hidden_size)\n",
    "        self.W_att = nn.Linear(hidden_size*D, hidden_size)\n",
    "        self.V_att = nn.Linear(hidden_size, 1)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, decoder_prev_hidden, encoder_contexts):\n",
    "\n",
    "        \"\"\"\n",
    "        The method that takes decoder hiddenstates and encoder hidden contexts to produce attention weighted context vector.\n",
    "        Params:\n",
    "            decoder_prev_hidden : The decoder's hidden state at t-1.\n",
    "            encoder_contexts : The encoder hidden states from all time steps.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        decoder_prev_hidden = decoder_prev_hidden.reshape(self.batch_size,1,-1)\n",
    "        \n",
    "        ## Following the same expression as seen in lectures. [Slide 256 in https://www.cse.iitm.ac.in/~miteshk/CS7015/Slides/Teaching/pdf/Lecture16.pdf]\n",
    "        scores = self.V_att(torch.tanh(self.U_att(decoder_prev_hidden.reshape(self.batch_size,1,-1)) + self.W_att(encoder_contexts))).squeeze(2).unsqueeze(1)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        ##compute context of each word in the batch, by considering attention\n",
    "        context = torch.bmm(weights, encoder_contexts)\n",
    "\n",
    "        return context, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3698224-b989-4dfd-99a0-0007c176a5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The class that implements the encoder using Recurrent Units RNN/LSTM/GRU, as needed, by extending the nn.Module class from torch.    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, source_vocab_size,hidden_size,embedding_size,rnn_type = \"GRU\",padding_idx = None ,dropout=0.1,num_layers = 1,bidirectional = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        The constructor of the Encoder Class.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            source_vocab_size : The vocabulary size of the source language.\n",
    "            hidden_size : The dimension of the hidden state of the recurrent cell.\n",
    "            embedding_size : The dimension of the embedding used.\n",
    "            rnn_type : \"GRU\"/\"LSTM\"/\"RNN\", case INsensitive. Default : \"GRU\".\n",
    "            padding_idx : The id corresponding to the token <pad>.\n",
    "            dropout : Droput probability. Default : 0.1\n",
    "            num_layers(int) : The number of encoder (recurrence unit) layers. Default : 1\n",
    "            bidirectional : True/False. If True, encoding is done by parsing input L->R and R->L, hence doubling the hiddenstate size. Default False.\n",
    "\n",
    "        Return:\n",
    "            None.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_type = rnn_type\n",
    "\n",
    "        self.D = 1 ##the number of directions in which the input is viewed.\n",
    "        if bidirectional:\n",
    "            self.D = 2\n",
    "\n",
    "        self.rnn_dropout = 0\n",
    "        if self.num_layers>1:\n",
    "            self.rnn_dropout = dropout\n",
    "\n",
    "        ##create an embedding layer, and ignore padding index\n",
    "        self.embedding = nn.Embedding(source_vocab_size, self.embedding_size,padding_idx = padding_idx)\n",
    "        \n",
    "        if self.rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(self.embedding_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=self.rnn_dropout)\n",
    "        elif self.rnn_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(self.embedding_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=self.rnn_dropout)\n",
    "        elif self.rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(self.embedding_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=self.rnn_dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input,hidden=None,cell=None):\n",
    "\n",
    "        \"\"\"\n",
    "        The method to perform forward pass of the encoder.\n",
    "\n",
    "        Params : \n",
    "            Input : The encoded batch of input tensors.\n",
    "            hidden : Default is None. If the unit is LSTM, it is the previous hidden state.\n",
    "            cell : Default is None. If the unit is LSTM, it is the cell state.\n",
    "\n",
    "        Returns : \n",
    "            output, hidden, cell.\n",
    "            hidden and cell are current hidden and cell states in case of LSTM and they are None in other cases.\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        input_embedding = self.dropout(self.embedding(input))\n",
    "        \n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            output, (hidden, cell) = self.rnn(input_embedding, (hidden, cell))\n",
    "        else:\n",
    "            output, hidden = self.rnn(input_embedding)\n",
    "            cell = None\n",
    "        \n",
    "        return output, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c0c87-21d9-4609-9739-9df28e7844bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "535e291a-e0ae-453c-96aa-5e18bafe2173",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    \"\"\"\n",
    "    The class to implement Decoder in the encoder-decoder architecture using \"RNN\"/\"LSTM\"/\"GRU\".\n",
    "\n",
    "    While the code is flexible enough to support separate types of recurrent units for encoder and decoder,\n",
    "    In this assignment, I have chosen to use the same type recurrent unit for both.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size,embedding_size,target_vocab_size,rnn_type,batch_size,use_attention = True,padding_idx = None,num_layers = 1,bidirectional = False,dropout=0,device = \"cpu\"):\n",
    "        \n",
    "        \"\"\"\n",
    "        The constructor of this class. Perfoms setup necessary for training.\n",
    "\n",
    "        hidden_size : The dimension of the hidden state of the recurrent cell.\n",
    "        embedding_size : The dimension of the embedding used.\n",
    "        target_vocab_size : The vocabulary size of the target language.\n",
    "        rnn_type : \"GRU\"/\"LSTM\"/\"RNN\", case INsensitive. Default : \"GRU\".\n",
    "        batch_size : The batch size used for training. This is needed to resize dimensions in the BahdanauAttention's forward pass.\n",
    "        use_attention : Boolean variable, default True, indicating to make use of BahdanauAttention.\n",
    "        padding_idx : The id corresponding to the token <pad>.\n",
    "        dropout : Droput probability. Default : 0.1\n",
    "        num_layers(int) : The number of encoder (recurrence unit) layers. Default : 1\n",
    "        bidirectional : True/False. If True, encoding is done by parsing input L->R and R->L, hence doubling the hiddenstate size. Default False.\n",
    "        device : The device on which the processing happens. Default : \"cpu\".\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn_type = rnn_type\n",
    "        self.device = device\n",
    "        self.D = 1 ##the number of directions in which the input is viewed.\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.use_attention = use_attention\n",
    "        if bidirectional:\n",
    "            self.D = 2\n",
    "\n",
    "        ## In h0 (the input to the decoder) first dimension expected is number of directions X number of layers\n",
    "        self.expected_h0_dim1 = self.D*self.num_layers\n",
    "\n",
    "        ##create an embedding layer, and ignore padding index\n",
    "        self.embedding = nn.Embedding(target_vocab_size, self.embedding_size,padding_idx = padding_idx)\n",
    "\n",
    "        if self.use_attention:\n",
    "            self.attention = BahdanauAttention(hidden_size,self.D,self.expected_h0_dim1,batch_size)\n",
    "            recurrent_unit_input_dim = self.embedding_size + self.D*hidden_size\n",
    "\n",
    "        else:\n",
    "            recurrent_unit_input_dim = self.embedding_size\n",
    "\n",
    "\n",
    "        self.rnn_dropout = 0\n",
    "        if self.num_layers>1:\n",
    "            self.rnn_dropout = dropout\n",
    "\n",
    "        #self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional)\n",
    "        if self.rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(recurrent_unit_input_dim, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=self.rnn_dropout)\n",
    "        elif self.rnn_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(recurrent_unit_input_dim, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=self.rnn_dropout)\n",
    "        elif self.rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(recurrent_unit_input_dim, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=self.rnn_dropout)\n",
    "\n",
    "        ## Passing the hidden state through a fully conencted layer and then applying softmax\n",
    "        self.output_layer = nn.Linear(self.hidden_size*self.D, target_vocab_size)\n",
    "\n",
    "    def forward(self, encoder_hidden_contexts, encoder_last_hidden,encoder_cell,target_tensor,eval_mode = False,teacher_forcing_ratio=0):\n",
    "        \"\"\"\n",
    "        Method that Implements the forward pass of the decoder.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            encoder_hidden_contexts : The hidden contexts from all time steps of the encoder.\n",
    "            encoder_last_hidden : The the last hidden state of the encoder, the context, passed as the first decoder_hidden.\n",
    "            encoder_cell : It is relevant iff the recurrent unit used is LSTM, all other times it would be 0.\n",
    "            eval_mode : Boolean variable, if true, it adjusts the dimensions to predict for a single word.\n",
    "            teacher_forcing_ratio : value in [0,1]. It is essentially the probability, with which true input is fed into the decoder at a time step. Default is 0.\n",
    "\n",
    "            Returns decoder_outputs,decoder_hidden\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encoder_hidden_contexts.size(0)\n",
    "        if not eval_mode:\n",
    "            max_word_len = target_tensor.size(1)\n",
    "\n",
    "        ## eval mode is for looking at a specific word that is predicted to compare with the correct word.\n",
    "        if eval_mode:\n",
    "            batch_size = 1\n",
    "            max_word_len = 30 ## an arbitrary number, larger in expecected sense.\n",
    "\n",
    "        #the decoder predicts one character at a time, and hence we use a list to store all the predictions.\n",
    "        decoder_outputs = []\n",
    "        if self.use_attention:\n",
    "            attentions = []\n",
    "        else:\n",
    "            attentions = None\n",
    "\n",
    "        ## At the first time step <SOS> token (which has an id 0, is fed as an input to the decoder).\n",
    "        decoder_input = torch.zeros((batch_size, 1), dtype=torch.long, device=self.device)\n",
    "        decoder_hidden = encoder_last_hidden ## in the first time step of the decoder, the output of the encoder is the input.\n",
    "        decoder_cell = encoder_cell ## the cell state, which is initially same as that of encoder, (applies to LSTM unit only)\n",
    "\n",
    "        for step in range(max_word_len):\n",
    "\n",
    "            ## eval mode is for looking at a specific word that is predicted to compare with the correct word.\n",
    "            if eval_mode:\n",
    "                decoder_input = decoder_input.view(1,-1)\n",
    "\n",
    "            embedding = self.embedding(decoder_input)\n",
    "\n",
    "\n",
    "            if decoder_hidden.shape[0] != self.expected_h0_dim1:\n",
    "                reshaped_hidden = decoder_hidden.repeat(self.expected_h0_dim1,1,1)\n",
    "            else:\n",
    "                reshaped_hidden = decoder_hidden\n",
    "\n",
    "\n",
    "            if self.use_attention:\n",
    "                ## the attention part.\n",
    "                decoder_prev_hidden = reshaped_hidden.permute(1, 0, 2)\n",
    "                context_vector, attention_weights = self.attention(decoder_prev_hidden, encoder_hidden_contexts)\n",
    "                tmp_input = torch.cat((embedding, context_vector), dim=2)\n",
    "            else:\n",
    "                ## introducing non-lineartiy through ReLU activation\n",
    "                activated_embedding = F.relu(embedding)\n",
    "                tmp_input = activated_embedding\n",
    "\n",
    "\n",
    "            if self.rnn_type == \"LSTM\":\n",
    "                tmp_output, (decoder_hidden, decoder_cell) = self.rnn(tmp_input, (reshaped_hidden, decoder_cell))\n",
    "            else:\n",
    "                tmp_output, decoder_hidden = self.rnn(tmp_input, reshaped_hidden)\n",
    "                cell = None\n",
    "\n",
    "            decoder_output = self.output_layer(tmp_output.squeeze(0))\n",
    "\n",
    "            ## randomly sample a number in (0,1) and if the number is less than the teacher forcing ratio\n",
    "            ## apply teacher forcing at the current step\n",
    "            apply_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            if (target_tensor is not None) and (apply_teacher_forcing):\n",
    "\n",
    "                ## Teacher forcing: Feed the target as the next input\n",
    "                ## extract the 't'th token from th target string to feed as input at \"t\"th time step.\n",
    "                decoder_input = target_tensor[:, step].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                ##greedily pick predictions, i.e pick the character corresponding to the hightest probability\n",
    "                _,preds = torch.max(decoder_output,dim=2)\n",
    "                decoder_input = preds.detach()\n",
    "\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            if self.use_attention:\n",
    "                attentions.append(attention_weights)\n",
    "\n",
    "        ## concatenate the predictions across all the timesteps into a singel tensor\n",
    "        ## found in literature that log_softmax does better than softmax, hence going with that.\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "\n",
    "        ## the idea is to have a common API for both attention and normal decoder, achiveing ease of use.\n",
    "        return decoder_outputs, decoder_hidden, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c38af4-7bf7-4adf-ab8b-71ebb0b69416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,decoder_optimizer, criterion,teacher_forcing_ratio,ignore_padding=True,device='cpu'):\n",
    "\n",
    "    tot_correct_word_preds = 0\n",
    "    tot_words = 0\n",
    "    \n",
    "    total_loss = 0\n",
    "    for data in tqdm(dataloader):\n",
    "        ## print(f\"Data Shape : {data[0].shape,data[1].shape}\")\n",
    "        input_tensor, target_tensor,_,target_max_len = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        batch_size = data[0].shape[0]\n",
    "\n",
    "        if encoder.rnn_type == \"LSTM\":\n",
    "            encoder_hidden = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "            encoder_cell = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "        else:\n",
    "            encoder_hidden = None\n",
    "            encoder_cell = None\n",
    "        \n",
    "\n",
    "        encoder_hidden_contexts, encoder_last_hidden, encoder_cell = encoder(input_tensor,encoder_hidden,encoder_cell)\n",
    "        \n",
    "        decoder_outputs, _, _ = decoder(encoder_hidden_contexts, encoder_last_hidden,encoder_cell, target_tensor=target_tensor,teacher_forcing_ratio = teacher_forcing_ratio)\n",
    "\n",
    "        multi_step_preds = torch.argmax(decoder_outputs,dim=2)\n",
    "        multi_step_pred_correctness = (multi_step_preds ==  target_tensor)\n",
    "        num_words = multi_step_preds.shape[0]\n",
    "        \n",
    "        if ignore_padding: ## if padding has to be ignored.\n",
    "\n",
    "            ## for each word, based on the padding token ID, find the first occurance of the padding token, marking the begining of padding.\n",
    "            \n",
    "            ## argmax is not supported for bool on cuda, hence casting it to long.\n",
    "            padding_start = torch.argmax((target_tensor == dataloader.dataset.pad_token_id).to(torch.long),dim=1).to(device)\n",
    "            ## Creating a mask with 1's in each position of a padding token\n",
    "            mask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\n",
    "            #print(mask)\n",
    "            \n",
    "            ##doing a logical OR with the mask makes sure that the padding tokens do not affect the correctness of the word\n",
    "            tot_correct_word_preds += (torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum()).item()\n",
    "            tot_words += num_words\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_loss = round(total_loss / len(dataloader),4)\n",
    "    epoch_accuracy = round(tot_correct_word_preds*100/tot_words,2)\n",
    "\n",
    "    return epoch_loss,epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aee733-e313-47eb-b041-7af96d5a65db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e907afc-4b4b-4f7b-8e90-e605ba441e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(dataloader,encoder,decoder,criterion,padding_token_id,end_token_id = 1,ignore_padding = True,device='cpu'):\n",
    "\n",
    "    \"\"\"\n",
    "    Method to compute the accuracy using the model (encoder-decoder) using dataloader.\n",
    "\n",
    "    This method returns word and character level accuracy.\n",
    "\n",
    "        Word Level Accuracy : Accuracy is computed at the word level and a word is right iff every character is predicted correctly.\n",
    "        Char Level Accuracy : Accuracy is computed by comparing each predicted character wrt the correct char.\n",
    "\n",
    "    Params:\n",
    "\n",
    "        dataloader : The train/test/valid dataloader.\n",
    "        encoder : The encoder \n",
    "        decoder : The decoder\n",
    "        padding_token_id : The id of the padding token.\n",
    "        ignore_padding : If True, then in word level accuracy, the padding characters are ignored in computing the word level accuracy.\n",
    "                        char level accuracy, the padding characters are not considered at all.\n",
    "\n",
    "                        If false, padding is considered to be a part of the word (for word level accuracy) and \n",
    "    \"\"\"\n",
    "\n",
    "    char_lvl_accuracy = 0\n",
    "    word_level_accuracy = 0\n",
    "\n",
    "    tot_chars = 0\n",
    "    tot_words = 0\n",
    "\n",
    "    tot_correct_char_preds = 0\n",
    "    tot_correct_word_preds = 0\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    #criterion = loss_criterion.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        train = 0\n",
    "\n",
    "        if encoder.training and decoder.training: ## reset the the model back to train mode\n",
    "            train = 1\n",
    "\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        for data in dataloader:\n",
    "            \n",
    "            input_tensor, target_tensor,_,target_max_len = data\n",
    "\n",
    "            batch_size = data[0].shape[0]\n",
    "    \n",
    "            if encoder.rnn_type == \"LSTM\":\n",
    "                encoder_hidden = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "                encoder_cell = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "            else:\n",
    "                encoder_hidden = None\n",
    "                encoder_cell = None\n",
    "            \n",
    "    \n",
    "            encoder_hidden_contexts, encoder_last_hidden, encoder_cell = encoder(input_tensor,encoder_hidden,encoder_cell)\n",
    "            ## even though we are passing target tensor, the teacher forcing ratio is 0, so no teacher forcing\n",
    "            decoder_outputs, _, _ = decoder(encoder_hidden_contexts, encoder_last_hidden,encoder_cell, target_tensor = target_tensor,teacher_forcing_ratio = 0)\n",
    "    \n",
    "            loss += criterion(\n",
    "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                target_tensor.view(-1)).item()\n",
    "    \n",
    "            ## For a batch, for each character find the most probable output word.\n",
    "            multi_step_preds = torch.argmax(decoder_outputs,dim=2)\n",
    "            multi_step_pred_correctness = (multi_step_preds ==  target_tensor)\n",
    "            num_chars = multi_step_preds.numel() ##find the total number of characters in the current batch\n",
    "            num_words = multi_step_preds.shape[0] ##find the total number of words in the current batch.\n",
    "    \n",
    "            if ignore_padding: ## if padding has to be ignored.\n",
    "    \n",
    "                ## for each word, based on the padding token ID, find the first occurance of the padding token, marking the begining of padding.\n",
    "                \n",
    "                ## argmax is not supported for bool on cuda, hence casting it to long.\n",
    "                padding_start = torch.argmax((target_tensor == dataloader.dataset.pad_token_id).to(torch.long),dim=1).to(device)\n",
    "                ## Creating a mask with 1's in each position of a padding token\n",
    "                mask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\n",
    "                #print(mask)\n",
    "                \n",
    "                ##doing a logical OR with the mask makes sure that the padding tokens do not affect the correctness of the word\n",
    "                tot_correct_word_preds += (torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum()).item()\n",
    "                tot_words += num_words\n",
    "    \n",
    "                ##creating a complement of the mask so to mark padding tokens as irrelevant\n",
    "                complement_mask = (1-mask.int()).bool()\n",
    "                num_pad_chars = mask.int().sum().item()\n",
    "                ##counting number of non_pad_chars to compute accuracy.\n",
    "                num_non_pad_chars = num_chars - num_pad_chars\n",
    "    \n",
    "                tot_correct_char_preds += (torch.logical_and(multi_step_pred_correctness,complement_mask).int().sum()).item()\n",
    "                tot_chars += num_non_pad_chars\n",
    "                \n",
    "        \n",
    "            else: ##otherwise.\n",
    "    \n",
    "                tot_correct_word_preds += (torch.all(multi_step_pred_correctness,dim=1).int().sum()).item()\n",
    "                tot_words += num_words\n",
    "                \n",
    "                tot_correct_char_preds += (multi_step_pred_correctness.int().sum()).item()\n",
    "                tot_chars += num_chars\n",
    "\n",
    "        #print(tot_correct_char_preds,tot_chars)\n",
    "        #print(tot_correct_word_preds,tot_words)\n",
    "    \n",
    "        char_lvl_accuracy = round(tot_correct_char_preds*100/tot_chars,2)\n",
    "        word_lvl_accuracy = round(tot_correct_word_preds*100/tot_words,2)\n",
    "\n",
    "        loss /= dataloader.dataset.data.shape[0]\n",
    "\n",
    "        if train:\n",
    "\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "    \n",
    "        return round(loss,4),char_lvl_accuracy,word_lvl_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f149d-39ce-46ef-9b72-3cb24a57d7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f33e86bf-bdef-4939-9f94-66c10e15665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader,valid_loader, encoder, decoder, n_epochs,padding_idx,optimiser = \"adam\",loss=\"crossentropy\",weight_decay=0, lr=0.001,teacher_forcing = False,teacher_forcing_ratio = 0,print_every=100, plot_every=100,device='cpu',wandb_logging = False):\n",
    "    \n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    ## specify the optimiser\n",
    "    if optimiser.lower() == \"adam\":\n",
    "        encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "    elif optimiser.lower() == \"nadam\":\n",
    "        encoder_optimizer = optim.NAdam(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        decoder_optimizer = optim.NAdam(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "    elif optimiser.lower() == \"rmsprop\":\n",
    "        encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        \n",
    "    ## Specify the loss criteria\n",
    "    if loss.lower() == \"crossentropy\":\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index = padding_idx).to(device)\n",
    "\n",
    "    lp = train_dataloader.dataset.lp\n",
    "    \n",
    "    #criterion = loss_criterion.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "\n",
    "        train_loss,train_accuracy = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,teacher_forcing_ratio,device=device)\n",
    "        print_loss_total += train_loss\n",
    "        plot_loss_total += train_loss\n",
    "        val_loss,_,val_accuracy = compute_accuracy(valid_loader,encoder,decoder,criterion,padding_token_id = lp.source_char2id['<pad>'],end_token_id = lp.source_char2id['<end>'],ignore_padding=True,device=device)\n",
    "\n",
    "        print(f\"Epoch {epoch}\\t Train Loss : {train_loss}\\t Train Acc : {train_accuracy}% \\t Val Loss : {val_loss}\\t Val Acc : {val_accuracy}%\")\n",
    "        if wandb_logging:\n",
    "            wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f32dff-d6d8-465f-a75a-c878d3d4bac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0bf6186-c346-41f5-b23a-70ac8dec092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, word, language_processor,device = \"cpu\"):\n",
    "\n",
    "    lp = language_processor\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        train = 0\n",
    "\n",
    "        if encoder.training and decoder.training: ## reset the the model back to train mode\n",
    "            train = 1\n",
    "\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        input_tensor = torch.tensor(lp.encode_word(word,lp.source_lang,padding=False,append_eos = True)).to(device).view(1,-1)\n",
    "\n",
    "        batch_size = 1\n",
    "\n",
    "        if encoder.rnn_type == \"LSTM\":\n",
    "                encoder_hidden = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "                encoder_cell = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "        else:\n",
    "            encoder_hidden = None\n",
    "            encoder_cell = None\n",
    "        \n",
    "\n",
    "        encoder_hidden_contexts, encoder_last_hidden, encoder_cell = encoder(input_tensor,encoder_hidden,encoder_cell)\n",
    "\n",
    "        \"\"\"if encoder_hidden.shape[0] != decoder.expected_h0_dim1:\n",
    "            reshaped_encoder_hidden = encoder_hidden.repeat(decoder.expected_h0_dim1,1,1)\n",
    "        else:\n",
    "            reshaped_encoder_hidden = encoder_hidden\"\"\"\n",
    "\n",
    "        #print(encoder_hidden.shape,encoder_hidden.shape)\n",
    "\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_hidden_contexts, encoder_last_hidden,encoder_cell ,eval_mode = True,target_tensor = None)\n",
    "\n",
    "        output_size = len(list(train_lp.target_char2id.keys()))\n",
    "        decoder_outputs = decoder_outputs.view(28,-1)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_chars = []\n",
    "        \"\"\"for idx in decoded_ids:\n",
    "            if idx.item() == end_token_id:\n",
    "                break\n",
    "            decoded_chars.append(lp.target_id2char[idx.item()])\"\"\"\n",
    "\n",
    "        decoded_word = lp.decode_word(decoded_ids.cpu().numpy(),lp.target_lang)\n",
    "\n",
    "    if train:\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "    \n",
    "    return decoded_word, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce503bff-3de9-49b3-beec-16fa599b10d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22585702-e9b2-4dcb-8260-45a67f5ee101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_start_expt(config,wandb_log = True,kaggle=False):\n",
    "    \n",
    "    batch_size = config['batch_size']\n",
    "    target_lang = \"tel\"\n",
    "\n",
    "    if kaggle:\n",
    "        base_dir = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        base_dir = \"aksharantar_sampled/\"\n",
    "        device = torch.device(\"mps\")\n",
    "\n",
    "    use_meta_tokens = True\n",
    "    append_eos = 1\n",
    "    \n",
    "    lang_dir = base_dir + target_lang + \"/\"\n",
    "    \n",
    "    ##creating train loader\n",
    "    train_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"train\",meta_tokens=use_meta_tokens)\n",
    "    ## the ids of these tokens are the same in the source and target language\n",
    "    start_token_id = train_lp.source_char2id['<start>']\n",
    "    end_token_id = train_lp.source_char2id['<end>']\n",
    "    pad_token_id = train_lp.source_char2id['<pad>']\n",
    "\n",
    "    collate_fn_ptr = partial(collate_fn,pad_token_id=pad_token_id,device=device)\n",
    "    \n",
    "    train_dataset = WordDataset(train_lp,device=device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,collate_fn=collate_fn_ptr, shuffle=True)\n",
    "    \n",
    "    ## creating test loader\n",
    "    test_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"test\",meta_tokens=use_meta_tokens)\n",
    "    \n",
    "    ## to make sure that the same vocabulary and dictionaries are used everywhere\n",
    "    test_lp.source_char2id = train_lp.source_char2id\n",
    "    test_lp.source_id2char = train_lp.source_id2char\n",
    "    test_lp.target_char2id = train_lp.target_char2id\n",
    "    test_lp.target_id2char = train_lp.target_id2char\n",
    "    \n",
    "    test_dataset = WordDataset(test_lp,device=device)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,collate_fn=collate_fn_ptr, shuffle=True)\n",
    "    \n",
    "    ## creating validation loader\n",
    "    valid_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"valid\",meta_tokens=use_meta_tokens)\n",
    "    valid_lp.source_char2id = train_lp.source_char2id\n",
    "    valid_lp.source_id2char = train_lp.source_id2char\n",
    "    valid_lp.target_char2id = train_lp.target_char2id\n",
    "    valid_lp.target_id2char = train_lp.target_id2char\n",
    "    \n",
    "    valid_dataset = WordDataset(valid_lp,device=device)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size,collate_fn=collate_fn_ptr, shuffle=True)\n",
    "    \n",
    "    ##in principle these are all fixed across train/test/valid data\n",
    "    \n",
    "    #source_max_len = train_lp.source_max_len\n",
    "    #target_max_len = train_lp.target_max_len\n",
    "    \n",
    "    source_vocab_size = len(list(train_lp.source_char2id.keys()))\n",
    "    target_vocab_size = len(list(train_lp.target_char2id.keys()))\n",
    "    \n",
    "    hidden_size = config['hidden_size']\n",
    "    embedding_size = hidden_size\n",
    "    \n",
    "    epochs = config['epochs']\n",
    "    \n",
    "    optimiser = config['optimiser']\n",
    "    \n",
    "    weight_decay = config['weight_decay']\n",
    "    \n",
    "    lr = config['lr']\n",
    "    \n",
    "    num_encoder_layers = config['num_layers']\n",
    "    num_decoder_layers = num_encoder_layers\n",
    "    \n",
    "    ## Allowed Values : \"GRU\"/\"RNN\"/\"LSTM\" (not case sensitive)\n",
    "    rnn_type = config['rnn_type'].upper()\n",
    "    \n",
    "    bidirectional = config['bidirectional']\n",
    "    teacher_forcing_ratio = config['teacher_forcing_ratio']\n",
    "\n",
    "    teacher_forcing = False\n",
    "    \n",
    "    if teacher_forcing_ratio>0:\n",
    "        teacher_forcing = True\n",
    "    \n",
    "    \n",
    "    #loss_criterion =  nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "    \n",
    "    dropout=config['dropout']\n",
    "    \n",
    "    encoder = Encoder(source_vocab_size = source_vocab_size, hidden_size = hidden_size,embedding_size=embedding_size,rnn_type = rnn_type,padding_idx=pad_token_id,num_layers=num_encoder_layers,bidirectional=bidirectional,dropout=dropout).to(device)\n",
    "    \n",
    "    decoder = Decoder(hidden_size = hidden_size,embedding_size=embedding_size, target_vocab_size = target_vocab_size,batch_size = batch_size,rnn_type = rnn_type, padding_idx = None,num_layers = num_decoder_layers,bidirectional = bidirectional,dropout=dropout,device=device).to(device)\n",
    "    \n",
    "    #train(train_loader,valid_loader, encoder, decoder, 3,loss_criterion=loss_criterion, print_every=3, plot_every=5,device=device,teacher_forcing = teacher_forcing,teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "    train(train_loader,valid_loader, encoder, decoder, epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
    "\n",
    "    return encoder,decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07abf5c3-359d-4ca9-8f98-f99aad4c1199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc17b553b1145f1a43b6168ebd680c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fa2eb21f16f44caae85717118b371f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 82\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"config = {\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m    'hidden_size':128,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m        \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m        'weight_decay': 1e-5\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m}\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \n\u001b[1;32m     80\u001b[0m }\n\u001b[0;32m---> 82\u001b[0m encoder,decoder \u001b[38;5;241m=\u001b[39m \u001b[43msetup_and_start_expt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwandb_log\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mkaggle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 95\u001b[0m, in \u001b[0;36msetup_and_start_expt\u001b[0;34m(config, wandb_log, kaggle)\u001b[0m\n\u001b[1;32m     92\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(hidden_size \u001b[38;5;241m=\u001b[39m hidden_size,embedding_size\u001b[38;5;241m=\u001b[39membedding_size, target_vocab_size \u001b[38;5;241m=\u001b[39m target_vocab_size,batch_size \u001b[38;5;241m=\u001b[39m batch_size,rnn_type \u001b[38;5;241m=\u001b[39m rnn_type, padding_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,num_layers \u001b[38;5;241m=\u001b[39m num_decoder_layers,bidirectional \u001b[38;5;241m=\u001b[39m bidirectional,dropout\u001b[38;5;241m=\u001b[39mdropout,device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m#train(train_loader,valid_loader, encoder, decoder, 3,loss_criterion=loss_criterion, print_every=3, plot_every=5,device=device,teacher_forcing = teacher_forcing,teacher_forcing_ratio=teacher_forcing_ratio)\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mteacher_forcing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing\u001b[49m\u001b[43m,\u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwandb_logging\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwandb_log\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoder,decoder\n",
      "Cell \u001b[0;32mIn[9], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, valid_loader, encoder, decoder, n_epochs, padding_idx, optimiser, loss, weight_decay, lr, teacher_forcing, teacher_forcing_ratio, print_every, plot_every, device, wandb_logging)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#criterion = loss_criterion.to(device)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m---> 31\u001b[0m     train_loss,train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     print_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n\u001b[1;32m     33\u001b[0m     plot_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n",
      "Cell \u001b[0;32mIn[7], line 50\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio, ignore_padding, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m     tot_words \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_words\n\u001b[1;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     47\u001b[0m     decoder_outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, decoder_outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     48\u001b[0m     target_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     53\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"config = {\n",
    "    'hidden_size':128,\n",
    "        \n",
    "        'embedding_size':128,\n",
    "\n",
    "        'rnn_type' : \"gru\",\n",
    "        \n",
    "        'batch_size':256,\n",
    "        \n",
    "        'optimiser': \"adam\",\n",
    "\n",
    "        'num_layers' : 2,\n",
    "\n",
    "        'lr':  1e-3,\n",
    "\n",
    "        'dropout' : 0.1,\n",
    "        \n",
    "        'epochs' : 15,\n",
    "\n",
    "        'teacher_forcing_ratio' : 0.3,\n",
    "\n",
    "        'bidirectional' :  True,\n",
    "    \n",
    "        'weight_decay': 0\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'hidden_size':512,\n",
    "        \n",
    "        'embedding_size':512,\n",
    "\n",
    "        'rnn_type' : \"lstm\",\n",
    "        \n",
    "        'batch_size':64,\n",
    "        \n",
    "        'optimiser': \"nadam\",\n",
    "\n",
    "        'num_layers' : 3,\n",
    "\n",
    "        'lr':  1e-3,\n",
    "\n",
    "        'dropout' : 0.4,\n",
    "        \n",
    "        'epochs' : 15,\n",
    "\n",
    "        'teacher_forcing_ratio' : 0.4,\n",
    "\n",
    "        'bidirectional' :  True,\n",
    "    \n",
    "        'weight_decay': 1e-5\n",
    "}\"\"\"\n",
    "\n",
    "config = {\n",
    "\n",
    "        'batch_size':64,\n",
    "\n",
    "        'bidirectional' :  True,\n",
    "\n",
    "        'dropout' : 0.4,\n",
    "\n",
    "        'embedding_size':128,\n",
    "\n",
    "        'epochs' : 1,\n",
    "\n",
    "        'hidden_size':512,\n",
    "\n",
    "        'lr':  3e-4,\n",
    "\n",
    "        'num_layers' : 4,\n",
    "\n",
    "        'optimiser': \"nadam\",\n",
    "\n",
    "        'rnn_type' : \"lstm\",\n",
    "\n",
    "\n",
    "        'teacher_forcing_ratio' : 0.4,\n",
    "\n",
    "        'weight_decay': 1e-5\n",
    "\n",
    "}\n",
    "\n",
    "encoder,decoder = setup_and_start_expt(config,wandb_log = False,kaggle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45761b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf6c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a5cac-1e5f-4ed7-95bc-2b30cf58a0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92c7fb-0e80-4018-a93c-c441564e7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"\")\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'name' : 'PA3 Hyper Sweep GRU',\n",
    "    'metric': {\n",
    "      'name': 'Validation accuracy',\n",
    "      'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        \n",
    "         'hidden_size':{\n",
    "            'values':[128]#[64,128,256,512]\n",
    "        },\n",
    "        \n",
    "        'embedding_size':{\n",
    "            'values':[128]#[64,128,256,512]\n",
    "        },\n",
    "\n",
    "        'rnn_type':{\n",
    "            'values':[\"gru\"]#['lstm','rnn','gru']\n",
    "        },\n",
    "        \n",
    "        'batch_size':{\n",
    "            'values':[64]#[32,64,128,256]\n",
    "        },\n",
    "        \n",
    "        'optimiser': {\n",
    "            'values': [\"adam\"]#,\"rmsprop\",\"nadam\"]\n",
    "        },\n",
    "\n",
    "        'num_layers' :{\n",
    "            'values' : [2]#[1,2,3,4,5]\n",
    "        },\n",
    "\n",
    "        'lr': {\n",
    "            'values': [1e-3]#[1e-2,1e-3,1e-4,3e-4]\n",
    "        },\n",
    "\n",
    "        'dropout' : {\n",
    "\n",
    "            'values' : [0.1]#[0,0.1,0.2,0.3,0.4]\n",
    "        },\n",
    "        \n",
    "        'epochs' : {\n",
    "\n",
    "            'values' : [15]\n",
    "        },\n",
    "\n",
    "        'teacher_forcing_ratio' : {\n",
    "            'values' : [0.3]#[0,0.1,0.2,0.3,0.4,0.5]\n",
    "        },\n",
    "\n",
    "        'bidirectional' : {\n",
    "            'values' : [True]#[True,False]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0,1e-3,]#,1e-3,5e-3,5e-4]\n",
    "        },\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project='JV_CS23M036_TEJASVI_DL_ASSIGNMENT3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb1380-b7e4-4744-a8ef-3b720852639b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45ea38-320f-4939-bfed-0362b1ecd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    WandB calls main function each time with differnet combination.\n",
    "\n",
    "    We can retrive the same and use the same values for our hypermeters.\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    with wandb.init() as run:\n",
    "\n",
    "        run_name=\"-hl_\"+str(wandb.config.num_layers)+\"-hs_\"+str(wandb.config.hidden_size)+\"-es_\"+str(wandb.config.hidden_size)+\"-biDir_\"+str(wandb.config.bidirectional)\n",
    "\n",
    "        run_name = run_name+\"-rnn_type_\"+str(wandb.config.rnn_type)+run_name+\"-optim_\"+str(wandb.config.optimiser)+\"-lr_\"+str(wandb.config.lr)+\"-reg_\"+str(wandb.config.weight_decay)+\"-epochs_\"+str(wandb.config.epochs)+\"-tf_ratio_\"+str(wandb.config.teacher_forcing_ratio)\n",
    "\n",
    "        run_name = run_name+\"-dropout_\"+str(wandb.config.dropout)\n",
    "\n",
    "        wandb.run.name=run_name\n",
    "\n",
    "        setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, function=main,count=400) # calls main function for count number of times.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0c784-f46f-4e4f-ae48-55a3be347e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"op1,_ = evaluate(encoder, decoder, word=\"srirama\", language_processor=train_lp,device = device)\n",
    "\n",
    "op2,_ = evaluate(encoder, decoder, word=\"tejasvi\", language_processor=train_lp,device = device)\n",
    "\n",
    "op1_string = \"\".join(op1)\n",
    "print(op1_string[:8])\n",
    "\n",
    "op2_string = \"\".join(op2)\n",
    "print(op2_string[:7])\n",
    "\n",
    "print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a6d6e-bde3-46d6-b858-95fd8768a3ff",
   "metadata": {},
   "source": [
    "### JV\n",
    "\n",
    "###### To Do:\n",
    "\n",
    "1. Add LSTM,RNN support : Train\n",
    "2. Model parameter Initialization.\n",
    "3. Attention.\n",
    "4. Now create a seq2seq class, specify attention = True for attention to work.\n",
    "\n",
    "Hyper Params:\n",
    "\n",
    "1. Batch size\n",
    "2. Hidden layer size\n",
    "3. Embedding size\n",
    "4. number of encoder layers\n",
    "5. number of decoder layers\n",
    "6. bidirectional\n",
    "7. tf ratio ; 0/0.1/0.2/0.3/0.5\n",
    "8. Optimizer\n",
    "9. Learning Rate\n",
    "10. Batch size\n",
    "11. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e908c4f-af04-4ff8-95e4-875c6e5a156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dropout within GR/LSTM/RNN could be applied only when num_layers>1, so if we have 1 layer dropout of 0 is applied.\n",
    "However, the specified dropout is applied to embedding.\n",
    "\n",
    "considering encoder_layers = decoder_layers = num_layers.\n",
    "\n",
    "considering embedding_size = hidden_size.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
