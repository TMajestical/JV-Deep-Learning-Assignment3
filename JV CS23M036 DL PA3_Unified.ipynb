{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0cd007-c07d-41ae-a7d1-e8a396936a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75927828-0e72-4da3-a840-98d46c1e6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#!python3 -m pip install wandb\n",
    "import wandb\n",
    "\n",
    "from Core_Utils import *\n",
    "\n",
    "from Encoder_Decoder_Architecture import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d84851-1726-4bfb-955d-209cdc80bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d55ba868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineTranslator:\n",
    "\n",
    "    \"\"\"\n",
    "    The class that instantiates the encoder-decoder architecture and brings all methods relevant for training, computing accuracy and evaluation here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,source_vocab_size,target_vocab_size,hidden_size,embedding_size,rnn_type,batch_size,pad_token_id,dropout,num_layers,bidirectional,use_attention,device):\n",
    "\n",
    "        \"\"\"\n",
    "        The constructor of the class.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            source_vocab_size : The vocabulary size of the source language.\n",
    "            target_vocab_size : The vocabulary size of the target language.\n",
    "            hidden_size : The dimension of the hidden state of the recurrent cell.\n",
    "            embedding_size : The dimension of the embedding used.\n",
    "            rnn_type : \"GRU\"/\"LSTM\"/\"RNN\", case INsensitive. Default : \"GRU\".\n",
    "            batch_size : The batch size used for training. This is needed to resize dimensions in the BahdanauAttention's forward pass.\n",
    "            pad_token_id : The id corresponding to the token <pad>.\n",
    "            dropout : Droput probability. Encoder and Decoder by default use a dropout of 0.1, unless specified otherwise.\n",
    "            num_layers(int) : The number of encoder (recurrence unit) layers. Default : 1\n",
    "            bidirectional : True/False. If True, encoding is done by parsing input L->R and R->L, hence doubling the hiddenstate size. Default False.\n",
    "            use_attention : Boolean variable, default True, indicating to make use of BahdanauAttention.\n",
    "\n",
    "            Note : hidden_size,embedding_size,dopout, num_layers,bidirectional, rnn_type. These parameters are consistent across the encoder and decoder.\n",
    "                    However, the code supports use of different values.\n",
    "        \n",
    "        Returns:\n",
    "\n",
    "            None.\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        \n",
    "        self.encoder = Encoder(source_vocab_size = source_vocab_size, hidden_size = hidden_size,embedding_size=embedding_size,rnn_type = rnn_type,padding_idx=pad_token_id,num_layers=num_layers,bidirectional=bidirectional,dropout=dropout).to(self.device)\n",
    "    \n",
    "        self.decoder = Decoder(hidden_size = hidden_size,embedding_size=embedding_size, target_vocab_size = target_vocab_size,batch_size = batch_size,rnn_type = rnn_type,use_attention = use_attention, padding_idx = pad_token_id,num_layers = num_layers,bidirectional = bidirectional,dropout=dropout,device=self.device).to(self.device)\n",
    "\n",
    "        \n",
    "    def train_epoch(self,train_loader, encoder, decoder, encoder_optim,decoder_optim, loss_criterion,teacher_forcing_ratio,ignore_padding=True,device='cpu'):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to train the encoder-decoder model for 1 epoch.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            train_loader : The dataloader object (which wraps around WordDataset object of Core_Utils) corresponding to the traindata.\n",
    "            encoder : The encoder model object.\n",
    "            decoder : The decoder model object.\n",
    "            encoder_optim : A torch optim object, corresponding to the optimizer of encoder.\n",
    "            decoder_optim : A torch optim object, corresponding to the optimizer of decoder.\n",
    "            loss_criterion : The loss criterion\n",
    "            teacher_forcing_ratio : The teacher forcing ratio to be used.\n",
    "            ignore_padding : True, by default.\n",
    "            device : CPU by default.\n",
    "\n",
    "        Returns:\n",
    "            Loss and accuracy of the current epoch.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        tot_correct_word_preds = 0\n",
    "        tot_words = 0\n",
    "        epoch_loss = 0\n",
    "\n",
    "        for data in tqdm(train_loader):\n",
    "\n",
    "            input_tensor, target_tensor,_,_ = data\n",
    "\n",
    "            encoder_optim.zero_grad()\n",
    "            decoder_optim.zero_grad()\n",
    "\n",
    "            batch_size = data[0].shape[0]\n",
    "\n",
    "            if encoder.rnn_type == \"LSTM\":\n",
    "                encoder_hidden = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "                encoder_cell = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "            else:\n",
    "                encoder_hidden = None\n",
    "                encoder_cell = None\n",
    "            \n",
    "\n",
    "            encoder_hidden_contexts, encoder_last_hidden, encoder_cell = encoder(input_tensor,encoder_hidden,encoder_cell)\n",
    "            \n",
    "            decoder_outputs, _, _ = decoder(encoder_hidden_contexts, encoder_last_hidden,encoder_cell, target_tensor=target_tensor,teacher_forcing_ratio = teacher_forcing_ratio)\n",
    "\n",
    "            multi_step_preds = torch.argmax(decoder_outputs,dim=2)\n",
    "            multi_step_pred_correctness = (multi_step_preds ==  target_tensor)\n",
    "            num_words = multi_step_preds.shape[0]\n",
    "            \n",
    "            if ignore_padding: ## if padding has to be ignored.\n",
    "\n",
    "                ## for each word, based on the padding token ID, find the first occurance of the padding token, marking the begining of padding.\n",
    "                ## argmax is not supported for bool on cuda, hence casting it to long.\n",
    "                padding_start = torch.argmax((target_tensor == train_loader.dataset.pad_token_id).to(torch.long),dim=1).to(device)\n",
    "                ## Creating a mask with 1's in each position of a padding token\n",
    "                mask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\n",
    "                \n",
    "                ##doing a logical OR with the mask makes sure that the padding tokens do not affect the correctness of the word\n",
    "                tot_correct_word_preds += (torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum()).item()\n",
    "                tot_words += num_words\n",
    "\n",
    "            loss = loss_criterion(\n",
    "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                target_tensor.view(-1)\n",
    "            )\n",
    "            loss.backward()\n",
    "\n",
    "            encoder_optim.step()\n",
    "            decoder_optim.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        epoch_loss = round(epoch_loss / len(train_loader),4)\n",
    "        epoch_accuracy = round(tot_correct_word_preds*100/tot_words,2)\n",
    "\n",
    "        return epoch_loss,epoch_accuracy\n",
    "    \n",
    "    def train(self,train_loader,valid_loader, encoder, decoder, epochs,padding_idx,optimiser = \"adam\",loss=\"crossentropy\",weight_decay=0, lr=0.001,teacher_forcing_ratio = 0,device='cpu',wandb_logging = False):\n",
    "\n",
    "        \"\"\"\n",
    "        The method to train the encoder-decoder model. Makes use of other methods like train_epoch, compute_accuracy to train and return the accuracy.\n",
    "\n",
    "        train_loader : The dataloader object (which wraps around WordDataset object of Core_Utils) corresponding to the traindata.\n",
    "        valid_loader : The dataloader object (which wraps around WordDataset object of Core_Utils) corresponding to the Validaiton data.\n",
    "        encoder : The encoder model object.\n",
    "        decoder : The decoder model object. \n",
    "        epochs : Number of epochs of training.\n",
    "        padding_idx : The index corresponding to the <pad> token.\n",
    "        optimiser : The optimiser used for training, \"adam\"/\"nadam\"/\"rmsprop\", default : \"adam\". (Case sensitive)\n",
    "        loss : The loss function, only \"crossentropy\" is supported.\n",
    "        weight_decay : L2, regularization of encoder and decoder model weights.\n",
    "        lr : The learning rate, default is 0.001\n",
    "        teacher_forcing_ratio : Teacher forcing ratio, default is 0.\n",
    "        device : Default is CPU.\n",
    "        wandb_logging : Default is False.\n",
    "        \n",
    "        \"\"\"\n",
    "    \n",
    "        ## specify the optimiser\n",
    "        if optimiser.lower() == \"adam\":\n",
    "            encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "            decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "        elif optimiser.lower() == \"nadam\":\n",
    "            encoder_optimizer = optim.NAdam(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "            decoder_optimizer = optim.NAdam(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "        elif optimiser.lower() == \"rmsprop\":\n",
    "            encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "            decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "            \n",
    "        ## Specify the loss criteria\n",
    "        if loss.lower() == \"crossentropy\":\n",
    "            loss_criterion = nn.CrossEntropyLoss(ignore_index = padding_idx).to(device)\n",
    "\n",
    "        lp = train_loader.dataset.lp\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "\n",
    "            ## Train for 1 epoch.\n",
    "            train_loss,train_accuracy = self.train_epoch(train_loader, encoder, decoder, encoder_optimizer, decoder_optimizer, loss_criterion,teacher_forcing_ratio,device=device)\n",
    "\n",
    "            ## compute validation accuracy.\n",
    "            val_loss,_,val_accuracy = self.compute_accuracy(valid_loader,encoder,decoder,loss_criterion,ignore_padding=True,device=device)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}\\t Train Loss : {train_loss}\\t Train Acc : {train_accuracy}% \\t Val Loss : {val_loss}\\t Val Acc : {val_accuracy}%\")\n",
    "            if wandb_logging:\n",
    "                wandb.log({'epoch': epoch+1,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
    "\n",
    "    def compute_accuracy(self,dataloader,encoder,decoder,criterion,ignore_padding = True,device='cpu'):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to compute the accuracy using the model (encoder-decoder) using dataloader.\n",
    "\n",
    "        This method returns word and character level accuracy.\n",
    "\n",
    "            Word Level Accuracy : Accuracy is computed at the word level and a word is right iff every character is predicted correctly.\n",
    "            Char Level Accuracy : Accuracy is computed by comparing each predicted character wrt the correct char.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            dataloader : The train/test/valid dataloader.\n",
    "            encoder : The encoder \n",
    "            decoder : The decoder\n",
    "            padding_token_id : The id of the padding token.\n",
    "            ignore_padding : If True, then in word level accuracy, the padding characters are ignored in computing the word level accuracy.\n",
    "                            char level accuracy, the padding characters are not considered at all.\n",
    "\n",
    "                            If false, padding is considered to be a part of the word (for word level accuracy) and \n",
    "        \"\"\"\n",
    "\n",
    "        char_lvl_accuracy = 0\n",
    "        word_level_accuracy = 0\n",
    "\n",
    "        tot_chars = 0\n",
    "        tot_words = 0\n",
    "\n",
    "        tot_correct_char_preds = 0\n",
    "        tot_correct_word_preds = 0\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        #criterion = loss_criterion.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            train = 0\n",
    "\n",
    "            if encoder.training and decoder.training: ## reset the the model back to train mode\n",
    "                train = 1\n",
    "\n",
    "            encoder.eval()\n",
    "            decoder.eval()\n",
    "\n",
    "            for data in dataloader:\n",
    "                \n",
    "                input_tensor, target_tensor,_,target_max_len = data\n",
    "\n",
    "                batch_size = data[0].shape[0]\n",
    "        \n",
    "                if encoder.rnn_type == \"LSTM\":\n",
    "                    encoder_hidden = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "                    encoder_cell = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "                else:\n",
    "                    encoder_hidden = None\n",
    "                    encoder_cell = None\n",
    "                \n",
    "        \n",
    "                encoder_hidden_contexts, encoder_last_hidden, encoder_cell = encoder(input_tensor,encoder_hidden,encoder_cell)\n",
    "                ## even though we are passing target tensor, the teacher forcing ratio is 0, so no teacher forcing\n",
    "                decoder_outputs, _, _ = decoder(encoder_hidden_contexts, encoder_last_hidden,encoder_cell, target_tensor = target_tensor,teacher_forcing_ratio = 0)\n",
    "        \n",
    "                loss += criterion(decoder_outputs.view(-1, decoder_outputs.size(-1)), target_tensor.view(-1)).item()\n",
    "        \n",
    "                ## For a batch, for each character find the most probable output word.\n",
    "                multi_step_preds = torch.argmax(decoder_outputs,dim=2)\n",
    "                multi_step_pred_correctness = (multi_step_preds ==  target_tensor)\n",
    "                num_chars = multi_step_preds.numel() ##find the total number of characters in the current batch\n",
    "                num_words = multi_step_preds.shape[0] ##find the total number of words in the current batch.\n",
    "        \n",
    "                if ignore_padding: ## if padding has to be ignored.\n",
    "        \n",
    "                    ## for each word, based on the padding token ID, find the first occurance of the padding token, marking the begining of padding.\n",
    "                    \n",
    "                    ## argmax is not supported for bool on cuda, hence casting it to long.\n",
    "                    padding_start = torch.argmax((target_tensor == dataloader.dataset.pad_token_id).to(torch.long),dim=1).to(device)\n",
    "                    ## Creating a mask with 1's in each position of a padding token\n",
    "                    mask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\n",
    "                    #print(mask)\n",
    "                    \n",
    "                    ##doing a logical OR with the mask makes sure that the padding tokens do not affect the correctness of the word\n",
    "                    tot_correct_word_preds += (torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum()).item()\n",
    "                    tot_words += num_words\n",
    "        \n",
    "                    ##creating a complement of the mask so to mark padding tokens as irrelevant\n",
    "                    complement_mask = (1-mask.int()).bool()\n",
    "                    num_pad_chars = mask.int().sum().item()\n",
    "                    ##counting number of non_pad_chars to compute accuracy.\n",
    "                    num_non_pad_chars = num_chars - num_pad_chars\n",
    "        \n",
    "                    tot_correct_char_preds += (torch.logical_and(multi_step_pred_correctness,complement_mask).int().sum()).item()\n",
    "                    tot_chars += num_non_pad_chars\n",
    "                    \n",
    "            \n",
    "                else: ##otherwise.\n",
    "        \n",
    "                    tot_correct_word_preds += (torch.all(multi_step_pred_correctness,dim=1).int().sum()).item()\n",
    "                    tot_words += num_words\n",
    "                    \n",
    "                    tot_correct_char_preds += (multi_step_pred_correctness.int().sum()).item()\n",
    "                    tot_chars += num_chars\n",
    "\n",
    "            char_lvl_accuracy = round(tot_correct_char_preds*100/tot_chars,2)\n",
    "            word_lvl_accuracy = round(tot_correct_word_preds*100/tot_words,2)\n",
    "\n",
    "            loss /= dataloader.dataset.data.shape[0]\n",
    "\n",
    "            if train:\n",
    "\n",
    "                encoder.train()\n",
    "                decoder.train()\n",
    "        \n",
    "            return round(loss,4),char_lvl_accuracy,word_lvl_accuracy\n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f32dff-d6d8-465f-a75a-c878d3d4bac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0bf6186-c346-41f5-b23a-70ac8dec092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, word, language_processor,device = \"cpu\"):\n",
    "\n",
    "    lp = language_processor\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        train = 0\n",
    "\n",
    "        if encoder.training and decoder.training: ## reset the the model back to train mode\n",
    "            train = 1\n",
    "\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        input_tensor = torch.tensor(lp.encode_word(word,lp.source_lang,padding=False,append_eos = True)).to(device).view(1,-1)\n",
    "\n",
    "        batch_size = 1\n",
    "\n",
    "        if encoder.rnn_type == \"LSTM\":\n",
    "                encoder_hidden = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "                encoder_cell = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "        else:\n",
    "            encoder_hidden = None\n",
    "            encoder_cell = None\n",
    "        \n",
    "\n",
    "        encoder_hidden_contexts, encoder_last_hidden, encoder_cell = encoder(input_tensor,encoder_hidden,encoder_cell)\n",
    "\n",
    "        \"\"\"if encoder_hidden.shape[0] != decoder.expected_h0_dim1:\n",
    "            reshaped_encoder_hidden = encoder_hidden.repeat(decoder.expected_h0_dim1,1,1)\n",
    "        else:\n",
    "            reshaped_encoder_hidden = encoder_hidden\"\"\"\n",
    "\n",
    "        #print(encoder_hidden.shape,encoder_hidden.shape)\n",
    "\n",
    "        decoder_outputs, _, attentions = decoder(encoder_hidden_contexts, encoder_last_hidden,encoder_cell ,eval_mode = True,target_tensor = None)\n",
    "\n",
    "        output_size = len(list(lp.target_char2id.keys()))\n",
    "        decoder_outputs = decoder_outputs.view(30,-1)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_chars = []\n",
    "        \"\"\"for idx in decoded_ids:\n",
    "            if idx.item() == end_token_id:\n",
    "                break\n",
    "            decoded_chars.append(lp.target_id2char[idx.item()])\"\"\"\n",
    "\n",
    "        decoded_word = lp.decode_word(decoded_ids.cpu().numpy(),lp.target_lang)\n",
    "\n",
    "    if train:\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "    \n",
    "    return decoded_word, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22585702-e9b2-4dcb-8260-45a67f5ee101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_start_expt(config,wandb_log = True,kaggle=False):\n",
    "    \n",
    "    batch_size = config['batch_size']\n",
    "    target_lang = \"tel\"\n",
    "\n",
    "    if kaggle:\n",
    "        base_dir = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        base_dir = \"aksharantar_sampled/\"\n",
    "        device = torch.device(\"mps\")\n",
    "\n",
    "    use_meta_tokens = True\n",
    "    append_eos = 1\n",
    "    \n",
    "    lang_dir = base_dir + target_lang + \"/\"\n",
    "    \n",
    "    ##creating train loader\n",
    "    train_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"train\",meta_tokens=use_meta_tokens)\n",
    "    ## the ids of these tokens are the same in the source and target language\n",
    "    start_token_id = train_lp.source_char2id['<start>']\n",
    "    end_token_id = train_lp.source_char2id['<end>']\n",
    "    pad_token_id = train_lp.source_char2id['<pad>']\n",
    "\n",
    "    collate_fn_ptr = partial(collate_fn,pad_token_id=pad_token_id,device=device)\n",
    "    \n",
    "    train_dataset = WordDataset(train_lp,device=device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,collate_fn=collate_fn_ptr, shuffle=True)\n",
    "    \n",
    "    ## creating test loader\n",
    "    test_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"test\",meta_tokens=use_meta_tokens)\n",
    "    \n",
    "    ## to make sure that the same vocabulary and dictionaries are used everywhere\n",
    "    test_lp.source_char2id = train_lp.source_char2id\n",
    "    test_lp.source_id2char = train_lp.source_id2char\n",
    "    test_lp.target_char2id = train_lp.target_char2id\n",
    "    test_lp.target_id2char = train_lp.target_id2char\n",
    "    \n",
    "    test_dataset = WordDataset(test_lp,device=device)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,collate_fn=collate_fn_ptr, shuffle=True)\n",
    "    \n",
    "    ## creating validation loader\n",
    "    valid_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"valid\",meta_tokens=use_meta_tokens)\n",
    "    valid_lp.source_char2id = train_lp.source_char2id\n",
    "    valid_lp.source_id2char = train_lp.source_id2char\n",
    "    valid_lp.target_char2id = train_lp.target_char2id\n",
    "    valid_lp.target_id2char = train_lp.target_id2char\n",
    "    \n",
    "    valid_dataset = WordDataset(valid_lp,device=device)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size,collate_fn=collate_fn_ptr, shuffle=True)\n",
    "    \n",
    "    ##in principle these are all fixed across train/test/valid data\n",
    "    \n",
    "    #source_max_len = train_lp.source_max_len\n",
    "    #target_max_len = train_lp.target_max_len\n",
    "    \n",
    "    source_vocab_size = len(list(train_lp.source_char2id.keys()))\n",
    "    target_vocab_size = len(list(train_lp.target_char2id.keys()))\n",
    "    \n",
    "    hidden_size = config['hidden_size']\n",
    "    embedding_size = hidden_size\n",
    "    \n",
    "    epochs = config['epochs']\n",
    "    \n",
    "    optimiser = config['optimiser']\n",
    "    \n",
    "    weight_decay = config['weight_decay']\n",
    "    \n",
    "    lr = config['lr']\n",
    "    \n",
    "    num_layers = config['num_layers']\n",
    "    #num_decoder_layers = num_encoder_layers\n",
    "    \n",
    "    ## Allowed Values : \"GRU\"/\"RNN\"/\"LSTM\" (not case sensitive)\n",
    "    rnn_type = config['rnn_type'].upper()\n",
    "    \n",
    "    bidirectional = config['bidirectional']\n",
    "    teacher_forcing_ratio = config['teacher_forcing_ratio']\n",
    "\n",
    "    dropout=config['dropout']\n",
    "\n",
    "    use_attention = config['use_attention']\n",
    "    \n",
    "    #encoder = Encoder(source_vocab_size = source_vocab_size, hidden_size = hidden_size,embedding_size=embedding_size,rnn_type = rnn_type,padding_idx=pad_token_id,num_layers=num_encoder_layers,bidirectional=bidirectional,dropout=dropout).to(device)\n",
    "    \n",
    "    #decoder = Decoder(hidden_size = hidden_size,embedding_size=embedding_size, target_vocab_size = target_vocab_size,batch_size = batch_size,rnn_type = rnn_type,use_attention = use_attention, padding_idx = None,num_layers = num_decoder_layers,bidirectional = bidirectional,dropout=dropout,device=device).to(device)\n",
    "    \n",
    "    model = MachineTranslator(source_vocab_size,target_vocab_size,hidden_size,embedding_size,rnn_type,batch_size,pad_token_id,dropout,num_layers,bidirectional,use_attention,device)\n",
    "\n",
    "    #train(train_loader,valid_loader, encoder, decoder, 3,loss_criterion=loss_criterion, print_every=3, plot_every=5,device=device,teacher_forcing = teacher_forcing,teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "    model.train(train_loader,valid_loader,model.encoder, model.decoder, epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07abf5c3-359d-4ca9-8f98-f99aad4c1199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5026e7da4443ad99399c871db5d1f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2519cd7db4194c40ac790a9dc999f370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"config = {\n",
    "    'hidden_size':128,\n",
    "        \n",
    "        'embedding_size':128,\n",
    "\n",
    "        'rnn_type' : \"gru\",\n",
    "        \n",
    "        'batch_size':256,\n",
    "        \n",
    "        'optimiser': \"adam\",\n",
    "\n",
    "        'num_layers' : 2,\n",
    "\n",
    "        'lr':  1e-3,\n",
    "\n",
    "        'dropout' : 0.1,\n",
    "        \n",
    "        'epochs' : 15,\n",
    "\n",
    "        'teacher_forcing_ratio' : 0.3,\n",
    "\n",
    "        'bidirectional' :  True,\n",
    "    \n",
    "        'weight_decay': 0\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'hidden_size':512,\n",
    "        \n",
    "        'embedding_size':512,\n",
    "\n",
    "        'rnn_type' : \"lstm\",\n",
    "        \n",
    "        'batch_size':64,\n",
    "        \n",
    "        'optimiser': \"nadam\",\n",
    "\n",
    "        'num_layers' : 3,\n",
    "\n",
    "        'lr':  1e-3,\n",
    "\n",
    "        'dropout' : 0.4,\n",
    "        \n",
    "        'epochs' : 15,\n",
    "\n",
    "        'teacher_forcing_ratio' : 0.4,\n",
    "\n",
    "        'bidirectional' :  True,\n",
    "    \n",
    "        'weight_decay': 1e-5\n",
    "}\"\"\"\n",
    "\n",
    "config = {\n",
    "\n",
    "        'batch_size':64,\n",
    "\n",
    "        'bidirectional' :  True,\n",
    "\n",
    "        'dropout' : 0.4,\n",
    "\n",
    "        'embedding_size':128,\n",
    "\n",
    "        'epochs' : 1,\n",
    "\n",
    "        'hidden_size':512,\n",
    "\n",
    "        'lr':  3e-4,\n",
    "\n",
    "        'num_layers' : 4,\n",
    "\n",
    "        'optimiser': \"nadam\",\n",
    "\n",
    "        'rnn_type' : \"lstm\",\n",
    "\n",
    "        'teacher_forcing_ratio' : 0.4,\n",
    "\n",
    "        'weight_decay': 1e-5,\n",
    "\n",
    "        'use_attention' : True\n",
    "\n",
    "}\n",
    "\n",
    "model = setup_and_start_expt(config,wandb_log = False,kaggle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66d10667",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "kaggle = False\n",
    "\n",
    "target_lang = \"tel\"\n",
    "\n",
    "if kaggle:\n",
    "    base_dir = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    base_dir = \"aksharantar_sampled/\"\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "use_meta_tokens = True\n",
    "append_eos = 1\n",
    "\n",
    "lang_dir = base_dir + target_lang + \"/\"\n",
    "\n",
    "\n",
    "##creating train loader\n",
    "train_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"train\",meta_tokens=use_meta_tokens)\n",
    "## the ids of these tokens are the same in the source and target language\n",
    "start_token_id = train_lp.source_char2id['<start>']\n",
    "end_token_id = train_lp.source_char2id['<end>']\n",
    "pad_token_id = train_lp.source_char2id['<pad>']\n",
    "\n",
    "collate_fn_ptr = partial(collate_fn,pad_token_id=pad_token_id,device=device)\n",
    "\n",
    "train_dataset = WordDataset(train_lp,device=device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,collate_fn=collate_fn_ptr, shuffle=True)\n",
    "\n",
    "## creating test loader\n",
    "test_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"test\",meta_tokens=use_meta_tokens)\n",
    "\n",
    "## to make sure that the same vocabulary and dictionaries are used everywhere\n",
    "test_lp.source_char2id = train_lp.source_char2id\n",
    "test_lp.source_id2char = train_lp.source_id2char\n",
    "test_lp.target_char2id = train_lp.target_char2id\n",
    "test_lp.target_id2char = train_lp.target_id2char\n",
    "\n",
    "test_dataset = WordDataset(test_lp,device=device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,collate_fn=collate_fn_ptr, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 2).to(device)\n",
    "\n",
    "test_loss,_,test_accuracy = compute_accuracy(test_loader,encoder,decoder,criterion,padding_token_id = test_lp.source_char2id['<pad>'],end_token_id = test_lp.source_char2id['<end>'],ignore_padding=True,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e45761b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrirama\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m decoded_word, attentions \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_lp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, word, language_processor, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"if encoder_hidden.shape[0] != decoder.expected_h0_dim1:\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    reshaped_encoder_hidden = encoder_hidden.repeat(decoder.expected_h0_dim1,1,1)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03melse:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    reshaped_encoder_hidden = encoder_hidden\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#print(encoder_hidden.shape,encoder_hidden.shape)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m decoder_outputs, _, attentions \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_hidden_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_last_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_cell\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43meval_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(lp\u001b[38;5;241m.\u001b[39mtarget_char2id\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     39\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m decoder_outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/OM NAMO VENKATESAYA/Jai Vigneshwara IIT MADRAS/JV SEM2/JV Deep Learning/JV Assignments/JV-Deep-Learning-Assignment3/Encoder_Decoder_Architecture.py:280\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, encoder_hidden_contexts, encoder_last_hidden, encoder_cell, target_tensor, eval_mode, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    277\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m target_tensor[:, step]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Teacher forcing\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;66;03m##greedily pick predictions, i.e pick the character corresponding to the hightest probability\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     _,preds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    283\u001b[0m decoder_outputs\u001b[38;5;241m.\u001b[39mappend(decoder_output)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "word = \"srirama\"\n",
    "\n",
    "decoded_word, attentions = evaluate(encoder, decoder, word, train_lp,device = \"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c483e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf6c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a5cac-1e5f-4ed7-95bc-2b30cf58a0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92c7fb-0e80-4018-a93c-c441564e7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"\")\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'name' : 'PA3 Hyper Sweep GRU',\n",
    "    'metric': {\n",
    "      'name': 'Validation accuracy',\n",
    "      'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        \n",
    "         'hidden_size':{\n",
    "            'values':[128]#[64,128,256,512]\n",
    "        },\n",
    "        \n",
    "        'embedding_size':{\n",
    "            'values':[128]#[64,128,256,512]\n",
    "        },\n",
    "\n",
    "        'rnn_type':{\n",
    "            'values':[\"gru\"]#['lstm','rnn','gru']\n",
    "        },\n",
    "        \n",
    "        'batch_size':{\n",
    "            'values':[64]#[32,64,128,256]\n",
    "        },\n",
    "        \n",
    "        'optimiser': {\n",
    "            'values': [\"adam\"]#,\"rmsprop\",\"nadam\"]\n",
    "        },\n",
    "\n",
    "        'num_layers' :{\n",
    "            'values' : [2]#[1,2,3,4,5]\n",
    "        },\n",
    "\n",
    "        'lr': {\n",
    "            'values': [1e-3]#[1e-2,1e-3,1e-4,3e-4]\n",
    "        },\n",
    "\n",
    "        'dropout' : {\n",
    "\n",
    "            'values' : [0.1]#[0,0.1,0.2,0.3,0.4]\n",
    "        },\n",
    "        \n",
    "        'epochs' : {\n",
    "\n",
    "            'values' : [15]\n",
    "        },\n",
    "\n",
    "        'teacher_forcing_ratio' : {\n",
    "            'values' : [0.3]#[0,0.1,0.2,0.3,0.4,0.5]\n",
    "        },\n",
    "\n",
    "        'bidirectional' : {\n",
    "            'values' : [True]#[True,False]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0,1e-3,]#,1e-3,5e-3,5e-4]\n",
    "        },\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project='JV_CS23M036_TEJASVI_DL_ASSIGNMENT3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb1380-b7e4-4744-a8ef-3b720852639b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45ea38-320f-4939-bfed-0362b1ecd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    WandB calls main function each time with differnet combination.\n",
    "\n",
    "    We can retrive the same and use the same values for our hypermeters.\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    with wandb.init() as run:\n",
    "\n",
    "        run_name=\"-hl_\"+str(wandb.config.num_layers)+\"-hs_\"+str(wandb.config.hidden_size)+\"-es_\"+str(wandb.config.hidden_size)+\"-biDir_\"+str(wandb.config.bidirectional)\n",
    "\n",
    "        run_name = run_name+\"-rnn_type_\"+str(wandb.config.rnn_type)+run_name+\"-optim_\"+str(wandb.config.optimiser)+\"-lr_\"+str(wandb.config.lr)+\"-reg_\"+str(wandb.config.weight_decay)+\"-epochs_\"+str(wandb.config.epochs)+\"-tf_ratio_\"+str(wandb.config.teacher_forcing_ratio)\n",
    "\n",
    "        run_name = run_name+\"-dropout_\"+str(wandb.config.dropout)\n",
    "\n",
    "        wandb.run.name=run_name\n",
    "\n",
    "        setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, function=main,count=400) # calls main function for count number of times.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0c784-f46f-4e4f-ae48-55a3be347e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"op1,_ = evaluate(encoder, decoder, word=\"srirama\", language_processor=train_lp,device = device)\n",
    "\n",
    "op2,_ = evaluate(encoder, decoder, word=\"tejasvi\", language_processor=train_lp,device = device)\n",
    "\n",
    "op1_string = \"\".join(op1)\n",
    "print(op1_string[:8])\n",
    "\n",
    "op2_string = \"\".join(op2)\n",
    "print(op2_string[:7])\n",
    "\n",
    "print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a6d6e-bde3-46d6-b858-95fd8768a3ff",
   "metadata": {},
   "source": [
    "### JV\n",
    "\n",
    "###### To Do:\n",
    "\n",
    "1. Add LSTM,RNN support : Train\n",
    "2. Model parameter Initialization.\n",
    "3. Attention.\n",
    "4. Now create a seq2seq class, specify attention = True for attention to work.\n",
    "\n",
    "Hyper Params:\n",
    "\n",
    "1. Batch size\n",
    "2. Hidden layer size\n",
    "3. Embedding size\n",
    "4. number of encoder layers\n",
    "5. number of decoder layers\n",
    "6. bidirectional\n",
    "7. tf ratio ; 0/0.1/0.2/0.3/0.5\n",
    "8. Optimizer\n",
    "9. Learning Rate\n",
    "10. Batch size\n",
    "11. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e908c4f-af04-4ff8-95e4-875c6e5a156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dropout within GR/LSTM/RNN could be applied only when num_layers>1, so if we have 1 layer dropout of 0 is applied.\n",
    "However, the specified dropout is applied to embedding.\n",
    "\n",
    "considering encoder_layers = decoder_layers = num_layers.\n",
    "\n",
    "considering embedding_size = hidden_size.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
