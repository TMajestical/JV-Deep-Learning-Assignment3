{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0cd007-c07d-41ae-a7d1-e8a396936a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75927828-0e72-4da3-a840-98d46c1e6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "from functools import partial\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#!python3 -m pip install wandb\n",
    "import wandb\n",
    "\n",
    "from Core_Utils import *\n",
    "\n",
    "from Encoder_Decoder_Architecture import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d84851-1726-4bfb-955d-209cdc80bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d39e84-a7e5-4344-af5d-ed91b8c5d2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f7d1cf-6309-45ae-b1c2-73609064339f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3698224-b989-4dfd-99a0-0007c176a5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c0c87-21d9-4609-9739-9df28e7844bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e291a-e0ae-453c-96aa-5e18bafe2173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d55ba868",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineTranslator:\n",
    "\n",
    "    \"\"\"\n",
    "    The class that instantiates the encoder-decoder architecture and brings all methods relevant for training, computing accuracy and evaluation here.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,source_vocab_size,target_vocab_size,hidden_size,embedding_size,rnn_type,batch_size,pad_token_id,dropout,num_layers,bidirectional,use_attention,device):\n",
    "\n",
    "        \"\"\"\n",
    "        The constructor of the class.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            source_vocab_size : The vocabulary size of the source language.\n",
    "            target_vocab_size : The vocabulary size of the target language.\n",
    "            hidden_size : The dimension of the hidden state of the recurrent cell.\n",
    "            embedding_size : The dimension of the embedding used.\n",
    "            rnn_type : \"GRU\"/\"LSTM\"/\"RNN\", case INsensitive. Default : \"GRU\".\n",
    "            batch_size : The batch size used for training. This is needed to resize dimensions in the BahdanauAttention's forward pass.\n",
    "            pad_token_id : The id corresponding to the token <pad>.\n",
    "            dropout : Droput probability. Encoder and Decoder by default use a dropout of 0.1, unless specified otherwise.\n",
    "            num_layers(int) : The number of encoder (recurrence unit) layers. Default : 1\n",
    "            bidirectional : True/False. If True, encoding is done by parsing input L->R and R->L, hence doubling the hiddenstate size. Default False.\n",
    "            use_attention : Boolean variable, default True, indicating to make use of BahdanauAttention.\n",
    "\n",
    "\n",
    "            Note : hidden_size,embedding_size,dopout, num_layers,bidirectional. These parameters are consistent across the encoder and decoder.\n",
    "        \n",
    "        Returns:\n",
    "\n",
    "            None.\n",
    "\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.encoder = Encoder(source_vocab_size = source_vocab_size, hidden_size = hidden_size,embedding_size=embedding_size,rnn_type = rnn_type,padding_idx=pad_token_id,num_layers=num_layers,bidirectional=bidirectional,dropout=dropout).to(self.device)\n",
    "    \n",
    "        self.decoder = Decoder(hidden_size = hidden_size,embedding_size=embedding_size, target_vocab_size = target_vocab_size,batch_size = batch_size,rnn_type = rnn_type,use_attention = use_attention, padding_idx = pad_token_id,num_layers = num_layers,bidirectional = bidirectional,dropout=dropout,device=self.device).to(self.device)\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c38af4-7bf7-4adf-ab8b-71ebb0b69416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,decoder_optimizer, criterion,teacher_forcing_ratio,ignore_padding=True,device='cpu'):\n",
    "\n",
    "    tot_correct_word_preds = 0\n",
    "    tot_words = 0\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for data in tqdm(dataloader):\n",
    "\n",
    "        input_tensor, target_tensor,_,_ = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        batch_size = data[0].shape[0]\n",
    "\n",
    "        if encoder.rnn_type == \"LSTM\":\n",
    "            encoder_hidden = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "            encoder_cell = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "        else:\n",
    "            encoder_hidden = None\n",
    "            encoder_cell = None\n",
    "        \n",
    "\n",
    "        encoder_hidden_contexts, encoder_last_hidden, encoder_cell = encoder(input_tensor,encoder_hidden,encoder_cell)\n",
    "        \n",
    "        decoder_outputs, _, _ = decoder(encoder_hidden_contexts, encoder_last_hidden,encoder_cell, target_tensor=target_tensor,teacher_forcing_ratio = teacher_forcing_ratio)\n",
    "\n",
    "        multi_step_preds = torch.argmax(decoder_outputs,dim=2)\n",
    "        multi_step_pred_correctness = (multi_step_preds ==  target_tensor)\n",
    "        num_words = multi_step_preds.shape[0]\n",
    "        \n",
    "        if ignore_padding: ## if padding has to be ignored.\n",
    "\n",
    "            ## for each word, based on the padding token ID, find the first occurance of the padding token, marking the begining of padding.\n",
    "            ## argmax is not supported for bool on cuda, hence casting it to long.\n",
    "            padding_start = torch.argmax((target_tensor == dataloader.dataset.pad_token_id).to(torch.long),dim=1).to(device)\n",
    "            ## Creating a mask with 1's in each position of a padding token\n",
    "            mask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\n",
    "            \n",
    "            ##doing a logical OR with the mask makes sure that the padding tokens do not affect the correctness of the word\n",
    "            tot_correct_word_preds += (torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum()).item()\n",
    "            tot_words += num_words\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss = round(epoch_loss / len(dataloader),4)\n",
    "    epoch_accuracy = round(tot_correct_word_preds*100/tot_words,2)\n",
    "\n",
    "    return epoch_loss,epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aee733-e313-47eb-b041-7af96d5a65db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e907afc-4b4b-4f7b-8e90-e605ba441e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(dataloader,encoder,decoder,criterion,padding_token_id,end_token_id = 1,ignore_padding = True,device='cpu'):\n",
    "\n",
    "    \"\"\"\n",
    "    Method to compute the accuracy using the model (encoder-decoder) using dataloader.\n",
    "\n",
    "    This method returns word and character level accuracy.\n",
    "\n",
    "        Word Level Accuracy : Accuracy is computed at the word level and a word is right iff every character is predicted correctly.\n",
    "        Char Level Accuracy : Accuracy is computed by comparing each predicted character wrt the correct char.\n",
    "\n",
    "    Params:\n",
    "\n",
    "        dataloader : The train/test/valid dataloader.\n",
    "        encoder : The encoder \n",
    "        decoder : The decoder\n",
    "        padding_token_id : The id of the padding token.\n",
    "        ignore_padding : If True, then in word level accuracy, the padding characters are ignored in computing the word level accuracy.\n",
    "                        char level accuracy, the padding characters are not considered at all.\n",
    "\n",
    "                        If false, padding is considered to be a part of the word (for word level accuracy) and \n",
    "    \"\"\"\n",
    "\n",
    "    char_lvl_accuracy = 0\n",
    "    word_level_accuracy = 0\n",
    "\n",
    "    tot_chars = 0\n",
    "    tot_words = 0\n",
    "\n",
    "    tot_correct_char_preds = 0\n",
    "    tot_correct_word_preds = 0\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    #criterion = loss_criterion.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        train = 0\n",
    "\n",
    "        if encoder.training and decoder.training: ## reset the the model back to train mode\n",
    "            train = 1\n",
    "\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        for data in dataloader:\n",
    "            \n",
    "            input_tensor, target_tensor,_,target_max_len = data\n",
    "\n",
    "            batch_size = data[0].shape[0]\n",
    "    \n",
    "            if encoder.rnn_type == \"LSTM\":\n",
    "                encoder_hidden = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "                encoder_cell = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "            else:\n",
    "                encoder_hidden = None\n",
    "                encoder_cell = None\n",
    "            \n",
    "    \n",
    "            encoder_hidden_contexts, encoder_last_hidden, encoder_cell = encoder(input_tensor,encoder_hidden,encoder_cell)\n",
    "            ## even though we are passing target tensor, the teacher forcing ratio is 0, so no teacher forcing\n",
    "            decoder_outputs, _, _ = decoder(encoder_hidden_contexts, encoder_last_hidden,encoder_cell, target_tensor = target_tensor,teacher_forcing_ratio = 0)\n",
    "    \n",
    "            loss += criterion(\n",
    "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                target_tensor.view(-1)).item()\n",
    "    \n",
    "            ## For a batch, for each character find the most probable output word.\n",
    "            multi_step_preds = torch.argmax(decoder_outputs,dim=2)\n",
    "            multi_step_pred_correctness = (multi_step_preds ==  target_tensor)\n",
    "            num_chars = multi_step_preds.numel() ##find the total number of characters in the current batch\n",
    "            num_words = multi_step_preds.shape[0] ##find the total number of words in the current batch.\n",
    "    \n",
    "            if ignore_padding: ## if padding has to be ignored.\n",
    "    \n",
    "                ## for each word, based on the padding token ID, find the first occurance of the padding token, marking the begining of padding.\n",
    "                \n",
    "                ## argmax is not supported for bool on cuda, hence casting it to long.\n",
    "                padding_start = torch.argmax((target_tensor == dataloader.dataset.pad_token_id).to(torch.long),dim=1).to(device)\n",
    "                ## Creating a mask with 1's in each position of a padding token\n",
    "                mask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\n",
    "                #print(mask)\n",
    "                \n",
    "                ##doing a logical OR with the mask makes sure that the padding tokens do not affect the correctness of the word\n",
    "                tot_correct_word_preds += (torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum()).item()\n",
    "                tot_words += num_words\n",
    "    \n",
    "                ##creating a complement of the mask so to mark padding tokens as irrelevant\n",
    "                complement_mask = (1-mask.int()).bool()\n",
    "                num_pad_chars = mask.int().sum().item()\n",
    "                ##counting number of non_pad_chars to compute accuracy.\n",
    "                num_non_pad_chars = num_chars - num_pad_chars\n",
    "    \n",
    "                tot_correct_char_preds += (torch.logical_and(multi_step_pred_correctness,complement_mask).int().sum()).item()\n",
    "                tot_chars += num_non_pad_chars\n",
    "                \n",
    "        \n",
    "            else: ##otherwise.\n",
    "    \n",
    "                tot_correct_word_preds += (torch.all(multi_step_pred_correctness,dim=1).int().sum()).item()\n",
    "                tot_words += num_words\n",
    "                \n",
    "                tot_correct_char_preds += (multi_step_pred_correctness.int().sum()).item()\n",
    "                tot_chars += num_chars\n",
    "\n",
    "        #print(tot_correct_char_preds,tot_chars)\n",
    "        #print(tot_correct_word_preds,tot_words)\n",
    "    \n",
    "        char_lvl_accuracy = round(tot_correct_char_preds*100/tot_chars,2)\n",
    "        word_lvl_accuracy = round(tot_correct_word_preds*100/tot_words,2)\n",
    "\n",
    "        loss /= dataloader.dataset.data.shape[0]\n",
    "\n",
    "        if train:\n",
    "\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "    \n",
    "        return round(loss,4),char_lvl_accuracy,word_lvl_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f149d-39ce-46ef-9b72-3cb24a57d7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33e86bf-bdef-4939-9f94-66c10e15665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader,valid_loader, encoder, decoder, n_epochs,padding_idx,optimiser = \"adam\",loss=\"crossentropy\",weight_decay=0, lr=0.001,teacher_forcing = False,teacher_forcing_ratio = 0,device='cpu',wandb_logging = False):\n",
    "    \n",
    "    ## specify the optimiser\n",
    "    if optimiser.lower() == \"adam\":\n",
    "        encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "    elif optimiser.lower() == \"nadam\":\n",
    "        encoder_optimizer = optim.NAdam(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        decoder_optimizer = optim.NAdam(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "    elif optimiser.lower() == \"rmsprop\":\n",
    "        encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        \n",
    "    ## Specify the loss criteria\n",
    "    if loss.lower() == \"crossentropy\":\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index = padding_idx).to(device)\n",
    "\n",
    "    lp = train_dataloader.dataset.lp\n",
    "    \n",
    "    #criterion = loss_criterion.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "\n",
    "        ## Train for 1 epoch.\n",
    "        train_loss,train_accuracy = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,teacher_forcing_ratio,device=device)\n",
    "        ## compute validation accuracy.\n",
    "        val_loss,_,val_accuracy = compute_accuracy(valid_loader,encoder,decoder,criterion,padding_token_id = lp.source_char2id['<pad>'],end_token_id = lp.source_char2id['<end>'],ignore_padding=True,device=device)\n",
    "\n",
    "        print(f\"Epoch {epoch}\\t Train Loss : {train_loss}\\t Train Acc : {train_accuracy}% \\t Val Loss : {val_loss}\\t Val Acc : {val_accuracy}%\")\n",
    "        if wandb_logging:\n",
    "            wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f32dff-d6d8-465f-a75a-c878d3d4bac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0bf6186-c346-41f5-b23a-70ac8dec092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, word, language_processor,device = \"cpu\"):\n",
    "\n",
    "    lp = language_processor\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        train = 0\n",
    "\n",
    "        if encoder.training and decoder.training: ## reset the the model back to train mode\n",
    "            train = 1\n",
    "\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        input_tensor = torch.tensor(lp.encode_word(word,lp.source_lang,padding=False,append_eos = True)).to(device).view(1,-1)\n",
    "\n",
    "        batch_size = 1\n",
    "\n",
    "        if encoder.rnn_type == \"LSTM\":\n",
    "                encoder_hidden = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "                encoder_cell = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "        else:\n",
    "            encoder_hidden = None\n",
    "            encoder_cell = None\n",
    "        \n",
    "\n",
    "        encoder_hidden_contexts, encoder_last_hidden, encoder_cell = encoder(input_tensor,encoder_hidden,encoder_cell)\n",
    "\n",
    "        \"\"\"if encoder_hidden.shape[0] != decoder.expected_h0_dim1:\n",
    "            reshaped_encoder_hidden = encoder_hidden.repeat(decoder.expected_h0_dim1,1,1)\n",
    "        else:\n",
    "            reshaped_encoder_hidden = encoder_hidden\"\"\"\n",
    "\n",
    "        #print(encoder_hidden.shape,encoder_hidden.shape)\n",
    "\n",
    "        decoder_outputs, _, attentions = decoder(encoder_hidden_contexts, encoder_last_hidden,encoder_cell ,eval_mode = True,target_tensor = None)\n",
    "\n",
    "        output_size = len(list(lp.target_char2id.keys()))\n",
    "        decoder_outputs = decoder_outputs.view(30,-1)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_chars = []\n",
    "        \"\"\"for idx in decoded_ids:\n",
    "            if idx.item() == end_token_id:\n",
    "                break\n",
    "            decoded_chars.append(lp.target_id2char[idx.item()])\"\"\"\n",
    "\n",
    "        decoded_word = lp.decode_word(decoded_ids.cpu().numpy(),lp.target_lang)\n",
    "\n",
    "    if train:\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "    \n",
    "    return decoded_word, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce503bff-3de9-49b3-beec-16fa599b10d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22585702-e9b2-4dcb-8260-45a67f5ee101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_start_expt(config,wandb_log = True,kaggle=False):\n",
    "    \n",
    "    batch_size = config['batch_size']\n",
    "    target_lang = \"tel\"\n",
    "\n",
    "    if kaggle:\n",
    "        base_dir = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        base_dir = \"aksharantar_sampled/\"\n",
    "        device = torch.device(\"mps\")\n",
    "\n",
    "    use_meta_tokens = True\n",
    "    append_eos = 1\n",
    "    \n",
    "    lang_dir = base_dir + target_lang + \"/\"\n",
    "    \n",
    "    ##creating train loader\n",
    "    train_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"train\",meta_tokens=use_meta_tokens)\n",
    "    ## the ids of these tokens are the same in the source and target language\n",
    "    start_token_id = train_lp.source_char2id['<start>']\n",
    "    end_token_id = train_lp.source_char2id['<end>']\n",
    "    pad_token_id = train_lp.source_char2id['<pad>']\n",
    "\n",
    "    collate_fn_ptr = partial(collate_fn,pad_token_id=pad_token_id,device=device)\n",
    "    \n",
    "    train_dataset = WordDataset(train_lp,device=device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,collate_fn=collate_fn_ptr, shuffle=True)\n",
    "    \n",
    "    ## creating test loader\n",
    "    test_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"test\",meta_tokens=use_meta_tokens)\n",
    "    \n",
    "    ## to make sure that the same vocabulary and dictionaries are used everywhere\n",
    "    test_lp.source_char2id = train_lp.source_char2id\n",
    "    test_lp.source_id2char = train_lp.source_id2char\n",
    "    test_lp.target_char2id = train_lp.target_char2id\n",
    "    test_lp.target_id2char = train_lp.target_id2char\n",
    "    \n",
    "    test_dataset = WordDataset(test_lp,device=device)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,collate_fn=collate_fn_ptr, shuffle=True)\n",
    "    \n",
    "    ## creating validation loader\n",
    "    valid_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"valid\",meta_tokens=use_meta_tokens)\n",
    "    valid_lp.source_char2id = train_lp.source_char2id\n",
    "    valid_lp.source_id2char = train_lp.source_id2char\n",
    "    valid_lp.target_char2id = train_lp.target_char2id\n",
    "    valid_lp.target_id2char = train_lp.target_id2char\n",
    "    \n",
    "    valid_dataset = WordDataset(valid_lp,device=device)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size,collate_fn=collate_fn_ptr, shuffle=True)\n",
    "    \n",
    "    ##in principle these are all fixed across train/test/valid data\n",
    "    \n",
    "    #source_max_len = train_lp.source_max_len\n",
    "    #target_max_len = train_lp.target_max_len\n",
    "    \n",
    "    source_vocab_size = len(list(train_lp.source_char2id.keys()))\n",
    "    target_vocab_size = len(list(train_lp.target_char2id.keys()))\n",
    "    \n",
    "    hidden_size = config['hidden_size']\n",
    "    embedding_size = hidden_size\n",
    "    \n",
    "    epochs = config['epochs']\n",
    "    \n",
    "    optimiser = config['optimiser']\n",
    "    \n",
    "    weight_decay = config['weight_decay']\n",
    "    \n",
    "    lr = config['lr']\n",
    "    \n",
    "    num_encoder_layers = config['num_layers']\n",
    "    num_decoder_layers = num_encoder_layers\n",
    "    \n",
    "    ## Allowed Values : \"GRU\"/\"RNN\"/\"LSTM\" (not case sensitive)\n",
    "    rnn_type = config['rnn_type'].upper()\n",
    "    \n",
    "    bidirectional = config['bidirectional']\n",
    "    teacher_forcing_ratio = config['teacher_forcing_ratio']\n",
    "\n",
    "    teacher_forcing = False\n",
    "    \n",
    "    if teacher_forcing_ratio>0:\n",
    "        teacher_forcing = True\n",
    "    \n",
    "    \n",
    "    #loss_criterion =  nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "    \n",
    "    dropout=config['dropout']\n",
    "\n",
    "    use_attention = config['use_attention']\n",
    "    \n",
    "    encoder = Encoder(source_vocab_size = source_vocab_size, hidden_size = hidden_size,embedding_size=embedding_size,rnn_type = rnn_type,padding_idx=pad_token_id,num_layers=num_encoder_layers,bidirectional=bidirectional,dropout=dropout).to(device)\n",
    "    \n",
    "    decoder = Decoder(hidden_size = hidden_size,embedding_size=embedding_size, target_vocab_size = target_vocab_size,batch_size = batch_size,rnn_type = rnn_type,use_attention = use_attention, padding_idx = None,num_layers = num_decoder_layers,bidirectional = bidirectional,dropout=dropout,device=device).to(device)\n",
    "    \n",
    "    #train(train_loader,valid_loader, encoder, decoder, 3,loss_criterion=loss_criterion, print_every=3, plot_every=5,device=device,teacher_forcing = teacher_forcing,teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "    train(train_loader,valid_loader, encoder, decoder, epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
    "\n",
    "    return encoder,decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07abf5c3-359d-4ca9-8f98-f99aad4c1199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2675f0a7564249e69d9d6c9f16d1a58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d5fe60b309400aa8a519967d0d1b26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 83\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"config = {\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m    'hidden_size':128,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m        \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m        'weight_decay': 1e-5\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m}\"\"\"\u001b[39;00m\n\u001b[1;32m     53\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     54\u001b[0m \n\u001b[1;32m     55\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m64\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m }\n\u001b[0;32m---> 83\u001b[0m encoder,decoder \u001b[38;5;241m=\u001b[39m \u001b[43msetup_and_start_expt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwandb_log\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mkaggle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 97\u001b[0m, in \u001b[0;36msetup_and_start_expt\u001b[0;34m(config, wandb_log, kaggle)\u001b[0m\n\u001b[1;32m     94\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(hidden_size \u001b[38;5;241m=\u001b[39m hidden_size,embedding_size\u001b[38;5;241m=\u001b[39membedding_size, target_vocab_size \u001b[38;5;241m=\u001b[39m target_vocab_size,batch_size \u001b[38;5;241m=\u001b[39m batch_size,rnn_type \u001b[38;5;241m=\u001b[39m rnn_type,use_attention \u001b[38;5;241m=\u001b[39m use_attention, padding_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,num_layers \u001b[38;5;241m=\u001b[39m num_decoder_layers,bidirectional \u001b[38;5;241m=\u001b[39m bidirectional,dropout\u001b[38;5;241m=\u001b[39mdropout,device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#train(train_loader,valid_loader, encoder, decoder, 3,loss_criterion=loss_criterion, print_every=3, plot_every=5,device=device,teacher_forcing = teacher_forcing,teacher_forcing_ratio=teacher_forcing_ratio)\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mteacher_forcing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing\u001b[49m\u001b[43m,\u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwandb_logging\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwandb_log\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoder,decoder\n",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, valid_loader, encoder, decoder, n_epochs, padding_idx, optimiser, loss, weight_decay, lr, teacher_forcing, teacher_forcing_ratio, print_every, plot_every, device, wandb_logging)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#criterion = loss_criterion.to(device)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m---> 31\u001b[0m     train_loss,train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     print_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n\u001b[1;32m     33\u001b[0m     plot_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n",
      "Cell \u001b[0;32mIn[5], line 50\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio, ignore_padding, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m     tot_words \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_words\n\u001b[1;32m     46\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     47\u001b[0m     decoder_outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, decoder_outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     48\u001b[0m     target_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     49\u001b[0m )\n\u001b[0;32m---> 50\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     53\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"config = {\n",
    "    'hidden_size':128,\n",
    "        \n",
    "        'embedding_size':128,\n",
    "\n",
    "        'rnn_type' : \"gru\",\n",
    "        \n",
    "        'batch_size':256,\n",
    "        \n",
    "        'optimiser': \"adam\",\n",
    "\n",
    "        'num_layers' : 2,\n",
    "\n",
    "        'lr':  1e-3,\n",
    "\n",
    "        'dropout' : 0.1,\n",
    "        \n",
    "        'epochs' : 15,\n",
    "\n",
    "        'teacher_forcing_ratio' : 0.3,\n",
    "\n",
    "        'bidirectional' :  True,\n",
    "    \n",
    "        'weight_decay': 0\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'hidden_size':512,\n",
    "        \n",
    "        'embedding_size':512,\n",
    "\n",
    "        'rnn_type' : \"lstm\",\n",
    "        \n",
    "        'batch_size':64,\n",
    "        \n",
    "        'optimiser': \"nadam\",\n",
    "\n",
    "        'num_layers' : 3,\n",
    "\n",
    "        'lr':  1e-3,\n",
    "\n",
    "        'dropout' : 0.4,\n",
    "        \n",
    "        'epochs' : 15,\n",
    "\n",
    "        'teacher_forcing_ratio' : 0.4,\n",
    "\n",
    "        'bidirectional' :  True,\n",
    "    \n",
    "        'weight_decay': 1e-5\n",
    "}\"\"\"\n",
    "\n",
    "config = {\n",
    "\n",
    "        'batch_size':64,\n",
    "\n",
    "        'bidirectional' :  True,\n",
    "\n",
    "        'dropout' : 0.4,\n",
    "\n",
    "        'embedding_size':128,\n",
    "\n",
    "        'epochs' : 1,\n",
    "\n",
    "        'hidden_size':512,\n",
    "\n",
    "        'lr':  3e-4,\n",
    "\n",
    "        'num_layers' : 4,\n",
    "\n",
    "        'optimiser': \"nadam\",\n",
    "\n",
    "        'rnn_type' : \"lstm\",\n",
    "\n",
    "        'teacher_forcing_ratio' : 0.4,\n",
    "\n",
    "        'weight_decay': 1e-5,\n",
    "\n",
    "        'use_attention' : False\n",
    "\n",
    "}\n",
    "\n",
    "encoder,decoder = setup_and_start_expt(config,wandb_log = False,kaggle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d10667",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "batch_size = config['batch_size']\n",
    "kaggle = False\n",
    "\n",
    "target_lang = \"tel\"\n",
    "\n",
    "if kaggle:\n",
    "    base_dir = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    base_dir = \"aksharantar_sampled/\"\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device(\"mps\")\n",
    "\n",
    "\n",
    "use_meta_tokens = True\n",
    "append_eos = 1\n",
    "\n",
    "lang_dir = base_dir + target_lang + \"/\"\n",
    "\n",
    "\n",
    "##creating train loader\n",
    "train_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"train\",meta_tokens=use_meta_tokens)\n",
    "## the ids of these tokens are the same in the source and target language\n",
    "start_token_id = train_lp.source_char2id['<start>']\n",
    "end_token_id = train_lp.source_char2id['<end>']\n",
    "pad_token_id = train_lp.source_char2id['<pad>']\n",
    "\n",
    "collate_fn_ptr = partial(collate_fn,pad_token_id=pad_token_id,device=device)\n",
    "\n",
    "train_dataset = WordDataset(train_lp,device=device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,collate_fn=collate_fn_ptr, shuffle=True)\n",
    "\n",
    "## creating test loader\n",
    "test_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"test\",meta_tokens=use_meta_tokens)\n",
    "\n",
    "## to make sure that the same vocabulary and dictionaries are used everywhere\n",
    "test_lp.source_char2id = train_lp.source_char2id\n",
    "test_lp.source_id2char = train_lp.source_id2char\n",
    "test_lp.target_char2id = train_lp.target_char2id\n",
    "test_lp.target_id2char = train_lp.target_id2char\n",
    "\n",
    "test_dataset = WordDataset(test_lp,device=device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,collate_fn=collate_fn_ptr, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 2).to(device)\n",
    "\n",
    "test_loss,_,test_accuracy = compute_accuracy(test_loader,encoder,decoder,criterion,padding_token_id = test_lp.source_char2id['<pad>'],end_token_id = test_lp.source_char2id['<end>'],ignore_padding=True,device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45761b2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m word \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrirama\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m decoded_word, attentions \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_lp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 36\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, word, language_processor, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"if encoder_hidden.shape[0] != decoder.expected_h0_dim1:\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    reshaped_encoder_hidden = encoder_hidden.repeat(decoder.expected_h0_dim1,1,1)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03melse:\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    reshaped_encoder_hidden = encoder_hidden\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m#print(encoder_hidden.shape,encoder_hidden.shape)\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m decoder_outputs, _, attentions \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_hidden_contexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_last_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_cell\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43meval_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtarget_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(lp\u001b[38;5;241m.\u001b[39mtarget_char2id\u001b[38;5;241m.\u001b[39mkeys()))\n\u001b[1;32m     39\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m decoder_outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m30\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[21], line 153\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, encoder_hidden_contexts, encoder_last_hidden, encoder_cell, target_tensor, eval_mode, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    150\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m target_tensor[:, step]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Teacher forcing\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m##greedily pick predictions, i.e pick the character corresponding to the hightest probability\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     _,preds \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     decoder_input \u001b[38;5;241m=\u001b[39m preds\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    156\u001b[0m decoder_outputs\u001b[38;5;241m.\u001b[39mappend(decoder_output)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "word = \"srirama\"\n",
    "\n",
    "decoded_word, attentions = evaluate(encoder, decoder, word, train_lp,device = \"mps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1c483e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcf6c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a5cac-1e5f-4ed7-95bc-2b30cf58a0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92c7fb-0e80-4018-a93c-c441564e7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key=\"\")\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'name' : 'PA3 Hyper Sweep GRU',\n",
    "    'metric': {\n",
    "      'name': 'Validation accuracy',\n",
    "      'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        \n",
    "         'hidden_size':{\n",
    "            'values':[128]#[64,128,256,512]\n",
    "        },\n",
    "        \n",
    "        'embedding_size':{\n",
    "            'values':[128]#[64,128,256,512]\n",
    "        },\n",
    "\n",
    "        'rnn_type':{\n",
    "            'values':[\"gru\"]#['lstm','rnn','gru']\n",
    "        },\n",
    "        \n",
    "        'batch_size':{\n",
    "            'values':[64]#[32,64,128,256]\n",
    "        },\n",
    "        \n",
    "        'optimiser': {\n",
    "            'values': [\"adam\"]#,\"rmsprop\",\"nadam\"]\n",
    "        },\n",
    "\n",
    "        'num_layers' :{\n",
    "            'values' : [2]#[1,2,3,4,5]\n",
    "        },\n",
    "\n",
    "        'lr': {\n",
    "            'values': [1e-3]#[1e-2,1e-3,1e-4,3e-4]\n",
    "        },\n",
    "\n",
    "        'dropout' : {\n",
    "\n",
    "            'values' : [0.1]#[0,0.1,0.2,0.3,0.4]\n",
    "        },\n",
    "        \n",
    "        'epochs' : {\n",
    "\n",
    "            'values' : [15]\n",
    "        },\n",
    "\n",
    "        'teacher_forcing_ratio' : {\n",
    "            'values' : [0.3]#[0,0.1,0.2,0.3,0.4,0.5]\n",
    "        },\n",
    "\n",
    "        'bidirectional' : {\n",
    "            'values' : [True]#[True,False]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0,1e-3,]#,1e-3,5e-3,5e-4]\n",
    "        },\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project='JV_CS23M036_TEJASVI_DL_ASSIGNMENT3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb1380-b7e4-4744-a8ef-3b720852639b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b45ea38-320f-4939-bfed-0362b1ecd8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    '''\n",
    "    WandB calls main function each time with differnet combination.\n",
    "\n",
    "    We can retrive the same and use the same values for our hypermeters.\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    with wandb.init() as run:\n",
    "\n",
    "        run_name=\"-hl_\"+str(wandb.config.num_layers)+\"-hs_\"+str(wandb.config.hidden_size)+\"-es_\"+str(wandb.config.hidden_size)+\"-biDir_\"+str(wandb.config.bidirectional)\n",
    "\n",
    "        run_name = run_name+\"-rnn_type_\"+str(wandb.config.rnn_type)+run_name+\"-optim_\"+str(wandb.config.optimiser)+\"-lr_\"+str(wandb.config.lr)+\"-reg_\"+str(wandb.config.weight_decay)+\"-epochs_\"+str(wandb.config.epochs)+\"-tf_ratio_\"+str(wandb.config.teacher_forcing_ratio)\n",
    "\n",
    "        run_name = run_name+\"-dropout_\"+str(wandb.config.dropout)\n",
    "\n",
    "        wandb.run.name=run_name\n",
    "\n",
    "        setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, function=main,count=400) # calls main function for count number of times.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0c784-f46f-4e4f-ae48-55a3be347e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"op1,_ = evaluate(encoder, decoder, word=\"srirama\", language_processor=train_lp,device = device)\n",
    "\n",
    "op2,_ = evaluate(encoder, decoder, word=\"tejasvi\", language_processor=train_lp,device = device)\n",
    "\n",
    "op1_string = \"\".join(op1)\n",
    "print(op1_string[:8])\n",
    "\n",
    "op2_string = \"\".join(op2)\n",
    "print(op2_string[:7])\n",
    "\n",
    "print()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a6d6e-bde3-46d6-b858-95fd8768a3ff",
   "metadata": {},
   "source": [
    "### JV\n",
    "\n",
    "###### To Do:\n",
    "\n",
    "1. Add LSTM,RNN support : Train\n",
    "2. Model parameter Initialization.\n",
    "3. Attention.\n",
    "4. Now create a seq2seq class, specify attention = True for attention to work.\n",
    "\n",
    "Hyper Params:\n",
    "\n",
    "1. Batch size\n",
    "2. Hidden layer size\n",
    "3. Embedding size\n",
    "4. number of encoder layers\n",
    "5. number of decoder layers\n",
    "6. bidirectional\n",
    "7. tf ratio ; 0/0.1/0.2/0.3/0.5\n",
    "8. Optimizer\n",
    "9. Learning Rate\n",
    "10. Batch size\n",
    "11. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e908c4f-af04-4ff8-95e4-875c6e5a156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dropout within GR/LSTM/RNN could be applied only when num_layers>1, so if we have 1 layer dropout of 0 is applied.\n",
    "However, the specified dropout is applied to embedding.\n",
    "\n",
    "considering encoder_layers = decoder_layers = num_layers.\n",
    "\n",
    "considering embedding_size = hidden_size.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
