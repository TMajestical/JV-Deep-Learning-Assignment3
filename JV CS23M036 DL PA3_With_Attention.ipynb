{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0cd007-c07d-41ae-a7d1-e8a396936a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75927828-0e72-4da3-a840-98d46c1e6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#!python3 -m pip install wandb\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d84851-1726-4bfb-955d-209cdc80bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "094edcd3-d8da-4e5b-b48e-413e23060dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageProcessor:\n",
    "\n",
    "    def __init__(self,language_directory,target_lang_name,mode=\"train\",meta_tokens=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Default Constructor for this class.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            language_directory : ex : \"aksharantar_sampled/tel/\"\n",
    "            mode : \"train\" or \"test\" or \"valid\", accordingly the appropriate dataset is read.\n",
    "            meta_tokens : If true creates the first three tokens of the dictionary as <start>,<end>,<pad>.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        self.meta_tokens = meta_tokens ## boolean variable, if 1, then <start>,<end> and <pad> tokens are cosidered in the vocab.\n",
    "        self.language_directory = language_directory\n",
    "        self.target_lang_name = target_lang_name\n",
    "        self.mode = mode ## accordingly helps to read and generate the appropriate dataset.\n",
    "    \n",
    "        self.source_lang = 0\n",
    "        self.target_lang = 1\n",
    "\n",
    "        self.source_max_len = self.find_max_len(self.source_lang)\n",
    "        self.target_max_len = self.find_max_len(self.target_lang)\n",
    "\n",
    "        self.max_len = max(self.source_max_len,self.target_max_len)+1 ##accomodating End token also, irrespective of whether it is used.\n",
    "\n",
    "        self.source_char2id,self.source_id2char = self.build_char_vocab(self.source_lang,self.source_max_len)\n",
    "        self.target_char2id,self.target_id2char = self.build_char_vocab(self.target_lang,self.target_max_len)\n",
    "\n",
    "\n",
    "    def find_max_len(self,lang):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to find the maximum length of a word across train/test and validation data.\n",
    "\n",
    "        This would help in padding, the embedding accordingly.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            lang : 0/1 (source/target) language for which the length of the longest word must be found.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        train_df = pd.read_csv(self.language_directory+self.target_lang_name+\"_train.csv\",header=None)\n",
    "        test_df = pd.read_csv(self.language_directory+self.target_lang_name+\"_test.csv\",header=None)\n",
    "        valid_df = pd.read_csv(self.language_directory+self.target_lang_name+\"_valid.csv\",header=None)\n",
    "\n",
    "        train_max_len = len(max(list(train_df[lang]), key = len))\n",
    "        test_max_len = len(max(list(test_df[lang]), key = len))\n",
    "        valid_max_len = len(max(list(valid_df[lang]), key = len))\n",
    "\n",
    "        del train_df\n",
    "        del test_df\n",
    "        del valid_df\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        return max(train_max_len,test_max_len,valid_max_len)\n",
    "\n",
    "    def build_char_vocab(self,lang_id,max_len=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to create a vocabulary of characters in language corresponding to lang_id.\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_csv(self.language_directory+self.target_lang_name+\"_\"+self.mode+\".csv\",header=None)\n",
    "\n",
    "        self.data = df.to_numpy()\n",
    "\n",
    "        lang_chars = []\n",
    "        lang_words = df[lang_id].to_numpy()\n",
    "    \n",
    "        for word in lang_words:\n",
    "            lang_chars += list(word)\n",
    "    \n",
    "        unique_lang_chars =  sorted(list(set(lang_chars)))\n",
    "        \n",
    "        if self.meta_tokens:\n",
    "            char2id_dict = {'<start>':0,'<end>':1,'<pad>': 2}\n",
    "            id2char_dict = {0:'<start>',1:'<end>',2:'<pad>'}\n",
    "            \n",
    "        else:\n",
    "            char2id_dict = {}\n",
    "            id2char_dict = {}\n",
    "\n",
    "        start = len(char2id_dict) ##Key of each language character starts based on meta tokens are used or not.\n",
    "    \n",
    "        for i in range(len(unique_lang_chars)):\n",
    "            char2id_dict[unique_lang_chars[i]] = i+start\n",
    "            id2char_dict[i+start] = unique_lang_chars[i]\n",
    "    \n",
    "        del df\n",
    "        del lang_chars\n",
    "        del unique_lang_chars\n",
    "\n",
    "        gc.collect()\n",
    "    \n",
    "        return char2id_dict,id2char_dict\n",
    "\n",
    "    def encode_word(self,word,lang_id,padding=True,append_eos = True):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to encode characters of a given word.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            word: The word to be encoded.\n",
    "            lang_id : 0/1 for source/target lang.\n",
    "            padding : If true, the word encoding would be padded upto max len.\n",
    "            append_eos : Appends <end> token at the end of every word.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if lang_id == self.source_lang:\n",
    "            char2id_dict = self.source_char2id\n",
    "            \n",
    "        else:\n",
    "            char2id_dict = self.target_char2id\n",
    "        \n",
    "        max_len = self.max_len\n",
    "\n",
    "        word_encoding = []\n",
    "        \n",
    "        #if lang_id == self.source_lang:\n",
    "        #    word_encoding = [char2id_dict['<start>']] ##every input starts with the <start> token.\n",
    "        \n",
    "        for i in word.lower():\n",
    "            word_encoding.append(char2id_dict[i])\n",
    "\n",
    "        #if append_eos:\n",
    "        #    word_encoding.append(char2id_dict['<end>'])\n",
    "\n",
    "        ## pad till maxlen, if padding is used.\n",
    "        if padding:\n",
    "            word_encoding += [char2id_dict['<pad>']] * (max_len - len(word_encoding))\n",
    "        \n",
    "        return np.array(word_encoding)\n",
    "\n",
    "    def decode_word(self,code_word,lang_id):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to decode an encoded word.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            code_word : The encoded word.\n",
    "            lang_id : 0/1 for source/target lang.\n",
    "        \"\"\"\n",
    "    \n",
    "        word = []\n",
    "\n",
    "        if lang_id == self.source_lang:\n",
    "            id2char_dict = self.source_id2char\n",
    "            char2id_dict = self.source_char2id\n",
    "            \n",
    "        else:\n",
    "            id2char_dict = self.target_id2char\n",
    "            char2id_dict = self.target_char2id\n",
    "\n",
    "        start_idx = 0#1-lang_id\n",
    "        \n",
    "        for i in code_word[start_idx:]:\n",
    "            ## if we reached <end>, then stop decoding\n",
    "            if self.meta_tokens and i == char2id_dict['<end>'] or i == char2id_dict['<pad>']:\n",
    "                break\n",
    "            \n",
    "            word.append(id2char_dict[i])\n",
    "            \n",
    "        return np.array(word)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e5df96-e649-4354-852d-85f7116fd797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Class that inherits and overrides the methods of Dataset class. This helps in creating a data loader.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, language_processor,append_eos=True,device='cpu'):\n",
    "\n",
    "        self.lp = language_processor\n",
    "        self.data = self.lp.data\n",
    "        self.device = device\n",
    "        self.append_eos = append_eos\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_word, output_word = self.data[idx]\n",
    "        \n",
    "        input_sequence = self.lp.encode_word(input_word,self.lp.source_lang,padding=False,append_eos=self.append_eos)\n",
    "        output_sequence = self.lp.encode_word(output_word,self.lp.target_lang,padding=False,append_eos=self.append_eos)\n",
    "        \n",
    "\n",
    "        #if len(input_sequence) != len(output_sequence):\n",
    "        #    print(input_word,len(input_word),output_word,len(output_word))\n",
    "        \n",
    "        return torch.tensor(input_sequence).to(self.device), torch.tensor(output_sequence).to(self.device)\n",
    "        #return input_sequence, output_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6583324c-6495-4bb5-a536-d614671bc280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c93e7db4-7631-4b6b-9db9-bf18950fa6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_token_id = 2\n",
    "device = torch.device(\"mps\")\n",
    "#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \n",
    "    input_words, target_words = zip(*batch)\n",
    "    \n",
    "    padded_inputs = pad_sequence(input_words, batch_first=True, padding_value=pad_token_id)\n",
    "    \n",
    "    padded_targets = pad_sequence(target_words, batch_first=True, padding_value=pad_token_id)\n",
    "    \n",
    "    input_lengths = torch.LongTensor([len(seq) for seq in input_words]).to(device)\n",
    "    target_lengths = torch.LongTensor([len(seq) for seq in target_words]).to(device)\n",
    "    \n",
    "    return padded_inputs, padded_targets, input_lengths, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8d39e84-a7e5-4344-af5d-ed91b8c5d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,embedding_size,rnn_type,padding_index, dropout=0.1,num_layers=1,bidirectional=False):\n",
    "        \n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.D = 1 ##the number of directions in which the input is viewed.\n",
    "        if bidirectional:\n",
    "            self.D = 2\n",
    "        \n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size,padding_idx=padding_index)\n",
    "        #self.gru = nn.GRU(hidden_size, hidden_size,num_layers = num_layers,bidirectional=bidirectional, batch_first=True)\n",
    "        if self.rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=dropout)\n",
    "        elif self.rnn_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=dropout)\n",
    "        elif self.rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(hidden_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input,hidden=None,cell=None):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        else:\n",
    "            output, hidden = self.rnn(embedded)\n",
    "            cell = None\n",
    "        #output, hidden = self.gru(embedded)\n",
    "        return output, hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2feb515a-80c5-424b-b6d9-92deb32975f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size,D,expected_dim,batch_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size*expected_dim, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size*D, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "\n",
    "        #print(f\"(encoder_outputs)keys.shape:{keys.shape}\")\n",
    "        #print(f\"\\nquery.shape:{query.shape}\")\n",
    "        reshaped_query = query.reshape(self.batch_size,1,-1)\n",
    "        #print(f\"reshaped_query.shape:{reshaped_query.shape}\")\n",
    "        #print(f\"Wa(query.reshape(batch_size,1,-1)).shape : {self.Wa(reshaped_query).shape}\")\n",
    "        #print(f\"Ua(keys).shape : {self.Ua(keys).shape}\")\n",
    "        \n",
    "        scores = self.Va(torch.tanh(self.Wa(query.reshape(self.batch_size,1,-1)) + self.Ua(keys)))\n",
    "\n",
    "        #print(f\"scores.shape : {scores.shape}\")\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "        #print(f\"scores.squeeze(2).unsqueeze(1).shape : {scores.shape}\")\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        #print(f\"weights.shape : {weights.shape}\\n\")\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c91f2024-0b53-4cc8-8601-f52f0a4cbd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size,embedding_size, output_size,rnn_type,max_len,batch_size,padding_index,start_token_id, dropout=0.1,num_layers=1,bidirectional=False,device = \"cpu\"):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.start_token_id = start_token_id\n",
    "        \n",
    "        self.D = 1 ##the number of directions in which the input is viewed.\n",
    "        if bidirectional:\n",
    "            self.D = 2\n",
    "            \n",
    "        ## In h0 (the input to the decoder) first dimension expected is number of directions X number of layers \n",
    "        self.expected_h0_dim1 = self.D*self.num_layers\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size*self.D,padding_idx=padding_index)\n",
    "        self.attention = BahdanauAttention(hidden_size,self.D,self.expected_h0_dim1,batch_size)\n",
    "\n",
    "        #self.gru = nn.GRU(2 * self.D * hidden_size, hidden_size,num_layers=self.num_layers,bidirectional = bidirectional, batch_first=True)\n",
    "        if self.rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(2 * self.D * hidden_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=dropout)\n",
    "        elif self.rnn_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(2 * self.D * hidden_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=dropout)\n",
    "        elif self.rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(2 * self.D * hidden_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional,dropout=dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size*self.D, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden,encoder_cell, target_tensor=None,MAX_LENGTH=28,teacher_forcing_ratio = 0):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(self.start_token_id)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_cell = encoder_cell ## the cell state, which is initially same as that of encoder, (applies to LSTM unit only)\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        max_len = target_tensor.size(1)\n",
    "\n",
    "        for i in range(max_len):\n",
    "            decoder_output, decoder_hidden,decoder_cell, attn_weights = self.forward_step(decoder_input, decoder_hidden,decoder_cell, encoder_outputs)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            if (target_tensor is not None) and (teacher_force):\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden,cell, encoder_outputs):\n",
    "        embedded =  self.dropout(self.embedding(input))\n",
    "        \n",
    "\n",
    "        if hidden.shape[0] != self.expected_h0_dim1:\n",
    "            reshaped_hidden = hidden.repeat(self.expected_h0_dim1,1,1)\n",
    "        else:\n",
    "            reshaped_hidden = hidden\n",
    "\n",
    "        #print(f\"encoder_outputs.shape:{encoder_outputs.shape}\")\n",
    "        #print(f\"hidden.shape:{hidden.shape}\")\n",
    "        #print(f\"reshaped_hidden.shape:{reshaped_hidden.shape}\")\n",
    "\n",
    "        query = reshaped_hidden.permute(1, 0, 2)\n",
    "        #print(f\"query.shape:{query.shape}\")\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        #print(f\"embedded.shape:{embedded.shape}\")\n",
    "        #print(f\"context.shape:{context.shape}\")\n",
    "        #print(f\"attn_weights.shape:{attn_weights.shape}\")\n",
    "        input = torch.cat((embedded, context), dim=2)\n",
    "        #print(f\"input_gru.shape:{input_gru.shape}\")\n",
    "        \n",
    "        \n",
    "\n",
    "        #output, hidden = self.gru(input_gru, reshaped_hidden)\n",
    "        if self.rnn_type == \"LSTM\":\n",
    "            output, (hidden, cell) = self.rnn(input, (reshaped_hidden, cell))\n",
    "        else:\n",
    "            output, hidden = self.rnn(input, reshaped_hidden)\n",
    "            cell = None\n",
    "        \n",
    "        #print(f\"output1.shape:{output.shape}\")\n",
    "        output = self.out(output)\n",
    "        #print(f\"output2.shape:{output.shape}\")\n",
    "        #print(\"========================================================================\")\n",
    "\n",
    "        return output, hidden,cell, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0e72aef-f110-4cf5-9fe0-37ca7e45fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,decoder_optimizer, criterion,teacher_forcing_ratio=0,ignore_padding=True):\n",
    "\n",
    "    tot_correct_word_preds = 0\n",
    "    tot_words = 0\n",
    "    \n",
    "    total_loss = 0\n",
    "    for data in tqdm(dataloader):\n",
    "        input_tensor, target_tensor,_,tar_max_len = data\n",
    "\n",
    "        max_len = torch.max(tar_max_len).item()\n",
    "        \n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        batch_size = data[0].shape[0]\n",
    "\n",
    "        #print(data[0].shape,batch_size)\n",
    "\n",
    "        if encoder.rnn_type == \"LSTM\":\n",
    "            encoder_hidden = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "            encoder_cell = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "        else:\n",
    "            encoder_hidden = None\n",
    "            encoder_cell = None\n",
    "\n",
    "        encoder_outputs, encoder_hidden, encoder_cell = encoder(input_tensor,encoder_hidden,encoder_cell)\n",
    "\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden,encoder_cell, target_tensor,max_len,teacher_forcing_ratio = teacher_forcing_ratio)\n",
    "\n",
    "        multi_step_preds = torch.argmax(decoder_outputs,dim=2)\n",
    "        multi_step_pred_correctness = (multi_step_preds ==  target_tensor)\n",
    "        num_words = batch_size\n",
    "        \n",
    "        if ignore_padding: ## if padding has to be ignored.\n",
    "\n",
    "            ## for each word, based on the padding token ID, find the first occurance of the padding token, marking the begining of padding.\n",
    "            \n",
    "            ## argmax is not supported for bool on cuda, hence casting it to long.\n",
    "            padding_start = torch.argmax((target_tensor == pad_token_id).to(torch.long),dim=1).to(device)\n",
    "            ## Creating a mask with 1's in each position of a padding token\n",
    "            mask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\n",
    "            #print(mask)\n",
    "            \n",
    "            ##doing a logical OR with the mask makes sure that the padding tokens do not affect the correctness of the word\n",
    "            tot_correct_word_preds += (torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum()).item()\n",
    "            tot_words += num_words\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    epoch_loss = round(total_loss / len(dataloader),4)\n",
    "    epoch_accuracy = round(tot_correct_word_preds*100/tot_words,2)\n",
    "\n",
    "    return epoch_loss,epoch_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf5a8020-4e7b-4c9e-9464-94a87d03a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader,valid_dataloader, encoder, decoder, n_epochs,padding_idx,optimiser = \"adam\",loss=\"crossentropy\",weight_decay=0, lr=0.001,teacher_forcing = False,teacher_forcing_ratio = 0,device = 'cpu',print_every=100, plot_every=100,wandb_logging = False):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    ## specify the optimiser\n",
    "    if optimiser.lower() == \"adam\":\n",
    "        encoder_optimizer = optim.Adam(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        decoder_optimizer = optim.Adam(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "    elif optimiser.lower() == \"nadam\":\n",
    "        encoder_optimizer = optim.NAdam(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        decoder_optimizer = optim.NAdam(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "\n",
    "    elif optimiser.lower() == \"rmsprop\":\n",
    "        encoder_optimizer = optim.RMSprop(encoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        decoder_optimizer = optim.RMSprop(decoder.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "        \n",
    "    ## Specify the loss criteria\n",
    "    if loss.lower() == \"crossentropy\":\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index = padding_idx).to(device)\n",
    "\n",
    "    lp = train_dataloader.dataset.lp\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        train_loss,train_accuracy = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,teacher_forcing_ratio)\n",
    "        print_loss_total += train_loss\n",
    "        plot_loss_total += train_loss\n",
    "        val_loss,_,val_word_level_accuracy = compute_accuracy(valid_dataloader,encoder,decoder,criterion,padding_token_id = lp.source_char2id['<pad>'],end_token_id = lp.source_char2id['<end>'],ignore_padding=True,device=device)\n",
    "\n",
    "        print(f\"Epoch {epoch}\\t Train Loss : {train_loss}\\t Train Acc : {train_accuracy}% \\t Val Loss : {val_loss}\\t Val Acc : {val_word_level_accuracy}%\")\n",
    "        \n",
    "        if wandb_logging:\n",
    "            wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_word_level_accuracy})\n",
    "        #print(f\"JV Char Accuracy:{JV_char_accuracy}%\\t JV Word Accuracy:{JV_word_accuracy}%\\t Loss : {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f579fd3-a0dd-4aeb-acfb-775ae2d73cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(dataloader,encoder,decoder,loss_criterion,padding_token_id,end_token_id = 1,ignore_padding = True,device='cpu'):\n",
    "\n",
    "    \"\"\"\n",
    "    Method to compute the accuracy using the model (encoder-decoder) using dataloader.\n",
    "\n",
    "    This method returns word and character level accuracy.\n",
    "\n",
    "        Word Level Accuracy : Accuracy is computed at the word level and a word is right iff every character is predicted correctly.\n",
    "        Char Level Accuracy : Accuracy is computed by comparing each predicted character wrt the correct char.\n",
    "\n",
    "    Params:\n",
    "\n",
    "        dataloader : The train/test/valid dataloader.\n",
    "        encoder : The encoder \n",
    "        decoder : The decoder\n",
    "        padding_token_id : The id of the padding token.\n",
    "        ignore_padding : If True, then in word level accuracy, the padding characters are ignored in computing the word level accuracy.\n",
    "                        char level accuracy, the padding characters are not considered at all.\n",
    "\n",
    "                        If false, padding is considered to be a part of the word (for word level accuracy) and \n",
    "    \"\"\"\n",
    "\n",
    "    word_level_accuracy = 0\n",
    "\n",
    "    tot_words = 0\n",
    "\n",
    "    tot_correct_word_preds = 0\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    criterion = loss_criterion.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        train = 0\n",
    "\n",
    "        if encoder.training and decoder.training: ## reset the the model back to train mode\n",
    "            train = 1\n",
    "\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        for data in dataloader:\n",
    "            \n",
    "            input_tensor, target_tensor,_,tar_max_len = data\n",
    "\n",
    "            max_len = torch.max(tar_max_len).item()\n",
    "\n",
    "            batch_size = data[0].shape[0]\n",
    "    \n",
    "            if encoder.rnn_type == \"LSTM\":\n",
    "                encoder_hidden = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "                encoder_cell = torch.zeros(encoder.num_layers*encoder.D, batch_size, encoder.hidden_size, device=device)\n",
    "            else:\n",
    "                encoder_hidden = None\n",
    "                encoder_cell = None\n",
    "    \n",
    "            encoder_outputs, encoder_hidden, encoder_cell = encoder(input_tensor,encoder_hidden,encoder_cell)\n",
    "\n",
    "            ## even though we are passing target tensor, the teacher forcing ratio is 0, so no teacher forcing\n",
    "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden,encoder_cell, target_tensor,max_len,teacher_forcing_ratio = 0)\n",
    "            \n",
    "            loss += criterion(\n",
    "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                target_tensor.view(-1)).item()\n",
    "    \n",
    "            ## For a batch, for each character find the most probable output word.\n",
    "            multi_step_preds = torch.argmax(decoder_outputs,dim=2)\n",
    "            multi_step_pred_correctness = (multi_step_preds ==  target_tensor)\n",
    "            num_chars = multi_step_preds.numel() ##find the total number of characters in the current batch\n",
    "            num_words = batch_size ##find the total number of words in the current batch.\n",
    "    \n",
    "            if ignore_padding: ## if padding has to be ignored.\n",
    "    \n",
    "                ## for each word, based on the padding token ID, find the first occurance of the padding token, marking the begining of padding.\n",
    "                \n",
    "                ## argmax is not supported for bool on cuda, hence casting it to long.\n",
    "                padding_start = torch.argmax((target_tensor == pad_token_id).to(torch.long),dim=1).to(device)\n",
    "                ## Creating a mask with 1's in each position of a padding token\n",
    "                mask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\n",
    "                #print(mask)\n",
    "                \n",
    "                ##doing a logical OR with the mask makes sure that the padding tokens do not affect the correctness of the word\n",
    "                tot_correct_word_preds += (torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum()).item()\n",
    "                tot_words += num_words\n",
    "    \n",
    "            else: ##otherwise.\n",
    "    \n",
    "                tot_correct_word_preds += (torch.all(multi_step_pred_correctness,dim=1).int().sum()).item()\n",
    "                tot_words += num_words\n",
    "                \n",
    "                tot_correct_char_preds += (multi_step_pred_correctness.int().sum()).item()\n",
    "                tot_chars += num_chars\n",
    "\n",
    "        #print(tot_correct_char_preds,tot_chars)\n",
    "        #print(tot_correct_word_preds,tot_words)\n",
    "    \n",
    "        word_lvl_accuracy = round(tot_correct_word_preds*100/tot_words,2)\n",
    "\n",
    "        loss /= dataloader.dataset.data.shape[0]\n",
    "\n",
    "        if train:\n",
    "\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "    \n",
    "        return round(loss,4),None,word_lvl_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1de634bd-9e38-4733-9aeb-9120523794a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119afd79-6f6a-492b-b831-9a4c18940a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4fd51f1-c5e6-44fb-a2c8-facf263a3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_start_expt(config,wandb_log = True,kaggle=False):\n",
    "    \n",
    "    batch_size = config['batch_size']\n",
    "    target_lang = \"tel\"\n",
    "\n",
    "\n",
    "    if kaggle:\n",
    "        base_dir = \"/kaggle/input/aksharantar-sampled/aksharantar_sampled/\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        base_dir = \"aksharantar_sampled/\"\n",
    "        device = torch.device(\"mps\")\n",
    "    \n",
    "    use_meta_tokens = True\n",
    "    append_eos = 1\n",
    "    \n",
    "    lang_dir = base_dir + target_lang + \"/\"\n",
    "    \n",
    "    ##creating train loader\n",
    "    train_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"train\",meta_tokens=use_meta_tokens)\n",
    "    ## the ids of these tokens are the same in the source and target language\n",
    "    start_token_id = train_lp.source_char2id['<start>']\n",
    "    end_token_id = train_lp.source_char2id['<end>']\n",
    "    pad_token_id = train_lp.source_char2id['<pad>']\n",
    "    \n",
    "    train_dataset = WordDataset(train_lp,device=device)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size,collate_fn=collate_fn, shuffle=True)\n",
    "    \n",
    "    ## creating test loader\n",
    "    test_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"test\",meta_tokens=use_meta_tokens)\n",
    "    \n",
    "    ## to make sure that the same vocabulary and dictionaries are used everywhere\n",
    "    test_lp.source_char2id = train_lp.source_char2id\n",
    "    test_lp.source_id2char = train_lp.source_id2char\n",
    "    test_lp.target_char2id = train_lp.target_char2id\n",
    "    test_lp.target_id2char = train_lp.target_id2char\n",
    "    \n",
    "    test_dataset = WordDataset(test_lp,device=device)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size,collate_fn=collate_fn, shuffle=True)\n",
    "    \n",
    "    ## creating validation loader\n",
    "    valid_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"valid\",meta_tokens=use_meta_tokens)\n",
    "    valid_lp.source_char2id = train_lp.source_char2id\n",
    "    valid_lp.source_id2char = train_lp.source_id2char\n",
    "    valid_lp.target_char2id = train_lp.target_char2id\n",
    "    valid_lp.target_id2char = train_lp.target_id2char\n",
    "    \n",
    "    valid_dataset = WordDataset(valid_lp,device=device)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size,collate_fn=collate_fn, shuffle=True)\n",
    "    \n",
    "    ##in principle these are all fixed across train/test/valid data\n",
    "    \n",
    "    #source_max_len = train_lp.source_max_len\n",
    "    #target_max_len = train_lp.target_max_len\n",
    "    \n",
    "    input_size = len(list(train_lp.source_char2id.keys()))\n",
    "    output_size = len(list(train_lp.target_char2id.keys()))\n",
    "    \n",
    "    hidden_size = config['hidden_size']\n",
    "    embedding_size = hidden_size\n",
    "    \n",
    "    epochs = config['epochs']\n",
    "    \n",
    "    optimiser = config['optimiser']\n",
    "    \n",
    "    weight_decay = config['weight_decay']\n",
    "    \n",
    "    lr = config['lr']\n",
    "    \n",
    "    num_encoder_layers = config['num_layers']\n",
    "    num_decoder_layers = num_encoder_layers\n",
    "    \n",
    "    ## Allowed Values : \"GRU\"/\"RNN\"/\"LSTM\" (not case sensitive)\n",
    "    rnn_type = config['rnn_type'].upper()\n",
    "    \n",
    "    bidirectional = config['bidirectional']\n",
    "    teacher_forcing_ratio = config['teacher_forcing_ratio']\n",
    "\n",
    "    teacher_forcing = False\n",
    "    \n",
    "    if teacher_forcing_ratio>0:\n",
    "        teacher_forcing = True\n",
    "    \n",
    "    \n",
    "    #loss_criterion =  nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "    \n",
    "    dropout=config['dropout']\n",
    "    \n",
    "    encoder = EncoderRNN(input_size = input_size, hidden_size = hidden_size,embedding_size=embedding_size,rnn_type = rnn_type,padding_index=pad_token_id,num_layers=num_encoder_layers,bidirectional=bidirectional,dropout=dropout).to(device)\n",
    "    \n",
    "    decoder = AttnDecoderRNN(hidden_size = hidden_size,embedding_size=embedding_size, output_size = output_size,rnn_type = rnn_type,max_len = train_lp.max_len,batch_size = batch_size,start_token_id = start_token_id, padding_index = None,num_layers = num_decoder_layers,bidirectional = bidirectional,dropout=dropout,device=device).to(device)\n",
    "    \n",
    "    #train(train_loader,valid_loader, encoder, decoder, 3,loss_criterion=loss_criterion, print_every=3, plot_every=5,device=device,teacher_forcing = teacher_forcing,teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "\n",
    "    train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae6b5d29-a3b9-4737-ae30-202c96f8a2d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4f7924da1d4646806b8d0abb6f7751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522fe78064a846d39433823c31a83655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\t Train Loss : 0.9818\t Train Acc : 25.38% \t Val Loss : 0.0117\t Val Acc : 44.36%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd07e07e975a42c0b87a632db16a77dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\t Train Loss : 0.388\t Train Acc : 50.45% \t Val Loss : 0.0109\t Val Acc : 49.39%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae578e5ea4b4690897182e8c632278f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\t Train Loss : 0.3316\t Train Acc : 54.96% \t Val Loss : 0.0104\t Val Acc : 50.93%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f797a2ccd7a54f3baf838e9bb086bb16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\t Train Loss : 0.3085\t Train Acc : 57.15% \t Val Loss : 0.0101\t Val Acc : 51.27%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b259c3f01554ed3a77ea987214c035a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\t Train Loss : 0.2933\t Train Acc : 58.81% \t Val Loss : 0.01\t Val Acc : 51.39%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48bd7352f9674113926cdac01a8e5c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\t Train Loss : 0.2844\t Train Acc : 59.66% \t Val Loss : 0.0098\t Val Acc : 52.32%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed1b66b04d5446aab18c0cd59abce6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 53\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"config = {\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m    'hidden_size':128,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m        \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m        'weight_decay': 0\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m}\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     29\u001b[0m         \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1e-5\u001b[39m\n\u001b[1;32m     51\u001b[0m }\n\u001b[0;32m---> 53\u001b[0m \u001b[43msetup_and_start_expt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwandb_log\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mkaggle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 95\u001b[0m, in \u001b[0;36msetup_and_start_expt\u001b[0;34m(config, wandb_log, kaggle)\u001b[0m\n\u001b[1;32m     91\u001b[0m decoder \u001b[38;5;241m=\u001b[39m AttnDecoderRNN(hidden_size \u001b[38;5;241m=\u001b[39m hidden_size,embedding_size\u001b[38;5;241m=\u001b[39membedding_size, output_size \u001b[38;5;241m=\u001b[39m output_size,rnn_type \u001b[38;5;241m=\u001b[39m rnn_type,max_len \u001b[38;5;241m=\u001b[39m train_lp\u001b[38;5;241m.\u001b[39mmax_len,batch_size \u001b[38;5;241m=\u001b[39m batch_size,start_token_id \u001b[38;5;241m=\u001b[39m start_token_id, padding_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,num_layers \u001b[38;5;241m=\u001b[39m num_decoder_layers,bidirectional \u001b[38;5;241m=\u001b[39m bidirectional,dropout\u001b[38;5;241m=\u001b[39mdropout,device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m#train(train_loader,valid_loader, encoder, decoder, 3,loss_criterion=loss_criterion, print_every=3, plot_every=5,device=device,teacher_forcing = teacher_forcing,teacher_forcing_ratio=teacher_forcing_ratio)\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43mteacher_forcing\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing\u001b[49m\u001b[43m,\u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwandb_logging\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mwandb_log\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataloader, valid_dataloader, encoder, decoder, n_epochs, padding_idx, optimiser, loss, weight_decay, lr, teacher_forcing, teacher_forcing_ratio, device, print_every, plot_every, wandb_logging)\u001b[0m\n\u001b[1;32m     24\u001b[0m lp \u001b[38;5;241m=\u001b[39m train_dataloader\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mlp\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m---> 28\u001b[0m     train_loss,train_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     print_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n\u001b[1;32m     30\u001b[0m     plot_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\n",
      "Cell \u001b[0;32mIn[10], line 52\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, teacher_forcing_ratio, ignore_padding)\u001b[0m\n\u001b[1;32m     46\u001b[0m     tot_words \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_words\n\u001b[1;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\n\u001b[1;32m     49\u001b[0m     decoder_outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, decoder_outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)),\n\u001b[1;32m     50\u001b[0m     target_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     51\u001b[0m )\n\u001b[0;32m---> 52\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     55\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/dl/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"config = {\n",
    "    'hidden_size':128,\n",
    "        \n",
    "        'embedding_size':128,\n",
    "\n",
    "        'rnn_type' : \"gru\",\n",
    "        \n",
    "        'batch_size':256,\n",
    "        \n",
    "        'optimiser': \"adam\",\n",
    "\n",
    "        'num_layers' : 2,\n",
    "\n",
    "        'lr':  1e-3,\n",
    "\n",
    "        'dropout' : 0.1,\n",
    "        \n",
    "        'epochs' : 15,\n",
    "\n",
    "        'teacher_forcing_ratio' : 0.3,\n",
    "\n",
    "        'bidirectional' :  True,\n",
    "    \n",
    "        'weight_decay': 0\n",
    "}\"\"\"\n",
    "\n",
    "config = {\n",
    "    'hidden_size':512,\n",
    "        \n",
    "        'embedding_size':512,\n",
    "\n",
    "        'rnn_type' : \"lstm\",\n",
    "        \n",
    "        'batch_size':64,\n",
    "        \n",
    "        'optimiser': \"nadam\",\n",
    "\n",
    "        'num_layers' : 3,\n",
    "\n",
    "        'lr':  1e-3,\n",
    "\n",
    "        'dropout' : 0.4,\n",
    "        \n",
    "        'epochs' : 15,\n",
    "\n",
    "        'teacher_forcing_ratio' : 0.4,\n",
    "\n",
    "        'bidirectional' :  True,\n",
    "    \n",
    "        'weight_decay': 1e-5\n",
    "}\n",
    "\n",
    "setup_and_start_expt(config,wandb_log = False,kaggle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f803e738-f7bb-4833-90ce-c1d53c0284c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m036\u001b[0m (\u001b[33mtmajestical\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/tejasmalladi/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 8haktxy5\n",
      "Sweep URL: https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key=\"\")\n",
    "\n",
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'name' : 'PA3 Hyper Sweep Attention GRU',\n",
    "    'metric': {\n",
    "      'name': 'Validation accuracy',\n",
    "      'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        \n",
    "         'hidden_size':{\n",
    "            'values':[64,128,256,512]\n",
    "        },\n",
    "        \n",
    "        'embedding_size':{\n",
    "            'values':[64,128,256,512]\n",
    "        },\n",
    "\n",
    "        'rnn_type':{\n",
    "            'values':['gru','lstm','rnn']\n",
    "        },\n",
    "        \n",
    "        'batch_size':{\n",
    "            'values':[64,128,256,512]\n",
    "        },\n",
    "        \n",
    "        'optimiser': {\n",
    "            'values': [\"adam\",\"rmsprop\",\"nadam\"]\n",
    "        },\n",
    "\n",
    "        'num_layers' :{\n",
    "            'values' : [1,2,3,4]\n",
    "        },\n",
    "\n",
    "        'lr': {\n",
    "            'values': [1e-2,1e-3,1e-4,3e-4]\n",
    "        },\n",
    "\n",
    "        'dropout' : {\n",
    "\n",
    "            'values' : [0,0.1,0.2,0.3,0.4]\n",
    "        },\n",
    "        \n",
    "        'epochs' : {\n",
    "\n",
    "            'values' : [15]\n",
    "        },\n",
    "\n",
    "        'teacher_forcing_ratio' : {\n",
    "            'values' : [0,0.1,0.2,0.3,0.4,0.5]\n",
    "        },\n",
    "\n",
    "        'bidirectional' : {\n",
    "            'values' : [True,False]\n",
    "        },\n",
    "        'weight_decay': {\n",
    "            'values': [0,1e-3,1e-4,1e-5]\n",
    "        },\n",
    "        }\n",
    "    }\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project='JV_CS23M036_TEJASVI_DL_ASSIGNMENT3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8e00c26-5404-4118-b8a7-61d65d00fb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: eibr0j71 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: rnn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tejasmalladi/Documents/OM NAMO VENKATESAYA/Jai Vigneshwara IIT MADRAS/JV SEM2/JV Deep Learning/JV Assignments/JV-Deep-Learning-Assignment3/wandb/run-20240512_125255-eibr0j71</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/eibr0j71' target=\"_blank\">honest-sweep-1</a></strong> to <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/eibr0j71' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/eibr0j71</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574c0c9f59f5455c809a168a26394fb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e268c97aff740b194bd29e94a4a8fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\t Train Loss : 2.9608\t Train Acc : 1.07% \t Val Loss : 0.0087\t Val Acc : 1.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "    setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "    train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "    wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "NameError: name 'val_accuracy' is not defined\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">honest-sweep-1</strong> at: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/eibr0j71' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/eibr0j71</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240512_125255-eibr0j71/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run eibr0j71 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tejasmalladi/miniconda3/envs/dl/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "    setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "    train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "    wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "NameError: name 'val_accuracy' is not defined\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run eibr0j71 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/tejasmalladi/miniconda3/envs/dl/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'val_accuracy' is not defined\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9ygb3s5f with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: gru\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tejasmalladi/Documents/OM NAMO VENKATESAYA/Jai Vigneshwara IIT MADRAS/JV SEM2/JV Deep Learning/JV Assignments/JV-Deep-Learning-Assignment3/wandb/run-20240512_125404-9ygb3s5f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/9ygb3s5f' target=\"_blank\">glamorous-sweep-2</a></strong> to <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/9ygb3s5f' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/9ygb3s5f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ee57d0b1a3a48639577426a9e2218a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a12c6a5f0b048218b34aa05b7fafd76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\t Train Loss : 1.9392\t Train Acc : 3.88% \t Val Loss : 0.0303\t Val Acc : 4.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "    setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "    train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "    wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "NameError: name 'val_accuracy' is not defined\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">glamorous-sweep-2</strong> at: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/9ygb3s5f' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/9ygb3s5f</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240512_125404-9ygb3s5f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 9ygb3s5f errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tejasmalladi/miniconda3/envs/dl/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "    setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "    train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "    wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "NameError: name 'val_accuracy' is not defined\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 9ygb3s5f errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/tejasmalladi/miniconda3/envs/dl/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'val_accuracy' is not defined\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2cb1t69t with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: rnn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tejasmalladi/Documents/OM NAMO VENKATESAYA/Jai Vigneshwara IIT MADRAS/JV SEM2/JV Deep Learning/JV Assignments/JV-Deep-Learning-Assignment3/wandb/run-20240512_125613-2cb1t69t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/2cb1t69t' target=\"_blank\">earthy-sweep-3</a></strong> to <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/2cb1t69t' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/2cb1t69t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e52ec0dc25944bec834edd5f5b8c973e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff09ab760604cffa792dc9f973ee7d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\t Train Loss : 2.7948\t Train Acc : 2.49% \t Val Loss : 0.0299\t Val Acc : 4.1%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "    setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "    train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "    wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "NameError: name 'val_accuracy' is not defined\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">earthy-sweep-3</strong> at: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/2cb1t69t' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/2cb1t69t</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240512_125613-2cb1t69t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 2cb1t69t errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tejasmalladi/miniconda3/envs/dl/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "    setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "    train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "    wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "NameError: name 'val_accuracy' is not defined\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 2cb1t69t errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/tejasmalladi/miniconda3/envs/dl/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'val_accuracy' is not defined\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1vwzofhh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.01\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: rnn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tejasmalladi/Documents/OM NAMO VENKATESAYA/Jai Vigneshwara IIT MADRAS/JV SEM2/JV Deep Learning/JV Assignments/JV-Deep-Learning-Assignment3/wandb/run-20240512_125842-1vwzofhh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/1vwzofhh' target=\"_blank\">volcanic-sweep-4</a></strong> to <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/1vwzofhh' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/1vwzofhh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc694fd4a93e48d7b925b90c8ae24cab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23893bd365ed49fe98b829da35c615c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\t Train Loss : 3.507\t Train Acc : 2.42% \t Val Loss : 0.0539\t Val Acc : 2.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "    setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "    train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "    wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "NameError: name 'val_accuracy' is not defined\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1837b7ce274309a68fc5d6bff686d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-sweep-4</strong> at: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/1vwzofhh' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/1vwzofhh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240512_125842-1vwzofhh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 1vwzofhh errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tejasmalladi/miniconda3/envs/dl/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "    setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "    train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "    wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "NameError: name 'val_accuracy' is not defined\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 1vwzofhh errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/tejasmalladi/miniconda3/envs/dl/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'val_accuracy' is not defined\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7d0eq1el with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: rnn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tejasmalladi/Documents/OM NAMO VENKATESAYA/Jai Vigneshwara IIT MADRAS/JV SEM2/JV Deep Learning/JV Assignments/JV-Deep-Learning-Assignment3/wandb/run-20240512_130038-7d0eq1el</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/7d0eq1el' target=\"_blank\">absurd-sweep-5</a></strong> to <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/7d0eq1el' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/7d0eq1el</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62336c03456b46e1aedb81f649d989c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cd9a1a85c549139e5060672cc744d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\t Train Loss : 2.7038\t Train Acc : 1.46% \t Val Loss : 0.0161\t Val Acc : 2.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "    setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "    train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "    wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "NameError: name 'val_accuracy' is not defined\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e67ea5f47646438b97b914d9bf1d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">absurd-sweep-5</strong> at: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/7d0eq1el' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/7d0eq1el</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240512_130038-7d0eq1el/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 7d0eq1el errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tejasmalladi/miniconda3/envs/dl/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "    setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "    train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "    wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "NameError: name 'val_accuracy' is not defined\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 7d0eq1el errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/tejasmalladi/miniconda3/envs/dl/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'val_accuracy' is not defined\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bb23h3rf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlr: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimiser: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \trnn_type: lstm\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tejasmalladi/Documents/OM NAMO VENKATESAYA/Jai Vigneshwara IIT MADRAS/JV SEM2/JV Deep Learning/JV Assignments/JV-Deep-Learning-Assignment3/wandb/run-20240512_130159-bb23h3rf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/bb23h3rf' target=\"_blank\">visionary-sweep-6</a></strong> to <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/sweeps/8haktxy5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/bb23h3rf' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/bb23h3rf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5623d6070f348d1bc08048cbd060deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e1bd4dfb27453883e64dc9fb16ca12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\t Train Loss : 1.5307\t Train Acc : 11.87% \t Val Loss : 0.0085\t Val Acc : 31.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "    setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "    train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "    wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "NameError: name 'val_accuracy' is not defined\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8616d205172140dcb3e7918aec931d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">visionary-sweep-6</strong> at: <a href='https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/bb23h3rf' target=\"_blank\">https://wandb.ai/tmajestical/JV_CS23M036_TEJASVI_DL_ASSIGNMENT3/runs/bb23h3rf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240512_130159-bb23h3rf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run bb23h3rf errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/tejasmalladi/miniconda3/envs/dl/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "    setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "    train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "  File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "    wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "NameError: name 'val_accuracy' is not defined\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run bb23h3rf errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/Users/tejasmalladi/miniconda3/envs/dl/lib/python3.8/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/2210584461.py\", line 20, in main\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/982323058.py\", line 95, in setup_and_start_expt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     train(train_loader,valid_loader, encoder, decoder, n_epochs = epochs,padding_idx = pad_token_id,optimiser = optimiser,weight_decay=weight_decay, lr=lr,teacher_forcing = teacher_forcing,teacher_forcing_ratio = teacher_forcing_ratio,device=device,wandb_logging = wandb_log)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/var/folders/z4/__x0n5jn4f1c4z527hs4r4_00000gn/T/ipykernel_8571/1550435551.py\", line 35, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     wandb.log({'epoch': epoch,'train loss': train_loss, 'train accuracy': train_accuracy, 'Validation loss': val_loss, 'Validation accuracy': val_accuracy})\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'val_accuracy' is not defined\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "Detected 5 failed runs in a row at start, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 5 failed runs in a row at start, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: To change this value set WANDB_AGENT_MAX_INITIAL_FAILURES=val\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    '''\n",
    "    WandB calls main function each time with differnet combination.\n",
    "\n",
    "    We can retrive the same and use the same values for our hypermeters.\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    with wandb.init() as run:\n",
    "\n",
    "        run_name=\"-hl_\"+str(wandb.config.num_layers)+\"-hs_\"+str(wandb.config.hidden_size)+\"-es_\"+str(wandb.config.hidden_size)+\"-biDir_\"+str(wandb.config.bidirectional)\n",
    "\n",
    "        run_name = run_name+\"-rnn_type_\"+str(wandb.config.rnn_type)+run_name+\"-optim_\"+str(wandb.config.optimiser)+\"-lr_\"+str(wandb.config.lr)+\"-reg_\"+str(wandb.config.weight_decay)+\"-epochs_\"+str(wandb.config.epochs)+\"-tf_ratio_\"+str(wandb.config.teacher_forcing_ratio)\n",
    "\n",
    "        run_name = run_name+\"-dropout_\"+str(wandb.config.dropout)\n",
    "\n",
    "        wandb.run.name=run_name\n",
    "\n",
    "        setup_and_start_expt(wandb.config,wandb_log = True,kaggle=False)\n",
    "\n",
    "\n",
    "wandb.agent(sweep_id, function=main,count=10) # calls main function for count number of times.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce3a580-730f-49ff-b4e2-ba337c1394ea",
   "metadata": {},
   "source": [
    "#### To Do\n",
    "\n",
    "1. ~Add support for multi-layers [currently bidirectional is only supported]~\n",
    "2. ~Ignore Padding Index~\n",
    "3. Add support for LSTM and RNN\n",
    "4. Include Teacher forcing, with ratio.\n",
    "5. Add support for eval mode : to predict for individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e908c4f-af04-4ff8-95e4-875c6e5a156b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae96bcd-a13c-45c5-887f-fd5f2045abdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
