{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c7dcbe7-698c-4e1a-b53f-56a11603ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75927828-0e72-4da3-a840-98d46c1e6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d84851-1726-4bfb-955d-209cdc80bc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094edcd3-d8da-4e5b-b48e-413e23060dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageProcessor:\n",
    "\n",
    "    def __init__(self,language_directory,target_lang_name,mode=\"train\",meta_tokens=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Default Constructor for this class.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            language_directory : ex : \"aksharantar_sampled/tel/\"\n",
    "            mode : \"train\" or \"test\" or \"valid\", accordingly the appropriate dataset is read.\n",
    "            meta_tokens : If true creates the first three tokens of the dictionary as <start>,<end>,<pad>.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        self.meta_tokens = meta_tokens\n",
    "        self.language_directory = language_directory\n",
    "        self.target_lang_name = target_lang\n",
    "        self.mode = mode\n",
    "    \n",
    "        self.source_lang = 0\n",
    "        self.target_lang = 1\n",
    "\n",
    "        self.source_max_len = self.find_max_len(self.source_lang)\n",
    "        self.target_max_len = self.find_max_len(self.target_lang)\n",
    "\n",
    "        self.max_len = max(self.source_max_len,self.target_max_len)\n",
    "\n",
    "        self.source_char2id,self.source_id2char = self.build_char_vocab(self.source_lang,self.source_max_len)\n",
    "        self.target_char2id,self.target_id2char = self.build_char_vocab(self.target_lang,self.target_max_len)\n",
    "\n",
    "\n",
    "    def find_max_len(self,lang):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to find the maximum length of a word across train/test and validation data.\n",
    "\n",
    "        This would help in padding, the embedding accordingly.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            lang : 0/1 (source/target) language for which the length of the longest word must be found.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        train_df = pd.read_csv(self.language_directory+self.target_lang_name+\"_train.csv\",header=None)\n",
    "        test_df = pd.read_csv(self.language_directory+self.target_lang_name+\"_test.csv\",header=None)\n",
    "        valid_df = pd.read_csv(self.language_directory+self.target_lang_name+\"_valid.csv\",header=None)\n",
    "\n",
    "        train_max_len = len(max(list(train_df[lang]), key = len))\n",
    "        test_max_len = len(max(list(test_df[lang]), key = len))\n",
    "        valid_max_len = len(max(list(valid_df[lang]), key = len))\n",
    "\n",
    "        del train_df\n",
    "        del test_df\n",
    "        del valid_df\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        return max(train_max_len,test_max_len,valid_max_len)\n",
    "\n",
    "    def build_char_vocab(self,lang_id,max_len=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to create a vocabulary of characters in language corresponding to lang_id.\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_csv(self.language_directory+self.target_lang_name+\"_\"+self.mode+\".csv\",header=None)\n",
    "\n",
    "        self.data = df.to_numpy()\n",
    "\n",
    "        lang_chars = []\n",
    "        lang_words = df[lang_id].to_numpy()\n",
    "    \n",
    "        for word in lang_words:\n",
    "            lang_chars += list(word)\n",
    "    \n",
    "        unique_lang_chars =  sorted(list(set(lang_chars)))\n",
    "    \n",
    "        start = 0\n",
    "        \n",
    "        if self.meta_tokens:\n",
    "            char2id_dict = {'<start>':0,'<end>':1,'<pad>': 2}\n",
    "            id2char_dict = {0:'<start>',1:'<end>',2:'<pad>'}\n",
    "            start = 3\n",
    "        else:\n",
    "            char2id_dict = {}\n",
    "            id2char_dict = {}\n",
    "    \n",
    "        for i in range(len(unique_lang_chars)):\n",
    "            char2id_dict[unique_lang_chars[i]] = i+start\n",
    "            id2char_dict[i+start] = unique_lang_chars[i]\n",
    "    \n",
    "        del df\n",
    "        del lang_chars\n",
    "        del unique_lang_chars\n",
    "\n",
    "        gc.collect()\n",
    "    \n",
    "        return char2id_dict,id2char_dict\n",
    "\n",
    "    def encode_word(self,word,lang_id,padding=False,append_eos = True):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to encode characters of a given word.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            word: The word to be encoded.\n",
    "            lang_id : 0/1 for source/target lang.\n",
    "            padding : If true, the word encoding would be padded upto max len.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if lang_id == self.source_lang:\n",
    "            char2id_dict = self.source_char2id\n",
    "            \n",
    "        else:\n",
    "            char2id_dict = self.target_char2id\n",
    "        \n",
    "        max_len = self.max_len\n",
    "        \n",
    "        word_encoding = []\n",
    "        \n",
    "        for i in word:\n",
    "            word_encoding.append(char2id_dict[i])\n",
    "\n",
    "        if padding:\n",
    "            word_encoding += [char2id_dict['<pad>']] * (max_len - len(word_encoding))\n",
    "\n",
    "        if append_eos:\n",
    "            word_encoding.append(char2id_dict['<end>'])\n",
    "        \n",
    "        return np.array(word_encoding)\n",
    "\n",
    "    def decode_word(self,code_word,lang_id):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to decode an encoded word.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            code_word : The encoded word.\n",
    "            lang_id : 0/1 for source/target lang.\n",
    "        \"\"\"\n",
    "    \n",
    "        word = []\n",
    "\n",
    "        if lang_id == self.source_lang:\n",
    "            id2char_dict = self.source_id2char\n",
    "            char2id_dict = self.source_char2id\n",
    "            \n",
    "        else:\n",
    "            id2char_dict = self.target_id2char\n",
    "            char2id_dict = self.target_char2id\n",
    "        \n",
    "        for i in code_word:\n",
    "            ## if we reached padding, then stop decoding\n",
    "            if self.meta_tokens and i == char2id_dict['<pad>']:\n",
    "                break\n",
    "            \n",
    "            word.append(id2char_dict[i])\n",
    "            \n",
    "        return np.array(word)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e5df96-e649-4354-852d-85f7116fd797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, language_processor,device='cpu'):\n",
    "\n",
    "        self.lp = language_processor\n",
    "        self.data = self.lp.data\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_word, output_word = self.data[idx]\n",
    "        \n",
    "        input_sequence = self.lp.encode_word(input_word,self.lp.source_lang,padding=True,append_eos=True)\n",
    "        output_sequence = self.lp.encode_word(output_word,self.lp.target_lang,padding=True,append_eos=True)\n",
    "        \n",
    "        return torch.tensor(input_sequence).to(device), torch.tensor(output_sequence).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c05ce5-5d0e-4e8b-89bd-5f6ef688eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Data Loading\n",
    "batch_size = 32\n",
    "\n",
    "base_dir = \"aksharantar_sampled/\"\n",
    "target_lang = \"tel\"\n",
    "\n",
    "use_meta_tokens = True\n",
    "\n",
    "lang_dir = base_dir + target_lang + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "751abaf1-9aa9-404b-91aa-cb30403695a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "##creating train loader\n",
    "train_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"train\",meta_tokens=use_meta_tokens)\n",
    "train_dataset = WordDataset(train_lp,device=device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "## creating test loader\n",
    "test_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"test\",meta_tokens=use_meta_tokens)\n",
    "test_dataset = WordDataset(test_lp,device=device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "## creating validation loader\n",
    "valid_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"valid\",meta_tokens=use_meta_tokens)\n",
    "valid_dataset = WordDataset(valid_lp,device=device)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5779acf7-6962-4416-a218-5f83778dd835",
   "metadata": {},
   "outputs": [],
   "source": [
    "##in principle these are all fixed across train/test/valid data\n",
    "\n",
    "start_token_id = train_lp.source_char2id['<start>']\n",
    "end_token_id = train_lp.source_char2id['<end>']\n",
    "pad_token_id = train_lp.source_char2id['<pad>']\n",
    "\n",
    "source_max_len = train_lp.source_max_len\n",
    "target_max_len = train_lp.target_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d39e84-a7e5-4344-af5d-ed91b8c5d2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19bc8b4b-40a5-41cc-9e3a-e4f081c427be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
