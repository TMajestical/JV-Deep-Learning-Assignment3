{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c7dcbe7-698c-4e1a-b53f-56a11603ae20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75927828-0e72-4da3-a840-98d46c1e6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d84851-1726-4bfb-955d-209cdc80bc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "094edcd3-d8da-4e5b-b48e-413e23060dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageProcessor:\n",
    "\n",
    "    def __init__(self,language_directory,target_lang_name,mode=\"train\",meta_tokens=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Default Constructor for this class.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            language_directory : ex : \"aksharantar_sampled/tel/\"\n",
    "            mode : \"train\" or \"test\" or \"valid\", accordingly the appropriate dataset is read.\n",
    "            meta_tokens : If true creates the first three tokens of the dictionary as <start>,<end>,<pad>.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        self.meta_tokens = meta_tokens\n",
    "        self.language_directory = language_directory\n",
    "        self.target_lang_name = target_lang\n",
    "        self.mode = mode\n",
    "    \n",
    "        self.source_lang = 0\n",
    "        self.target_lang = 1\n",
    "\n",
    "        self.source_max_len = self.find_max_len(self.source_lang)\n",
    "        self.target_max_len = self.find_max_len(self.target_lang)\n",
    "\n",
    "        self.max_len = max(self.source_max_len,self.target_max_len)\n",
    "\n",
    "        self.source_char2id,self.source_id2char = self.build_char_vocab(self.source_lang,self.source_max_len)\n",
    "        self.target_char2id,self.target_id2char = self.build_char_vocab(self.target_lang,self.target_max_len)\n",
    "\n",
    "\n",
    "    def find_max_len(self,lang):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to find the maximum length of a word across train/test and validation data.\n",
    "\n",
    "        This would help in padding, the embedding accordingly.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            lang : 0/1 (source/target) language for which the length of the longest word must be found.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        train_df = pd.read_csv(self.language_directory+self.target_lang_name+\"_train.csv\",header=None)\n",
    "        test_df = pd.read_csv(self.language_directory+self.target_lang_name+\"_test.csv\",header=None)\n",
    "        valid_df = pd.read_csv(self.language_directory+self.target_lang_name+\"_valid.csv\",header=None)\n",
    "\n",
    "        train_max_len = len(max(list(train_df[lang]), key = len))\n",
    "        test_max_len = len(max(list(test_df[lang]), key = len))\n",
    "        valid_max_len = len(max(list(valid_df[lang]), key = len))\n",
    "\n",
    "        del train_df\n",
    "        del test_df\n",
    "        del valid_df\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        return max(train_max_len,test_max_len,valid_max_len)\n",
    "\n",
    "    def build_char_vocab(self,lang_id,max_len=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to create a vocabulary of characters in language corresponding to lang_id.\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_csv(self.language_directory+self.target_lang_name+\"_\"+self.mode+\".csv\",header=None)\n",
    "\n",
    "        self.data = df.to_numpy()\n",
    "\n",
    "        lang_chars = []\n",
    "        lang_words = df[lang_id].to_numpy()\n",
    "    \n",
    "        for word in lang_words:\n",
    "            lang_chars += list(word)\n",
    "    \n",
    "        unique_lang_chars =  sorted(list(set(lang_chars)))\n",
    "    \n",
    "        start = 0\n",
    "        \n",
    "        if self.meta_tokens:\n",
    "            char2id_dict = {'<start>':0,'<end>':1,'<pad>': 2}\n",
    "            id2char_dict = {0:'<start>',1:'<end>',2:'<pad>'}\n",
    "            start = 3\n",
    "        else:\n",
    "            char2id_dict = {}\n",
    "            id2char_dict = {}\n",
    "    \n",
    "        for i in range(len(unique_lang_chars)):\n",
    "            char2id_dict[unique_lang_chars[i]] = i+start\n",
    "            id2char_dict[i+start] = unique_lang_chars[i]\n",
    "    \n",
    "        del df\n",
    "        del lang_chars\n",
    "        del unique_lang_chars\n",
    "\n",
    "        gc.collect()\n",
    "    \n",
    "        return char2id_dict,id2char_dict\n",
    "\n",
    "    def encode_word(self,word,lang_id,padding=False,append_eos = True):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to encode characters of a given word.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            word: The word to be encoded.\n",
    "            lang_id : 0/1 for source/target lang.\n",
    "            padding : If true, the word encoding would be padded upto max len.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if lang_id == self.source_lang:\n",
    "            char2id_dict = self.source_char2id\n",
    "            \n",
    "        else:\n",
    "            char2id_dict = self.target_char2id\n",
    "        \n",
    "        max_len = self.max_len\n",
    "        \n",
    "        word_encoding = []\n",
    "        \n",
    "        for i in word.lower():\n",
    "            word_encoding.append(char2id_dict[i])\n",
    "\n",
    "        if padding:\n",
    "            word_encoding += [char2id_dict['<pad>']] * (max_len - len(word_encoding))\n",
    "\n",
    "        if append_eos:\n",
    "            word_encoding.append(char2id_dict['<end>'])\n",
    "        \n",
    "        return np.array(word_encoding)\n",
    "\n",
    "    def decode_word(self,code_word,lang_id):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to decode an encoded word.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            code_word : The encoded word.\n",
    "            lang_id : 0/1 for source/target lang.\n",
    "        \"\"\"\n",
    "    \n",
    "        word = []\n",
    "\n",
    "        if lang_id == self.source_lang:\n",
    "            id2char_dict = self.source_id2char\n",
    "            char2id_dict = self.source_char2id\n",
    "            \n",
    "        else:\n",
    "            id2char_dict = self.target_id2char\n",
    "            char2id_dict = self.target_char2id\n",
    "        \n",
    "        for i in code_word:\n",
    "            ## if we reached padding, then stop decoding\n",
    "            if self.meta_tokens and i == char2id_dict['<pad>']:\n",
    "                break\n",
    "            \n",
    "            word.append(id2char_dict[i])\n",
    "            \n",
    "        return np.array(word)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1e5df96-e649-4354-852d-85f7116fd797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, language_processor,append_eos=True,device='cpu'):\n",
    "\n",
    "        self.lp = language_processor\n",
    "        self.data = self.lp.data\n",
    "        self.device = device\n",
    "        self.append_eos = append_eos\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_word, output_word = self.data[idx]\n",
    "        \n",
    "        input_sequence = self.lp.encode_word(input_word,self.lp.source_lang,padding=True,append_eos=True)\n",
    "        output_sequence = self.lp.encode_word(output_word,self.lp.target_lang,padding=True,append_eos=True)\n",
    "        \n",
    "        return torch.tensor(input_sequence).to(device), torch.tensor(output_sequence).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c05ce5-5d0e-4e8b-89bd-5f6ef688eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "base_dir = \"aksharantar_sampled/\"\n",
    "target_lang = \"tel\"\n",
    "\n",
    "use_meta_tokens = True\n",
    "append_eos = 1\n",
    "\n",
    "lang_dir = base_dir + target_lang + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "751abaf1-9aa9-404b-91aa-cb30403695a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "##creating train loader\n",
    "train_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"train\",meta_tokens=use_meta_tokens)\n",
    "train_dataset = WordDataset(train_lp,device=device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "## creating test loader\n",
    "test_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"test\",meta_tokens=use_meta_tokens)\n",
    "test_dataset = WordDataset(test_lp,device=device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "## creating validation loader\n",
    "valid_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"valid\",meta_tokens=use_meta_tokens)\n",
    "valid_dataset = WordDataset(valid_lp,device=device)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87c677-8931-4b8b-8cf3-97cd597e2065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5779acf7-6962-4416-a218-5f83778dd835",
   "metadata": {},
   "outputs": [],
   "source": [
    "##in principle these are all fixed across train/test/valid data\n",
    "\n",
    "start_token_id = train_lp.source_char2id['<start>']\n",
    "end_token_id = train_lp.source_char2id['<end>']\n",
    "pad_token_id = train_lp.source_char2id['<pad>']\n",
    "\n",
    "source_max_len = train_lp.source_max_len\n",
    "target_max_len = train_lp.target_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d39e84-a7e5-4344-af5d-ed91b8c5d2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3698224-b989-4dfd-99a0-0007c176a5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,padding_idx = None ,dropout_p=0.1,num_layers = 1,bidirectional = False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size,padding_idx = padding_idx)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3681ca-02e0-4ca2-94af-43af4f8f821b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e6c0c87-21d9-4609-9739-9df28e7844bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size,max_len,start_token_id,num_layers = 1,bidirectional = False):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.max_len = max_len\n",
    "        self.start_token_id = start_token_id\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        \n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(self.start_token_id)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(self.max_len):\n",
    "            ## print(i,self.max_len)\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
    "            ## print(decoder_output.shape)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        output = self.embedding(input)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb0d70-d255-49e2-b349-56e3be50e0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0b65188-48a0-4bf6-9bd9-cbab7ca05b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOne thing to take care of:\\n\\n    In encoder we set no grad for pad token in nn.Embed\\n    \\n    In decoder, \\n        => need to see what to do after actual word is generated. How to deal with padding?\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "One thing to take care of:\n",
    "\n",
    "    In encoder we set no grad for pad token in nn.Embed\n",
    "    \n",
    "    In decoder, \n",
    "        => need to see what to do after actual word is generated. How to deal with padding?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67c38af4-7bf7-4adf-ab8b-71ebb0b69416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,decoder_optimizer, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in dataloader:\n",
    "        ## print(f\"Data Shape : {data[0].shape,data[1].shape}\")\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        #print(decoder_outputs.shape,decoder_outputs.view(-1, decoder_outputs.size(-1)).shape,target_tensor.shape,target_tensor.view(-1).shape)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72aee733-e313-47eb-b041-7af96d5a65db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e907afc-4b4b-4f7b-8e90-e605ba441e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(dataloader,encoder,decoder,padding_token_id,ignore_padding = True,device='cpu'):\n",
    "\n",
    "    \"\"\"\n",
    "    Method to compute the accuracy using the model (encoder-decoder) using dataloader.\n",
    "\n",
    "    This method returns word and character level accuracy.\n",
    "\n",
    "        Word Level Accuracy : Accuracy is computed at the word level and a word is right iff every character is predicted correctly.\n",
    "        Char Level Accuracy : Accuracy is computed by comparing each predicted character wrt the correct char.\n",
    "\n",
    "    Params:\n",
    "\n",
    "        dataloader : The train/test/valid dataloader.\n",
    "        encoder : The encoder \n",
    "        decoder : The decoder\n",
    "        padding_token_id : The id of the padding token.\n",
    "        ignore_padding : If True, then in word level accuracy, the padding characters are ignored in computing the word level accuracy.\n",
    "                        char level accuracy, the padding characters are not considered at all.\n",
    "\n",
    "                        If false, padding is considered to be a part of the word (for word level accuracy) and \n",
    "    \"\"\"\n",
    "\n",
    "    char_lvl_accuracy = 0\n",
    "    word_level_accuracy = 0\n",
    "\n",
    "    tot_chars = 0\n",
    "    tot_words = 0\n",
    "\n",
    "    tot_correct_char_preds = 0\n",
    "    tot_correct_word_preds = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        \n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "\n",
    "        ## For a batch, for each character find the most probable output word.\n",
    "        multi_step_preds = torch.argmax(decoder_outputs,dim=2)\n",
    "        multi_step_pred_correctness = (multi_step_preds ==  target_tensor)\n",
    "        num_chars = multi_step_preds.numel() ##find the total number of characters in the current batch\n",
    "        num_words = multi_step_preds.shape[0] ##find the total number of words in the current batch.\n",
    "\n",
    "        if ignore_padding: ## if padding has to be ignored.\n",
    "\n",
    "            ## for each word, based on the padding token ID, find the first occurance of the padding token, marking the begining of padding.\n",
    "            padding_start = torch.argmax(target_tensor == padding_token_id,dim=1).to(device)\n",
    "            ## Creating a mask with 1's in each position of a padding token\n",
    "            mask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\n",
    "\n",
    "            ##doing a logical OR with the mask makes sure that the padding tokens do not affect the correctness of the word\n",
    "            tot_correct_word_preds += (torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum()).item()\n",
    "            tot_words += num_words\n",
    "\n",
    "            ##creating a complement of the mask so to mark padding tokens as irrelevant\n",
    "            complement_mask = (1-mask.int()).bool()\n",
    "            num_pad_chars = mask.int().sum().item()\n",
    "            ##counting number of non_pad_chars to compute accuracy.\n",
    "            num_non_pad_chars = num_chars - num_pad_chars\n",
    "\n",
    "            tot_correct_char_preds += (torch.logical_and(multi_step_pred_correctness,complement_mask).int().sum()).item()\n",
    "            tot_chars += num_non_pad_chars\n",
    "            \n",
    "    \n",
    "        else: ##otherwise.\n",
    "\n",
    "            tot_correct_word_preds += (torch.all(multi_step_pred_correctness,dim=1).int().sum()).item()\n",
    "            tot_words += num_words\n",
    "            \n",
    "            tot_correct_char_preds += (multi_step_pred_correctness.int().sum()).item()\n",
    "            tot_chars += num_chars\n",
    "\n",
    "    #print(tot_correct_char_preds,tot_chars)\n",
    "    #print(tot_correct_word_preds,tot_words)\n",
    "\n",
    "    char_lvl_accuracy = round(tot_correct_char_preds*100/tot_chars,2)\n",
    "    word_lvl_accuracy = round(tot_correct_word_preds*100/tot_words,2)\n",
    "\n",
    "    return char_lvl_accuracy,word_lvl_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f33e86bf-bdef-4939-9f94-66c10e15665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader,valid_loader, encoder, decoder, n_epochs, learning_rate=0.001,print_every=100, plot_every=100,device='cpu'):\n",
    "    \n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    lp = train_loader.dataset.lp\n",
    "    \n",
    "    criterion = nn.NLLLoss().to(device)\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        val_char_lvl_accuracy,val_word_level_accuracy = compute_accuracy(valid_loader,encoder,decoder,padding_token_id = lp.source_char2id['<pad>'],ignore_padding=True,device=device)\n",
    "\n",
    "        print(f\"Epoch {epoch}\\t C-Val Acc : {val_char_lvl_accuracy}%\\t W-Val Acc : {val_word_level_accuracy}%\")\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            #print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "            train_char_lvl_accuracy,train_word_level_accuracy = compute_accuracy(train_loader,encoder,decoder,padding_token_id = lp.source_char2id['<pad>'],ignore_padding=True,device=device)\n",
    "            print(f\"Epoch {epoch}\\t C-Train Acc : {train_char_lvl_accuracy}%\\t W-Train Acc : {train_word_level_accuracy}%\")\n",
    "            \n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca27dfe-606c-4a1e-8e26-981dc71f9b44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acd82d50-cf4c-4b8f-9905-ee4d07cde051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multi_step_preds = torch.argmax(decoder_ops,dim=2)\\n\\n(multi_step_preds ==  target_tensor).int().sum()\\n\\nnum_chars = multi_step_preds.numel()\\nnum_words = multi_step_preds.shape[0]\\n\\nmulti_step_pred_correctness = (multi_step_preds == target_tensor)\\n\\n\\nadding_start = torch.argmax(target_tensor == 2,dim=1).to(device)\\nmask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\\nword_level_accuracy = round(((torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum())/num_words).item()*100,2)\\n\\ncomplement_mask = (1-mask.int()).bool()\\nnum_pad_chars = mask.int().sum()\\nnum_non_pad_chars = num_chars - num_pad_chars\\n\\nmulti_step_pred_correctness_for_non_pad_chars = torch.logical_and(multi_step_pred_correctness,complement_mask).int().sum()/num_non_pad_chars\\nchar_lvl_accuracy = round(multi_step_pred_correctness_for_non_pad_chars.item()*100, 2)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"multi_step_preds = torch.argmax(decoder_ops,dim=2)\n",
    "\n",
    "(multi_step_preds ==  target_tensor).int().sum()\n",
    "\n",
    "num_chars = multi_step_preds.numel()\n",
    "num_words = multi_step_preds.shape[0]\n",
    "\n",
    "multi_step_pred_correctness = (multi_step_preds == target_tensor)\n",
    "\n",
    "\n",
    "adding_start = torch.argmax(target_tensor == 2,dim=1).to(device)\n",
    "mask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\n",
    "word_level_accuracy = round(((torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum())/num_words).item()*100,2)\n",
    "\n",
    "complement_mask = (1-mask.int()).bool()\n",
    "num_pad_chars = mask.int().sum()\n",
    "num_non_pad_chars = num_chars - num_pad_chars\n",
    "\n",
    "multi_step_pred_correctness_for_non_pad_chars = torch.logical_and(multi_step_pred_correctness,complement_mask).int().sum()/num_non_pad_chars\n",
    "char_lvl_accuracy = round(multi_step_pred_correctness_for_non_pad_chars.item()*100, 2)\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e03b50a-c511-4dc7-b976-304a97cff25b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44f32dff-d6d8-465f-a75a-c878d3d4bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0bf6186-c346-41f5-b23a-70ac8dec092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, word, language_processor,device = \"cpu\"):\n",
    "\n",
    "    lp = language_processor\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        input_tensor = torch.tensor(lp.encode_word(word,lp.source_lang,padding=True,append_eos = True)).to(device).view(1,-1)\n",
    "\n",
    "        print(input_tensor.shape)\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "        print(encoder_outputs.shape, encoder_hidden.shape)\n",
    "        \n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_chars = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == end_token_id:\n",
    "                decoded_chars.append('<end>')\n",
    "                break\n",
    "            decoded_chars.append(lp.target_id2char[idx.item()])\n",
    "    return decoded_chars, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22585702-e9b2-4dcb-8260-45a67f5ee101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02839da025b74da4b22e0e6e4f0049bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\t C-Val Acc : 3.18%\t W-Val Acc : 0.0%\n",
      "Epoch 2\t C-Val Acc : 3.75%\t W-Val Acc : 0.0%\n",
      "Epoch 3\t C-Val Acc : 5.0%\t W-Val Acc : 0.0%\n",
      "Epoch 3\t C-Train Acc : 48.11%\t W-Train Acc : 0.07%\n"
     ]
    }
   ],
   "source": [
    "input_size = len(list(train_lp.source_char2id.keys()))\n",
    "output_size = len(list(train_lp.target_char2id.keys()))\n",
    "\n",
    "hidden_size = 64\n",
    "batch_size = 32\n",
    "\n",
    "num_encoder_layers = 1\n",
    "num_decoder_layers = 1\n",
    "\n",
    "bidirectional = False\n",
    "\n",
    "encoder = EncoderRNN(input_size, hidden_size,num_encoder_layers,bidirectional).to(device)\n",
    "decoder = DecoderRNN(hidden_size, output_size,train_lp.max_len+append_eos,start_token_id,num_decoder_layers,bidirectional).to(device)\n",
    "\n",
    "train(train_loader,valid_loader, encoder, decoder, 3, print_every=3, plot_every=3,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94f0c784-f46f-4e4f-ae48-55a3be347e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 29])\n",
      "torch.Size([1, 29, 64]) torch.Size([1, 1, 64])\n",
      "స్రారియ్\n"
     ]
    }
   ],
   "source": [
    "op,_ = evaluate(encoder, decoder, word=\"srirama\", language_processor=train_lp,device = device)\n",
    "#op,_ = evaluate(encoder, decoder, word=\"mamidi\", language_processor=train_lp,device = device)\n",
    "\n",
    "op_string = \"\"\n",
    "\n",
    "for char in op:\n",
    "    if char == '<pad>':\n",
    "        break\n",
    "    op_string += char\n",
    "\n",
    "print(op_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20d61b-5635-4d10-83d0-26899e1054ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1) Push Current Encoder-Decoder Code.\n",
    "2) Implement accuracy computation (wordlevel and charlevel)\n",
    "3) Teacher forcing\n",
    "4) Support for LSTM, GRU & RNN\n",
    "5) Multiple layers : GRU etc\n",
    "6) Bi-directional\n",
    "7) Beam Search\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687139cb-98b0-4d39-8af6-3d38fbdf51d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd84afe-e554-4be6-97dc-329e9f93c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.tensor(train_lp.encode_word(\"srirama\",train_lp.source_lang,padding=True,append_eos = True)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54b37f5-4a6d-4f2e-b708-9b3a41e0363a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab1316e-25b8-4647-abed-1a8cbf0de934",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(train_lp.source_char2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616f5d97-51bf-4f4a-ab99-1a1981e0eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(train_lp.target_char2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea534b1-61c2-4374-a82e-bd2e0985864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lp.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e5ee9-697c-4c26-a241-b42e9e325a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lp.target_char2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc8b4b-40a5-41cc-9e3a-e4f081c427be",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
