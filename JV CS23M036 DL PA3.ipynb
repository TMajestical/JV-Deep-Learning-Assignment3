{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f0cd007-c07d-41ae-a7d1-e8a396936a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#JV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75927828-0e72-4da3-a840-98d46c1e6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,TensorDataset, DataLoader, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96d84851-1726-4bfb-955d-209cdc80bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 23\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "094edcd3-d8da-4e5b-b48e-413e23060dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageProcessor:\n",
    "\n",
    "    def __init__(self,language_directory,target_lang_name,mode=\"train\",meta_tokens=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Default Constructor for this class.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            language_directory : ex : \"aksharantar_sampled/tel/\"\n",
    "            mode : \"train\" or \"test\" or \"valid\", accordingly the appropriate dataset is read.\n",
    "            meta_tokens : If true creates the first three tokens of the dictionary as <start>,<end>,<pad>.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        self.meta_tokens = meta_tokens ## boolean variable, if 1, then <start>,<end> and <pad> tokens are cosidered in the vocab.\n",
    "        self.language_directory = language_directory\n",
    "        self.target_lang_name = target_lang\n",
    "        self.mode = mode ## accordingly helps to read and generate the appropriate dataset.\n",
    "    \n",
    "        self.source_lang = 0\n",
    "        self.target_lang = 1\n",
    "\n",
    "        self.source_max_len = self.find_max_len(self.source_lang)\n",
    "        self.target_max_len = self.find_max_len(self.target_lang)\n",
    "\n",
    "        self.max_len = max(self.source_max_len,self.target_max_len)+1 ##accomodating End token also, irrespective of whether it is used.\n",
    "\n",
    "        self.source_char2id,self.source_id2char = self.build_char_vocab(self.source_lang,self.source_max_len)\n",
    "        self.target_char2id,self.target_id2char = self.build_char_vocab(self.target_lang,self.target_max_len)\n",
    "\n",
    "\n",
    "    def find_max_len(self,lang):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to find the maximum length of a word across train/test and validation data.\n",
    "\n",
    "        This would help in padding, the embedding accordingly.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            lang : 0/1 (source/target) language for which the length of the longest word must be found.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        train_df = pd.read_csv(self.language_directory+self.target_lang_name+\"_train.csv\",header=None)\n",
    "        test_df = pd.read_csv(self.language_directory+self.target_lang_name+\"_test.csv\",header=None)\n",
    "        valid_df = pd.read_csv(self.language_directory+self.target_lang_name+\"_valid.csv\",header=None)\n",
    "\n",
    "        train_max_len = len(max(list(train_df[lang]), key = len))\n",
    "        test_max_len = len(max(list(test_df[lang]), key = len))\n",
    "        valid_max_len = len(max(list(valid_df[lang]), key = len))\n",
    "\n",
    "        del train_df\n",
    "        del test_df\n",
    "        del valid_df\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        return max(train_max_len,test_max_len,valid_max_len)\n",
    "\n",
    "    def build_char_vocab(self,lang_id,max_len=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to create a vocabulary of characters in language corresponding to lang_id.\n",
    "        \"\"\"\n",
    "\n",
    "        df = pd.read_csv(self.language_directory+self.target_lang_name+\"_\"+self.mode+\".csv\",header=None)\n",
    "\n",
    "        self.data = df.to_numpy()\n",
    "\n",
    "        lang_chars = []\n",
    "        lang_words = df[lang_id].to_numpy()\n",
    "    \n",
    "        for word in lang_words:\n",
    "            lang_chars += list(word)\n",
    "    \n",
    "        unique_lang_chars =  sorted(list(set(lang_chars)))\n",
    "        \n",
    "        if self.meta_tokens:\n",
    "            char2id_dict = {'<start>':0,'<end>':1,'<pad>': 2}\n",
    "            id2char_dict = {0:'<start>',1:'<end>',2:'<pad>'}\n",
    "            \n",
    "        else:\n",
    "            char2id_dict = {}\n",
    "            id2char_dict = {}\n",
    "\n",
    "        start = len(char2id_dict) ##Key of each language character starts based on meta tokens are used or not.\n",
    "    \n",
    "        for i in range(len(unique_lang_chars)):\n",
    "            char2id_dict[unique_lang_chars[i]] = i+start\n",
    "            id2char_dict[i+start] = unique_lang_chars[i]\n",
    "    \n",
    "        del df\n",
    "        del lang_chars\n",
    "        del unique_lang_chars\n",
    "\n",
    "        gc.collect()\n",
    "    \n",
    "        return char2id_dict,id2char_dict\n",
    "\n",
    "    def encode_word(self,word,lang_id,padding=True,append_eos = True):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to encode characters of a given word.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            word: The word to be encoded.\n",
    "            lang_id : 0/1 for source/target lang.\n",
    "            padding : If true, the word encoding would be padded upto max len.\n",
    "            append_eos : Appends <end> token at the end of every word.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        if lang_id == self.source_lang:\n",
    "            char2id_dict = self.source_char2id\n",
    "            \n",
    "        else:\n",
    "            char2id_dict = self.target_char2id\n",
    "        \n",
    "        max_len = self.max_len\n",
    "\n",
    "        word_encoding = []\n",
    "        \n",
    "        #if lang_id == self.source_lang:\n",
    "        #    word_encoding = [char2id_dict['<start>']] ##every input starts with the <start> token.\n",
    "        \n",
    "        for i in word.lower():\n",
    "            word_encoding.append(char2id_dict[i])\n",
    "\n",
    "        #if append_eos:\n",
    "        #    word_encoding.append(char2id_dict['<end>'])\n",
    "\n",
    "        ## pad till maxlen, if padding is used.\n",
    "        if padding:\n",
    "            word_encoding += [char2id_dict['<pad>']] * (max_len - len(word_encoding))\n",
    "        \n",
    "        return np.array(word_encoding)\n",
    "\n",
    "    def decode_word(self,code_word,lang_id):\n",
    "\n",
    "        \"\"\"\n",
    "        Method to decode an encoded word.\n",
    "\n",
    "        Params:\n",
    "\n",
    "            code_word : The encoded word.\n",
    "            lang_id : 0/1 for source/target lang.\n",
    "        \"\"\"\n",
    "    \n",
    "        word = []\n",
    "\n",
    "        if lang_id == self.source_lang:\n",
    "            id2char_dict = self.source_id2char\n",
    "            char2id_dict = self.source_char2id\n",
    "            \n",
    "        else:\n",
    "            id2char_dict = self.target_id2char\n",
    "            char2id_dict = self.target_char2id\n",
    "\n",
    "        start_idx = 0#1-lang_id\n",
    "        \n",
    "        for i in code_word[start_idx:]:\n",
    "            ## if we reached <end>, then stop decoding\n",
    "            if self.meta_tokens and i == char2id_dict['<end>'] or i == char2id_dict['<pad>']:\n",
    "                break\n",
    "            \n",
    "            word.append(id2char_dict[i])\n",
    "            \n",
    "        return np.array(word)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1e5df96-e649-4354-852d-85f7116fd797",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Class that inherits and overrides the methods of Dataset class. This helps in creating a data loader.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, language_processor,append_eos=True,device='cpu'):\n",
    "\n",
    "        self.lp = language_processor\n",
    "        self.data = self.lp.data\n",
    "        self.device = device\n",
    "        self.append_eos = append_eos\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_word, output_word = self.data[idx]\n",
    "        \n",
    "        input_sequence = self.lp.encode_word(input_word,self.lp.source_lang,padding=False,append_eos=self.append_eos)\n",
    "        output_sequence = self.lp.encode_word(output_word,self.lp.target_lang,padding=False,append_eos=self.append_eos)\n",
    "        \n",
    "\n",
    "        #if len(input_sequence) != len(output_sequence):\n",
    "        #    print(input_word,len(input_word),output_word,len(output_word))\n",
    "        \n",
    "        return torch.tensor(input_sequence).to(device), torch.tensor(output_sequence).to(device)\n",
    "        #return input_sequence, output_sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6583324c-6495-4bb5-a536-d614671bc280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c93e7db4-7631-4b6b-9db9-bf18950fa6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \n",
    "    input_words, target_words = zip(*batch)\n",
    "    \n",
    "    padded_inputs = pad_sequence(input_words, batch_first=True, padding_value=pad_token_id)\n",
    "    \n",
    "    padded_targets = pad_sequence(target_words, batch_first=True, padding_value=pad_token_id)\n",
    "    \n",
    "    input_lengths = torch.LongTensor([len(seq) for seq in input_words]).to(device)\n",
    "    target_lengths = torch.LongTensor([len(seq) for seq in target_words]).to(device)\n",
    "    \n",
    "    return padded_inputs, padded_targets, input_lengths, target_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44c05ce5-5d0e-4e8b-89bd-5f6ef688eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "base_dir = \"aksharantar_sampled/\"\n",
    "target_lang = \"tel\"\n",
    "\n",
    "use_meta_tokens = True\n",
    "append_eos = 1\n",
    "\n",
    "lang_dir = base_dir + target_lang + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "751abaf1-9aa9-404b-91aa-cb30403695a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "##creating train loader\n",
    "train_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"train\",meta_tokens=use_meta_tokens)\n",
    "train_dataset = WordDataset(train_lp,device=device)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size,collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "## creating test loader\n",
    "test_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"test\",meta_tokens=use_meta_tokens)\n",
    "\n",
    "## to make sure that the same vocabulary and dictionaries are used everywhere\n",
    "test_lp.source_char2id = train_lp.source_char2id\n",
    "test_lp.source_id2char = train_lp.source_id2char\n",
    "test_lp.target_char2id = train_lp.target_char2id\n",
    "test_lp.target_id2char = train_lp.target_id2char\n",
    "\n",
    "test_dataset = WordDataset(test_lp,device=device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size,collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "## creating validation loader\n",
    "valid_lp = LanguageProcessor(language_directory=lang_dir,target_lang_name=target_lang,mode=\"valid\",meta_tokens=use_meta_tokens)\n",
    "valid_lp.source_char2id = train_lp.source_char2id\n",
    "valid_lp.source_id2char = train_lp.source_id2char\n",
    "valid_lp.target_char2id = train_lp.target_char2id\n",
    "valid_lp.target_id2char = train_lp.target_id2char\n",
    "\n",
    "valid_dataset = WordDataset(valid_lp,device=device)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size,collate_fn=collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9f8fdd0-01d8-4bfd-8f9d-17a570e9c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##in principle these are all fixed across train/test/valid data\n",
    "\n",
    "## the ids of these tokens are the same in the source and target language\n",
    "start_token_id = train_lp.source_char2id['<start>']\n",
    "end_token_id = train_lp.source_char2id['<end>']\n",
    "pad_token_id = train_lp.source_char2id['<pad>']\n",
    "\n",
    "#source_max_len = train_lp.source_max_len\n",
    "#target_max_len = train_lp.target_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a06dc7ea-1065-46a6-8e2d-c49ce0cf6615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hastakalarupalu హస్తకళారూపాలు\n",
      "chintinchuvaadu చింతించువాడు\n",
      "vraaseddaamani వ్రాసేద్దామని\n",
      "gundichadevi గుండిచాదేవి\n",
      "niyamichaalani నియమిచాలని\n"
     ]
    }
   ],
   "source": [
    "## Verifying if the dataloaders are carrying data correctly.\n",
    "count = 0\n",
    "for data in train_loader:\n",
    "\n",
    "    inp,tar,_,_ = data\n",
    "\n",
    "    inp = inp.view(batch_size,-1).cpu().numpy()\n",
    "    tar = tar.view(batch_size,-1).cpu().numpy()\n",
    "\n",
    "    for i in range(min(batch_size,5)):\n",
    "        print(\"\".join(list(train_lp.decode_word(inp[i],0))),\"\".join(list(train_lp.decode_word(tar[i],1))))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d39e84-a7e5-4344-af5d-ed91b8c5d2ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3698224-b989-4dfd-99a0-0007c176a5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,padding_idx = None ,dropout_p=0.1,num_layers = 1,bidirectional = False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size,padding_idx = padding_idx)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3681ca-02e0-4ca2-94af-43af4f8f821b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e6c0c87-21d9-4609-9739-9df28e7844bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size,max_len,start_token_id,padding_idx = None,num_layers = 1,bidirectional = False):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.D = 1 ##the number of directions in which the input is viewed.\n",
    "        if bidirectional:\n",
    "            self.D = 2\n",
    "\n",
    "        ## In h0 (the input to the decoder) first dimension expected is number of directions X number of layers \n",
    "        self.expected_h0_dim1 = self.D*self.num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size,padding_idx = padding_idx)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True,num_layers = num_layers,bidirectional = bidirectional)\n",
    "        self.out = nn.Linear(hidden_size*self.D, output_size)\n",
    "        #self.max_len = max_len\n",
    "        self.start_token_id = start_token_id\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor,eval_mode = False,teacher_forcing_ratio=0):\n",
    "        \n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        if not eval_mode:\n",
    "            max_len = target_tensor.size(1)\n",
    "\n",
    "        ## eval mode is for explicitly looking at the word that is predicted to compare with the correct word.\n",
    "        if eval_mode:\n",
    "            batch_size = 1\n",
    "            max_len = 28\n",
    "\n",
    "        ## TRY : adding requires_grad = True in torch.empty\n",
    "        #decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(-1)\n",
    "        decoder_input = torch.zeros((batch_size, 1), dtype=torch.long, device=device)#.fill_(-1)\n",
    "        decoder_hidden = encoder_hidden ## in the first time step of the decoder, the output of the encoder is the input.\n",
    "        decoder_outputs = []\n",
    "\n",
    "        for i in range(max_len):\n",
    "            ## print(i,self.max_len)\n",
    "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden,eval_mode)\n",
    "            #print(f\"Step{i}\\t Input:{decoder_input.shape}\\t State:{decoder_hidden.shape}\\tOutput:{decoder_output.shape}\\t\")\n",
    "            ## print(decoder_output.shape)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            if (target_tensor is not None) and (teacher_force):\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden,eval_mode):\n",
    "\n",
    "        #print(hidden.unsqueeze(0).shape)\n",
    "        if eval_mode:\n",
    "            input = input.view(1,-1)\n",
    "\n",
    "        \n",
    "        output = self.embedding(input)\n",
    "\n",
    "        #print(hidden.shape)\n",
    "\n",
    "        if hidden.shape[0] != self.expected_h0_dim1:\n",
    "            reshaped_hidden = hidden.repeat(self.expected_h0_dim1,1,1)\n",
    "        else:\n",
    "            reshaped_hidden = hidden\n",
    "\n",
    "        #print(\"In DC\",hidden.shape,reshaped_hidden.shape)\n",
    "        \n",
    "        #print(output.shape,output.unsqueeze(0).shape,output.unsqueeze(0).transpose(0,1).shape)\n",
    "\n",
    "        #print(\"In DC INP:\",input.shape)\n",
    "        #print(\"In DC HS:\",reshaped_hidden.shape)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, reshaped_hidden)\n",
    "        #print(output.shape)\n",
    "        #print(\"Yo1\")\n",
    "        output = self.out(output.squeeze(0))\n",
    "        #print(\"Yo2\")\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cb0d70-d255-49e2-b349-56e3be50e0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0b65188-48a0-4bf6-9bd9-cbab7ca05b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOne thing to take care of:\\n\\n    In encoder we set no grad for pad token in nn.Embed\\n    \\n    In decoder, \\n        => need to see what to do after actual word is generated. How to deal with padding?\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "One thing to take care of:\n",
    "\n",
    "    In encoder we set no grad for pad token in nn.Embed\n",
    "    \n",
    "    In decoder, \n",
    "        => need to see what to do after actual word is generated. How to deal with padding?\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67c38af4-7bf7-4adf-ab8b-71ebb0b69416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,decoder_optimizer, criterion,teacher_forcing_ratio):\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in tqdm(dataloader):\n",
    "        ## print(f\"Data Shape : {data[0].shape,data[1].shape}\")\n",
    "        input_tensor, target_tensor,_,target_max_len = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        \n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor=target_tensor,teacher_forcing_ratio = teacher_forcing_ratio)\n",
    "\n",
    "        #print(decoder_outputs.shape,decoder_outputs.view(-1, decoder_outputs.size(-1)).shape,target_tensor.shape,target_tensor.view(-1).shape)\n",
    "\n",
    "        #print(f\"Decoder Outputs Shape for computing Loss : {decoder_outputs.view(-1, decoder_outputs.size(-1)).shape}\")\n",
    "        #print(f\"Target Shape for computing Loss : {target_tensor.view(-1).shape}\")\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72aee733-e313-47eb-b041-7af96d5a65db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e907afc-4b4b-4f7b-8e90-e605ba441e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(dataloader,encoder,decoder,loss_criterion,padding_token_id,end_token_id = 1,ignore_padding = True,device='cpu'):\n",
    "\n",
    "    \"\"\"\n",
    "    Method to compute the accuracy using the model (encoder-decoder) using dataloader.\n",
    "\n",
    "    This method returns word and character level accuracy.\n",
    "\n",
    "        Word Level Accuracy : Accuracy is computed at the word level and a word is right iff every character is predicted correctly.\n",
    "        Char Level Accuracy : Accuracy is computed by comparing each predicted character wrt the correct char.\n",
    "\n",
    "    Params:\n",
    "\n",
    "        dataloader : The train/test/valid dataloader.\n",
    "        encoder : The encoder \n",
    "        decoder : The decoder\n",
    "        padding_token_id : The id of the padding token.\n",
    "        ignore_padding : If True, then in word level accuracy, the padding characters are ignored in computing the word level accuracy.\n",
    "                        char level accuracy, the padding characters are not considered at all.\n",
    "\n",
    "                        If false, padding is considered to be a part of the word (for word level accuracy) and \n",
    "    \"\"\"\n",
    "\n",
    "    char_lvl_accuracy = 0\n",
    "    word_level_accuracy = 0\n",
    "\n",
    "    tot_chars = 0\n",
    "    tot_words = 0\n",
    "\n",
    "    tot_correct_char_preds = 0\n",
    "    tot_correct_word_preds = 0\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    criterion = loss_criterion.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        train = 0\n",
    "\n",
    "        if encoder.training and decoder.training: ## reset the the model back to train mode\n",
    "            train = 1\n",
    "\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        for data in dataloader:\n",
    "            \n",
    "            input_tensor, target_tensor,_,target_max_len = data\n",
    "    \n",
    "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "            ## even though we are passing target tensor, the teacher forcing ratio is 0, so no teacher forcing\n",
    "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor = target_tensor,teacher_forcing_ratio = 0)\n",
    "    \n",
    "            loss += criterion(\n",
    "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                target_tensor.view(-1)).item()\n",
    "    \n",
    "            ## For a batch, for each character find the most probable output word.\n",
    "            multi_step_preds = torch.argmax(decoder_outputs,dim=2)\n",
    "            multi_step_pred_correctness = (multi_step_preds ==  target_tensor)\n",
    "            num_chars = multi_step_preds.numel() ##find the total number of characters in the current batch\n",
    "            num_words = multi_step_preds.shape[0] ##find the total number of words in the current batch.\n",
    "    \n",
    "            if ignore_padding: ## if padding has to be ignored.\n",
    "    \n",
    "                ## for each word, based on the padding token ID, find the first occurance of the padding token, marking the begining of padding.\n",
    "                padding_start = torch.argmax(target_tensor == pad_token_id,dim=1).to(device)\n",
    "                ## Creating a mask with 1's in each position of a padding token\n",
    "                mask = (torch.arange(target_tensor.size(1)).unsqueeze(0).to(device) >= padding_start.unsqueeze(1))\n",
    "                #print(mask)\n",
    "                \n",
    "                ##doing a logical OR with the mask makes sure that the padding tokens do not affect the correctness of the word\n",
    "                tot_correct_word_preds += (torch.all(torch.logical_or(multi_step_pred_correctness,mask),dim=1).int().sum()).item()\n",
    "                tot_words += num_words\n",
    "    \n",
    "                ##creating a complement of the mask so to mark padding tokens as irrelevant\n",
    "                complement_mask = (1-mask.int()).bool()\n",
    "                num_pad_chars = mask.int().sum().item()\n",
    "                ##counting number of non_pad_chars to compute accuracy.\n",
    "                num_non_pad_chars = num_chars - num_pad_chars\n",
    "    \n",
    "                tot_correct_char_preds += (torch.logical_and(multi_step_pred_correctness,complement_mask).int().sum()).item()\n",
    "                tot_chars += num_non_pad_chars\n",
    "                \n",
    "        \n",
    "            else: ##otherwise.\n",
    "    \n",
    "                tot_correct_word_preds += (torch.all(multi_step_pred_correctness,dim=1).int().sum()).item()\n",
    "                tot_words += num_words\n",
    "                \n",
    "                tot_correct_char_preds += (multi_step_pred_correctness.int().sum()).item()\n",
    "                tot_chars += num_chars\n",
    "\n",
    "        #print(tot_correct_char_preds,tot_chars)\n",
    "        #print(tot_correct_word_preds,tot_words)\n",
    "    \n",
    "        char_lvl_accuracy = round(tot_correct_char_preds*100/tot_chars,2)\n",
    "        word_lvl_accuracy = round(tot_correct_word_preds*100/tot_words,2)\n",
    "\n",
    "        loss /= dataloader.dataset.data.shape[0]\n",
    "\n",
    "        if train:\n",
    "\n",
    "            encoder.train()\n",
    "            decoder.train()\n",
    "    \n",
    "        return round(loss,2),char_lvl_accuracy,word_lvl_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f149d-39ce-46ef-9b72-3cb24a57d7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f33e86bf-bdef-4939-9f94-66c10e15665a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader,valid_loader, encoder, decoder, n_epochs,loss_criterion = nn.NLLLoss(), learning_rate=0.001,teacher_forcing = False,teacher_forcing_ratio = 0,print_every=100, plot_every=100,device='cpu'):\n",
    "    \n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    lp = train_loader.dataset.lp\n",
    "    \n",
    "    criterion = loss_criterion.to(device)\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "\n",
    "        \"\"\"if epoch/n_epochs > teacher_forcing_ratio:\n",
    "            teacher_forcing = False\"\"\"\n",
    "        \n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion,teacher_forcing_ratio)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        val_loss,val_char_lvl_accuracy,val_word_level_accuracy = compute_accuracy(valid_loader,encoder,decoder,loss_criterion,padding_token_id = lp.source_char2id['<pad>'],end_token_id = lp.source_char2id['<end>'],ignore_padding=True,device=device)\n",
    "\n",
    "        print(f\"Epoch {epoch}\\t Val Loss : {val_loss}\\tC-Val Acc : {val_char_lvl_accuracy}%\\t W-Val Acc : {val_word_level_accuracy}%\")\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            #print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),epoch, epoch / n_epochs * 100, print_loss_avg))\n",
    "            train_loss,train_char_lvl_accuracy,train_word_level_accuracy = compute_accuracy(train_loader,encoder,decoder,loss_criterion,padding_token_id = lp.source_char2id['<pad>'],end_token_id = lp.source_char2id['<end>'],ignore_padding=True,device=device)\n",
    "            print(f\"Epoch {epoch}\\t Train Loss : {train_loss}\\t C-Train Acc : {train_char_lvl_accuracy}%\\t W-Train Acc : {train_word_level_accuracy}%\")\n",
    "            \n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e03b50a-c511-4dc7-b976-304a97cff25b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44f32dff-d6d8-465f-a75a-c878d3d4bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0bf6186-c346-41f5-b23a-70ac8dec092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, word, language_processor,device = \"cpu\"):\n",
    "\n",
    "    lp = language_processor\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        train = 0\n",
    "\n",
    "        if encoder.training and decoder.training: ## reset the the model back to train mode\n",
    "            train = 1\n",
    "\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        \n",
    "        input_tensor = torch.tensor(lp.encode_word(word,lp.source_lang,padding=True,append_eos = True)).to(device).view(1,-1)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "\n",
    "        \"\"\"if encoder_hidden.shape[0] != decoder.expected_h0_dim1:\n",
    "            reshaped_encoder_hidden = encoder_hidden.repeat(decoder.expected_h0_dim1,1,1)\n",
    "        else:\n",
    "            reshaped_encoder_hidden = encoder_hidden\"\"\"\n",
    "\n",
    "        #print(encoder_hidden.shape,encoder_hidden.shape)\n",
    "\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden,eval_mode = True,target_tensor = None)\n",
    "\n",
    "        output_size = len(list(train_lp.target_char2id.keys()))\n",
    "        decoder_outputs = decoder_outputs.view(28,-1)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_chars = []\n",
    "        \"\"\"for idx in decoded_ids:\n",
    "            if idx.item() == end_token_id:\n",
    "                break\n",
    "            decoded_chars.append(lp.target_id2char[idx.item()])\"\"\"\n",
    "\n",
    "        decoded_word = lp.decode_word(decoded_ids.cpu().numpy(),lp.target_lang)\n",
    "\n",
    "    if train:\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "\n",
    "    \n",
    "    return decoded_word, decoder_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce503bff-3de9-49b3-beec-16fa599b10d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22585702-e9b2-4dcb-8260-45a67f5ee101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86852ddad27548d5b317fcb24f454299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6021f407a7324a26bf8ab6ac13396cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\t Val Loss : 0.02\tC-Val Acc : 69.66%\t W-Val Acc : 26.34%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428a44c8b30b479da33485c7496e5fdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\t Val Loss : 0.01\tC-Val Acc : 77.96%\t W-Val Acc : 40.23%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a681d5ac61ec4fb6b5fb40d3916d2b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\t Val Loss : 0.01\tC-Val Acc : 80.26%\t W-Val Acc : 44.48%\n",
      "Epoch 3\t Train Loss : 0.01\t C-Train Acc : 82.74%\t W-Train Acc : 48.42%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beac0cdd52c841a2860837641210b9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\t Val Loss : 0.01\tC-Val Acc : 81.78%\t W-Val Acc : 47.53%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7129f8f45c4496bfe7741fb8bf56e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\t Val Loss : 0.01\tC-Val Acc : 82.18%\t W-Val Acc : 47.34%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c11bd4a164234a38b609e89679b3fea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\t Val Loss : 0.01\tC-Val Acc : 82.92%\t W-Val Acc : 49.68%\n",
      "Epoch 6\t Train Loss : 0.01\t C-Train Acc : 88.83%\t W-Train Acc : 62.41%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f43ba80be74f79bd3d848657cd31ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\t Val Loss : 0.01\tC-Val Acc : 83.32%\t W-Val Acc : 50.15%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37c19b0d47e44670a37ac8bea5af9f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\t Val Loss : 0.01\tC-Val Acc : 83.48%\t W-Val Acc : 50.27%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d4ccd693424774bd002feed307257b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\t Val Loss : 0.01\tC-Val Acc : 83.2%\t W-Val Acc : 49.78%\n",
      "Epoch 9\t Train Loss : 0.01\t C-Train Acc : 91.04%\t W-Train Acc : 68.19%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c288d92ba14d188f1af4c87dc16740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\t Val Loss : 0.01\tC-Val Acc : 83.87%\t W-Val Acc : 52.15%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_size = len(list(train_lp.source_char2id.keys()))\n",
    "output_size = len(list(train_lp.target_char2id.keys()))\n",
    "\n",
    "hidden_size = 128\n",
    "batch_size = 64\n",
    "\n",
    "num_encoder_layers = 2\n",
    "num_decoder_layers = 2\n",
    "\n",
    "bidirectional = True\n",
    "teacher_forcing = True\n",
    "teacher_forcing_ratio = 0.3\n",
    "\n",
    "loss_criterion =  nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "#loss_criterion = nn.NLLLoss(ignore_index=pad_token_id)\n",
    "#loss_criterion = nn.NLLLoss(ignore_index=pad_token_id)\n",
    "\n",
    "dropout=0.1\n",
    "\n",
    "encoder = EncoderRNN(input_size = input_size, hidden_size = hidden_size,num_layers=num_encoder_layers,bidirectional=bidirectional,padding_idx=pad_token_id,dropout_p=dropout).to(device)\n",
    "decoder = DecoderRNN(hidden_size = hidden_size, output_size = output_size,max_len = train_lp.max_len,start_token_id = start_token_id,num_layers = num_decoder_layers,bidirectional = bidirectional,padding_idx = None).to(device)\n",
    "\n",
    "train(train_loader,valid_loader, encoder, decoder, 10,loss_criterion=loss_criterion, print_every=3, plot_every=5,device=device,teacher_forcing = teacher_forcing,teacher_forcing_ratio=teacher_forcing_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edb1380-b7e4-4744-a8ef-3b720852639b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "94f0c784-f46f-4e4f-ae48-55a3be347e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "శ్రీరామా\n",
      "తెజస్వి\n",
      "\n"
     ]
    }
   ],
   "source": [
    "op1,_ = evaluate(encoder, decoder, word=\"srirama\", language_processor=train_lp,device = device)\n",
    "\n",
    "op2,_ = evaluate(encoder, decoder, word=\"tejasvi\", language_processor=train_lp,device = device)\n",
    "\n",
    "op1_string = \"\".join(op1)\n",
    "print(op_string[:8])\n",
    "\n",
    "op2_string = \"\".join(op2)\n",
    "print(op2_string[:7])\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a6d6e-bde3-46d6-b858-95fd8768a3ff",
   "metadata": {},
   "source": [
    "### JV\n",
    "\n",
    "###### To Do:\n",
    "\n",
    "1. Add LSTM,RNN support : Train\n",
    "2. Write code for attention.\n",
    "3. Write code for beam search.\n",
    "4. Now create a seq2seq class, specify attention = True for attention to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e908c4f-af04-4ff8-95e4-875c6e5a156b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
